{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0000-0001-0001-000000000000",
   "metadata": {},
   "source": [
    "# Dataset Construction\n",
    "\n",
    "Merges price features, fundamentals, and news embeddings into final ML dataset.\n",
    "\n",
    "**Steps**:\n",
    "1. Load price features (base table)\n",
    "2. Apply liquidity filter ($5 price, $10M avg volume)\n",
    "3. Point-in-time join fundamentals (45-day lag)\n",
    "4. Join news embeddings by trading_date\n",
    "5. Cross-sectional normalize all features\n",
    "6. Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "MIN_PRICE = 5.0\n",
    "MIN_DOLLAR_VOL = 10_000_000  # $10M\n",
    "FUNDAMENTAL_LAG_DAYS = 45  # Conservative 10-Q filing deadline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000003",
   "metadata": {},
   "source": [
    "## 1. Load price features (base table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.read_parquet(\"data/price_features.pqt\")\n",
    "prices[\"feature_date\"] = pd.to_datetime(prices[\"feature_date\"])\n",
    "prices[\"target_date\"] = pd.to_datetime(prices[\"target_date\"])\n",
    "\n",
    "print(f\"Price features: {len(prices):,} rows, {prices['symbol'].nunique():,} symbols\")\n",
    "print(f\"Date range: {prices['feature_date'].min().date()} to {prices['feature_date'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000005",
   "metadata": {},
   "source": [
    "## 2. Apply liquidity filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute dollar volume\n",
    "prices[\"dollar_vol\"] = prices[\"close\"] * prices[\"volume\"]\n",
    "\n",
    "# 20-day rolling average dollar volume (per symbol)\n",
    "prices = prices.sort_values([\"symbol\", \"feature_date\"])\n",
    "prices[\"avg_dollar_vol_20d\"] = prices.groupby(\"symbol\")[\"dollar_vol\"].transform(\n",
    "    lambda x: x.rolling(20, min_periods=10).mean()\n",
    ")\n",
    "\n",
    "print(f\"Dollar vol stats:\")\n",
    "print(prices[\"avg_dollar_vol_20d\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filters\n",
    "n_before = len(prices)\n",
    "mask = (prices[\"close\"] >= MIN_PRICE) & (prices[\"avg_dollar_vol_20d\"] >= MIN_DOLLAR_VOL)\n",
    "prices = prices[mask].copy()\n",
    "n_after = len(prices)\n",
    "\n",
    "print(f\"Liquidity filter: {n_before:,} -> {n_after:,} ({n_after/n_before*100:.1f}%)\")\n",
    "print(f\"Symbols remaining: {prices['symbol'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop helper columns\n",
    "prices = prices.drop(columns=[\"dollar_vol\", \"avg_dollar_vol_20d\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000009",
   "metadata": {},
   "source": [
    "## 3. Load and prepare fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all fundamental data\n",
    "metrics = pd.read_parquet(\"data/key_metrics.pqt\")\n",
    "ratios = pd.read_parquet(\"data/ratios.pqt\")\n",
    "growth = pd.read_parquet(\"data/growth.pqt\")\n",
    "\n",
    "print(f\"Metrics: {len(metrics):,} rows\")\n",
    "print(f\"Ratios: {len(ratios):,} rows\")\n",
    "print(f\"Growth: {len(growth):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features (~15-20 focused features)\n",
    "METRIC_COLS = [\n",
    "    \"evToEBITDA\",           # Value\n",
    "    \"freeCashFlowYield\",    # Value\n",
    "    \"earningsYield\",        # Value\n",
    "    \"returnOnEquity\",       # Quality\n",
    "    \"returnOnAssets\",       # Quality\n",
    "    \"returnOnInvestedCapital\",  # Quality\n",
    "    \"currentRatio\",         # Liquidity\n",
    "]\n",
    "\n",
    "RATIO_COLS = [\n",
    "    \"priceToEarningsRatio\",  # Value (P/E)\n",
    "    \"priceToBookRatio\",      # Value\n",
    "    \"priceToSalesRatio\",     # Value\n",
    "    \"grossProfitMargin\",     # Quality\n",
    "    \"operatingProfitMargin\", # Quality\n",
    "    \"netProfitMargin\",       # Quality\n",
    "    \"debtToEquityRatio\",     # Leverage\n",
    "    \"debtToAssetsRatio\",     # Leverage\n",
    "]\n",
    "\n",
    "GROWTH_COLS = [\n",
    "    \"revenueGrowth\",         # Growth\n",
    "    \"netIncomeGrowth\",       # Growth\n",
    "    \"epsgrowth\",             # Growth\n",
    "    \"operatingIncomeGrowth\", # Growth\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge fundamentals into single table\n",
    "metrics_sub = metrics[[\"symbol\", \"date\"] + METRIC_COLS].copy()\n",
    "ratios_sub = ratios[[\"symbol\", \"date\"] + RATIO_COLS].copy()\n",
    "growth_sub = growth[[\"symbol\", \"date\"] + GROWTH_COLS].copy()\n",
    "\n",
    "# Merge on symbol + date\n",
    "fundamentals = metrics_sub.merge(ratios_sub, on=[\"symbol\", \"date\"], how=\"outer\")\n",
    "fundamentals = fundamentals.merge(growth_sub, on=[\"symbol\", \"date\"], how=\"outer\")\n",
    "\n",
    "print(f\"Combined fundamentals: {len(fundamentals):,} rows\")\n",
    "print(f\"Symbols: {fundamentals['symbol'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add available_date = period_end + 45 days (point-in-time)\n",
    "fundamentals[\"period_end\"] = pd.to_datetime(fundamentals[\"date\"])\n",
    "fundamentals[\"available_date\"] = fundamentals[\"period_end\"] + timedelta(days=FUNDAMENTAL_LAG_DAYS)\n",
    "\n",
    "# Sort for asof merge\n",
    "fundamentals = fundamentals.sort_values([\"symbol\", \"available_date\"])\n",
    "\n",
    "print(f\"Example available_date:\")\n",
    "print(fundamentals[[\"symbol\", \"period_end\", \"available_date\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point-in-time join: for each (symbol, feature_date), get most recent fundamental\n",
    "# where available_date <= feature_date\n",
    "\n",
    "fund_cols = METRIC_COLS + RATIO_COLS + GROWTH_COLS\n",
    "\n",
    "def pit_join_fundamentals(prices_df: pd.DataFrame, fund_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Point-in-time join fundamentals to prices.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for symbol in prices_df[\"symbol\"].unique():\n",
    "        price_sym = prices_df[prices_df[\"symbol\"] == symbol].copy()\n",
    "        fund_sym = fund_df[fund_df[\"symbol\"] == symbol].copy()\n",
    "        \n",
    "        if fund_sym.empty:\n",
    "            # No fundamentals for this symbol\n",
    "            for col in fund_cols:\n",
    "                price_sym[col] = np.nan\n",
    "            price_sym[\"has_fundamentals\"] = 0\n",
    "            results.append(price_sym)\n",
    "            continue\n",
    "        \n",
    "        # asof merge: for each feature_date, get most recent available_date\n",
    "        price_sym = price_sym.sort_values(\"feature_date\")\n",
    "        fund_sym = fund_sym.sort_values(\"available_date\")\n",
    "        \n",
    "        merged = pd.merge_asof(\n",
    "            price_sym,\n",
    "            fund_sym[[\"available_date\"] + fund_cols],\n",
    "            left_on=\"feature_date\",\n",
    "            right_on=\"available_date\",\n",
    "            direction=\"backward\"\n",
    "        )\n",
    "        merged[\"has_fundamentals\"] = merged[fund_cols[0]].notna().astype(int)\n",
    "        merged = merged.drop(columns=[\"available_date\"])\n",
    "        results.append(merged)\n",
    "    \n",
    "    return pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# This can be slow, show progress\n",
    "print(\"Joining fundamentals (point-in-time)...\")\n",
    "df = pit_join_fundamentals(prices, fundamentals)\n",
    "\n",
    "print(f\"After fundamental join: {len(df):,} rows\")\n",
    "print(f\"Has fundamentals: {df['has_fundamentals'].sum():,} ({df['has_fundamentals'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000016",
   "metadata": {},
   "source": [
    "## 4. Load and join news embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings and news (for trading_date)\n",
    "embeddings = pd.read_parquet(\"data/news_embeddings.pqt\")\n",
    "news = pd.read_parquet(\"data/all_the_news_anon.pqt\")\n",
    "\n",
    "print(f\"Embeddings: {len(embeddings):,} rows\")\n",
    "print(f\"News: {len(news):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trading_date from news\n",
    "news_meta = news[[\"url\", \"symbol\", \"trading_date\"]].copy()\n",
    "news_meta[\"trading_date\"] = pd.to_datetime(news_meta[\"trading_date\"])\n",
    "\n",
    "# Join embeddings with trading_date\n",
    "emb_with_date = embeddings.merge(news_meta, on=[\"url\", \"symbol\"], how=\"inner\")\n",
    "print(f\"Embeddings with trading_date: {len(emb_with_date):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify embedding columns\n",
    "emb_cols = [c for c in embeddings.columns if c.startswith(\"emb_\")]\n",
    "print(f\"Embedding dimension: {len(emb_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate: mean embedding + count per (symbol, trading_date)\n",
    "emb_agg = emb_with_date.groupby([\"symbol\", \"trading_date\"]).agg(\n",
    "    **{col: (col, \"mean\") for col in emb_cols},\n",
    "    news_count=(\"url\", \"count\")\n",
    ").reset_index()\n",
    "\n",
    "print(f\"Aggregated embeddings: {len(emb_agg):,} (symbol, trading_date) pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join embeddings to dataset\n",
    "df = df.merge(\n",
    "    emb_agg,\n",
    "    left_on=[\"symbol\", \"feature_date\"],\n",
    "    right_on=[\"symbol\", \"trading_date\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "df = df.drop(columns=[\"trading_date\"], errors=\"ignore\")\n",
    "\n",
    "# Fill missing news\n",
    "df[\"news_count\"] = df[\"news_count\"].fillna(0).astype(int)\n",
    "df[emb_cols] = df[emb_cols].fillna(0)\n",
    "\n",
    "print(f\"After news join: {len(df):,} rows\")\n",
    "print(f\"Rows with news: {(df['news_count'] > 0).sum():,} ({(df['news_count'] > 0).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000022",
   "metadata": {},
   "source": [
    "## 5. Cross-sectional normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_sectional_zscore(df: pd.DataFrame, col: str, clip: float = 3.0) -> pd.Series:\n",
    "    \"\"\"Z-score within each date, with winsorization.\"\"\"\n",
    "    grouped = df.groupby(\"feature_date\")[col]\n",
    "    mean = grouped.transform(\"mean\")\n",
    "    std = grouped.transform(\"std\")\n",
    "    z = (df[col] - mean) / std\n",
    "    return z.clip(-clip, clip)\n",
    "\n",
    "def fill_with_median(df: pd.DataFrame, col: str) -> pd.Series:\n",
    "    \"\"\"Fill NaN with cross-sectional median.\"\"\"\n",
    "    median = df.groupby(\"feature_date\")[col].transform(\"median\")\n",
    "    return df[col].fillna(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize fundamental features\n",
    "for col in fund_cols:\n",
    "    # Fill missing with median first\n",
    "    df[col] = fill_with_median(df, col)\n",
    "    # Then z-score\n",
    "    df[f\"{col}_z\"] = cross_sectional_zscore(df, col)\n",
    "\n",
    "fund_cols_z = [f\"{col}_z\" for col in fund_cols]\n",
    "print(f\"Normalized {len(fund_cols)} fundamental features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize news_count\n",
    "df[\"news_count_z\"] = cross_sectional_zscore(df, \"news_count\")\n",
    "\n",
    "print(\"News count stats (z-scored):\")\n",
    "print(df[\"news_count_z\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000026",
   "metadata": {},
   "source": [
    "## 6. Final dataset assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price feature columns (already normalized in 1.4.1)\n",
    "price_feat_cols = [\n",
    "    \"overnight_gap_z\", \"intraday_ret_z\",\n",
    "    \"ret_1d_z\", \"ret_2d_z\", \"ret_3d_z\", \"ret_5d_z\",\n",
    "    \"vol_5d_z\", \"dist_from_high_5d_z\", \"dist_from_low_5d_z\"\n",
    "]\n",
    "\n",
    "# Assemble final columns\n",
    "id_cols = [\"symbol\", \"feature_date\", \"target_date\"]\n",
    "target_cols = [\"target_return\", \"target_demean\", \"target_rank\"]\n",
    "flag_cols = [\"has_fundamentals\"]\n",
    "\n",
    "final_cols = (\n",
    "    id_cols + \n",
    "    target_cols + \n",
    "    flag_cols +\n",
    "    price_feat_cols + \n",
    "    fund_cols_z + \n",
    "    [\"news_count_z\"] + \n",
    "    emb_cols\n",
    ")\n",
    "\n",
    "dataset = df[final_cols].copy()\n",
    "print(f\"Final columns: {len(final_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with any remaining NaN in features\n",
    "n_before = len(dataset)\n",
    "dataset = dataset.dropna()\n",
    "n_after = len(dataset)\n",
    "print(f\"Dropped {n_before - n_after:,} rows with NaN\")\n",
    "print(f\"Final dataset: {n_after:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(f\"Date range: {dataset['feature_date'].min().date()} to {dataset['feature_date'].max().date()}\")\n",
    "print(f\"Symbols: {dataset['symbol'].nunique():,}\")\n",
    "print(f\"Days: {dataset['feature_date'].nunique():,}\")\n",
    "print(f\"Avg rows per day: {len(dataset) / dataset['feature_date'].nunique():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature coverage\n",
    "print(f\"\\nFeature coverage:\")\n",
    "print(f\"  Has fundamentals: {dataset['has_fundamentals'].mean()*100:.1f}%\")\n",
    "print(f\"  Has news: {(dataset['news_count_z'] > 0).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "OUTPUT_PATH = Path(\"data/ml_dataset.pqt\")\n",
    "dataset.to_parquet(OUTPUT_PATH, index=False)\n",
    "print(f\"Saved to {OUTPUT_PATH}\")\n",
    "print(f\"File size: {OUTPUT_PATH.stat().st_size / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000032",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
