{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0000-0001-0001-000000000000",
   "metadata": {},
   "source": "# Price Features and Target\n\nComputes:\n1. **Target**: log return `log(close_{t+1} / close_t)`\n2. **Cross-sectional targets**: demeaned return, rank\n3. **Recent price features**: short-term returns, volatility, distance from highs/lows"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.read_parquet(\"data/prices.pqt\")\n",
    "print(f\"Prices: {len(prices):,} rows, {prices['symbol'].nunique():,} symbols\")\n",
    "print(f\"Columns: {list(prices.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse dates and sort\n",
    "prices[\"date\"] = pd.to_datetime(prices[\"date\"]).dt.date\n",
    "prices = prices.sort_values([\"symbol\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Date range: {prices['date'].min()} to {prices['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000004",
   "metadata": {},
   "outputs": [],
   "source": "# Compute log return for target (as per README section 10)\nprices[\"next_close\"] = prices.groupby(\"symbol\")[\"close\"].shift(-1)\nprices[\"target_date\"] = prices.groupby(\"symbol\")[\"date\"].shift(-1)\nprices[\"target_return\"] = np.log(prices[\"next_close\"] / prices[\"close\"])\n\nprices[[\"symbol\", \"date\", \"close\", \"next_close\", \"target_return\", \"target_date\"]].head(10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows without target (last day per symbol)\n",
    "prices = prices.dropna(subset=[\"target_return\"]).copy()\n",
    "print(f\"Rows with target: {len(prices):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify alignment: target_date > date\n",
    "invalid = prices[pd.to_datetime(prices[\"target_date\"]) <= pd.to_datetime(prices[\"date\"])]\n",
    "print(f\"Invalid rows (target_date <= date): {len(invalid)} (should be 0)\")\n",
    "\n",
    "# Gap distribution\n",
    "gap = (pd.to_datetime(prices[\"target_date\"]) - pd.to_datetime(prices[\"date\"])).dt.days\n",
    "print(f\"\\nGap distribution (days):\")\n",
    "print(gap.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000007",
   "metadata": {},
   "outputs": [],
   "source": "# Target stats\nprint(\"Target return (log) stats:\")\nprint(prices[\"target_return\"].describe())"
  },
  {
   "cell_type": "markdown",
   "id": "bv9dkdux6mf",
   "source": "## Cross-sectional targets\n\nPer README section 10:\n- **Demeaned return**: `r[i,t] - mean_j r[j,t]` (for regression)\n- **Rank**: percentile rank within day (for ranking loss)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "otvty9r0j9n",
   "source": "# Cross-sectional demeaned return (for regression objective)\nprices[\"target_demean\"] = prices.groupby(\"date\")[\"target_return\"].transform(\n    lambda x: x - x.mean()\n)\n\n# Cross-sectional rank (for ranking objective), scaled to [0, 1]\nprices[\"target_rank\"] = prices.groupby(\"date\")[\"target_return\"].rank(pct=True)\n\nprint(\"Demeaned return stats:\")\nprint(prices[\"target_demean\"].describe())\nprint(\"\\nRank stats:\")\nprint(prices[\"target_rank\"].describe())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "uhdxdf84fko",
   "source": "## Same-day price features\n\nUsing close(t) as proxy for price at 15:30 ET (when model runs):\n- `overnight_gap`: reaction to overnight news\n- `intraday_ret`: same-day continuation/reversal\n\nThis helps the model know if news has already been priced in.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "zk1w803chj",
   "source": "# Same-day features (using close(t) as proxy for 15:30 price)\nprices[\"close_lag1\"] = prices.groupby(\"symbol\")[\"close\"].shift(1)\n\n# Overnight gap: open(t) vs close(t-1)\nprices[\"overnight_gap\"] = prices[\"open\"] / prices[\"close_lag1\"] - 1\n\n# Intraday return: close(t) vs open(t)\nprices[\"intraday_ret\"] = prices[\"close\"] / prices[\"open\"] - 1\n\nprint(\"Same-day feature stats:\")\nprint(prices[[\"overnight_gap\", \"intraday_ret\"]].describe())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0qfddojvl8e",
   "source": "## Historical price features\n\nShort-term returns and volatility using data up to close(t-1).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "dqtnw1powh4",
   "source": "# Short-term returns (using lagged data)\n# ret_1d: close(t-1) / close(t-2) - 1\nfor lag in [1, 2, 3, 5]:\n    prices[f\"ret_{lag}d\"] = prices.groupby(\"symbol\")[\"close_lag1\"].transform(\n        lambda x: x / x.shift(lag) - 1\n    )\n\n# Short-term volatility: std of daily returns over past 5 days\nprices[\"daily_ret\"] = prices.groupby(\"symbol\")[\"close\"].pct_change()\nprices[\"vol_5d\"] = prices.groupby(\"symbol\")[\"daily_ret\"].transform(\n    lambda x: x.shift(1).rolling(5, min_periods=3).std()\n)\n\n# Distance from 5-day high/low (using lagged data)\nprices[\"high_5d\"] = prices.groupby(\"symbol\")[\"high\"].transform(\n    lambda x: x.shift(1).rolling(5, min_periods=1).max()\n)\nprices[\"low_5d\"] = prices.groupby(\"symbol\")[\"low\"].transform(\n    lambda x: x.shift(1).rolling(5, min_periods=1).min()\n)\nprices[\"dist_from_high_5d\"] = (prices[\"close_lag1\"] - prices[\"high_5d\"]) / prices[\"high_5d\"]\nprices[\"dist_from_low_5d\"] = (prices[\"close_lag1\"] - prices[\"low_5d\"]) / prices[\"low_5d\"]\n\n# Clean up intermediate columns\nprices = prices.drop(columns=[\"close_lag1\", \"daily_ret\", \"high_5d\", \"low_5d\"])\n\nprint(\"Historical price features added\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "u0fakwxtrap",
   "source": "# Check feature stats\nprice_feature_cols = [\n    \"overnight_gap\", \"intraday_ret\",  # same-day\n    \"ret_1d\", \"ret_2d\", \"ret_3d\", \"ret_5d\", \"vol_5d\",  # historical\n    \"dist_from_high_5d\", \"dist_from_low_5d\"\n]\nprint(\"Price feature stats:\")\nprint(prices[price_feature_cols].describe())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "jzt9ye5d80e",
   "source": "## Cross-sectional normalization of price features\n\nPer README section 8: z-score within each day",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "lsa3dl0z7gj",
   "source": "def cross_sectional_zscore(df: pd.DataFrame, col: str) -> pd.Series:\n    \"\"\"Z-score within each date, with winsorization at 3 std.\"\"\"\n    grouped = df.groupby(\"date\")[col]\n    mean = grouped.transform(\"mean\")\n    std = grouped.transform(\"std\")\n    z = (df[col] - mean) / std\n    # Winsorize at +/- 3\n    return z.clip(-3, 3)\n\n# Normalize price features\nfor col in price_feature_cols:\n    prices[f\"{col}_z\"] = cross_sectional_zscore(prices, col)\n\nnormalized_cols = [f\"{col}_z\" for col in price_feature_cols]\nprint(\"Normalized feature stats:\")\nprint(prices[normalized_cols].describe())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000008",
   "metadata": {},
   "outputs": [],
   "source": "# Select columns to save\n# Rename date -> feature_date for clarity when merging later\n\nid_cols = [\"symbol\", \"date\", \"target_date\"]\ntarget_cols = [\"target_return\", \"target_demean\", \"target_rank\"]\nraw_price_cols = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\nfeature_cols = normalized_cols  # Use normalized features\n\nout = prices[id_cols + target_cols + raw_price_cols + feature_cols].copy()\nout = out.rename(columns={\"date\": \"feature_date\"})\n\nprint(f\"Output columns: {list(out.columns)}\")\nprint(f\"Rows: {len(out):,}\")\nout.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000009",
   "metadata": {},
   "outputs": [],
   "source": "# Drop rows with missing features (early dates lacking history)\nn_before = len(out)\nout = out.dropna()\nn_after = len(out)\nprint(f\"Dropped {n_before - n_after:,} rows with missing features\")\nprint(f\"Final rows: {n_after:,}\")"
  },
  {
   "cell_type": "code",
   "id": "9edxm4qdv6u",
   "source": "out.to_parquet(\"data/price_features.pqt\", index=False)\nprint(f\"Saved to data/price_features.pqt\")\nprint(f\"File size: {Path('data/price_features.pqt').stat().st_size / 1e6:.1f} MB\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "6vxxw6eep77",
   "source": "# Summary\nprint(f\"Date range: {out['feature_date'].min()} to {out['feature_date'].max()}\")\nprint(f\"Symbols: {out['symbol'].nunique():,}\")\nprint(f\"Days: {out['feature_date'].nunique():,}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}