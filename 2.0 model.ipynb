{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0000-0001-0001-000000000000",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Multi-branch neural model with influence-controlled news embedding.\n",
    "\n",
    "**Table of Contents:**\n",
    "1. Load and split data\n",
    "2. Dataset and DataLoader\n",
    "3. Model architecture\n",
    "4. Training loop\n",
    "5. Evaluation: Rank IC and Basket Returns\n",
    "   - 5.1 Long-Short Analysis\n",
    "   - 5.2 Data Quality Check\n",
    "   - 5.3 Clipped Returns Evaluation\n",
    "6. News Ablation Analysis\n",
    "\n",
    "**Architecture** (per README section 9):\n",
    "- Fundamentals encoder → h_f (32 dims)\n",
    "- Price encoder → h_p (16 dims)\n",
    "- News encoder → h_n (32 dims)\n",
    "- Fusion: h = concat([h_f, h_p, α * h_n]), α=0.3\n",
    "- Output: scalar score for ranking\n",
    "\n",
    "**Training objective** (per README section 10):\n",
    "- Pairwise ranking loss within each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1b2c3d4-0001-0001-0001-000000000001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b2c3d4-0001-0001-0001-000000000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelConfig:\n",
    "    # Feature dimensions (set after loading data)\n",
    "    n_fundamental_features: int = 19\n",
    "    n_price_features: int = 9\n",
    "    n_embedding_dim: int = 768\n",
    "    \n",
    "    # Encoder latent dimensions\n",
    "    fundamental_latent: int = 32\n",
    "    price_latent: int = 16\n",
    "    news_latent: int = 32\n",
    "    \n",
    "    # Dropout\n",
    "    fundamental_dropout: float = 0.2\n",
    "    price_dropout: float = 0.2\n",
    "    news_dropout: float = 0.3\n",
    "    \n",
    "    # News influence cap\n",
    "    news_alpha: float = 0.7\n",
    "    \n",
    "    # Training\n",
    "    batch_size: int = 512\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-3\n",
    "    n_epochs: int = 20\n",
    "    pairs_per_day: int = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000003",
   "metadata": {},
   "source": [
    "## 1. Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1b2c3d4-0001-0001-0001-000000000004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 2,092,929 rows\n",
      "Date range: 2021-01-13 to 2025-12-18\n",
      "Symbols: 3,506\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"data/ml_dataset.pqt\")\n",
    "df[\"feature_date\"] = pd.to_datetime(df[\"feature_date\"])\n",
    "\n",
    "print(f\"Dataset: {len(df):,} rows\")\n",
    "print(f\"Date range: {df['feature_date'].min().date()} to {df['feature_date'].max().date()}\")\n",
    "print(f\"Symbols: {df['symbol'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b2c3d4-0001-0001-0001-000000000005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price features: 9\n",
      "Fundamental features: 19\n",
      "Embedding dims: 768\n"
     ]
    }
   ],
   "source": [
    "# Identify feature columns\n",
    "price_feat_cols = [\n",
    "    \"overnight_gap_z\", \"intraday_ret_z\",\n",
    "    \"ret_1d_z\", \"ret_2d_z\", \"ret_3d_z\", \"ret_5d_z\",\n",
    "    \"vol_5d_z\", \"dist_from_high_5d_z\", \"dist_from_low_5d_z\"\n",
    "]\n",
    "\n",
    "# Fundamental feature columns (normalized)\n",
    "fund_feat_cols = [c for c in df.columns if c.endswith(\"_z\") and c not in price_feat_cols and c != \"news_count_z\"]\n",
    "\n",
    "# Embedding columns\n",
    "emb_cols = [c for c in df.columns if c.startswith(\"emb_\")]\n",
    "\n",
    "print(f\"Price features: {len(price_feat_cols)}\")\n",
    "print(f\"Fundamental features: {len(fund_feat_cols)}\")\n",
    "print(f\"Embedding dims: {len(emb_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b2c3d4-0001-0001-0001-000000000006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1,418,494 rows, 830 days\n",
      "Val: 322,222 rows, 178 days\n",
      "Test: 352,213 rows, 178 days\n"
     ]
    }
   ],
   "source": [
    "# Time-based split (per README section 13)\n",
    "# Use last 20% of dates as test\n",
    "dates = sorted(df[\"feature_date\"].unique())\n",
    "n_dates = len(dates)\n",
    "train_end_idx = int(n_dates * 0.7)\n",
    "val_end_idx = int(n_dates * 0.85)\n",
    "\n",
    "train_dates = set(dates[:train_end_idx])\n",
    "val_dates = set(dates[train_end_idx:val_end_idx])\n",
    "test_dates = set(dates[val_end_idx:])\n",
    "\n",
    "train_df = df[df[\"feature_date\"].isin(train_dates)].copy()\n",
    "val_df = df[df[\"feature_date\"].isin(val_dates)].copy()\n",
    "test_df = df[df[\"feature_date\"].isin(test_dates)].copy()\n",
    "\n",
    "print(f\"Train: {len(train_df):,} rows, {len(train_dates)} days\")\n",
    "print(f\"Val: {len(val_df):,} rows, {len(val_dates)} days\")\n",
    "print(f\"Test: {len(test_df):,} rows, {len(test_dates)} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000007",
   "metadata": {},
   "source": [
    "## 2. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1b2c3d4-0001-0001-0001-000000000008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairwiseRankingDataset(Dataset):\n",
    "    \"\"\"Dataset that generates hard pairs from same day for ranking loss.\n",
    "    \n",
    "    Only uses rows with news, and samples from top vs bottom quartiles\n",
    "    for harder training signal.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        price_cols: list[str],\n",
    "        fund_cols: list[str],\n",
    "        emb_cols: list[str],\n",
    "        pairs_per_day: int = 100,\n",
    "        hard_fraction: float = 0.7,  # Mix of hard vs random pairs\n",
    "    ):\n",
    "        # Filter to rows with news only\n",
    "        has_news = (df[emb_cols].abs().sum(axis=1) > 0)\n",
    "        df_news = df[has_news].copy()\n",
    "        print(f\"Filtered to news-only: {len(df_news):,} / {len(df):,} rows ({len(df_news)/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        self.df = df_news.reset_index(drop=True)\n",
    "        self.price_cols = price_cols\n",
    "        self.fund_cols = fund_cols\n",
    "        self.emb_cols = emb_cols\n",
    "        self.pairs_per_day = pairs_per_day\n",
    "        self.hard_fraction = hard_fraction\n",
    "        \n",
    "        # Group by date and precompute quartile indices\n",
    "        self.date_groups = {}\n",
    "        for date, group in df_news.groupby(\"feature_date\"):\n",
    "            indices = group.index.tolist()\n",
    "            if len(indices) < 4:  # Need at least 4 for quartiles\n",
    "                continue\n",
    "            \n",
    "            # Sort by target return and get quartile indices\n",
    "            sorted_idx = group[\"target_return\"].sort_values().index.tolist()\n",
    "            q_size = len(sorted_idx) // 4\n",
    "            if q_size < 1:\n",
    "                continue\n",
    "            \n",
    "            bottom_q = sorted_idx[:q_size]  # Losers\n",
    "            top_q = sorted_idx[-q_size:]    # Winners\n",
    "            \n",
    "            self.date_groups[date] = {\n",
    "                \"all\": indices,\n",
    "                \"top\": top_q,\n",
    "                \"bottom\": bottom_q,\n",
    "            }\n",
    "        \n",
    "        self.dates = list(self.date_groups.keys())\n",
    "        print(f\"Days with sufficient news coverage: {len(self.dates)}\")\n",
    "        \n",
    "        # Precompute arrays for speed\n",
    "        self.price_arr = df_news[price_cols].values.astype(np.float32)\n",
    "        self.fund_arr = df_news[fund_cols].values.astype(np.float32)\n",
    "        self.emb_arr = df_news[emb_cols].values.astype(np.float32)\n",
    "        self.target_arr = df_news[\"target_return\"].values.astype(np.float32)\n",
    "        \n",
    "        # Map original index to position in filtered df\n",
    "        self.idx_map = {old_idx: new_idx for new_idx, old_idx in enumerate(df_news.index)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dates) * self.pairs_per_day\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get date\n",
    "        date_idx = idx // self.pairs_per_day\n",
    "        date = self.dates[date_idx]\n",
    "        groups = self.date_groups[date]\n",
    "        \n",
    "        # Decide: hard pair (top vs bottom) or random pair\n",
    "        use_hard = np.random.random() < self.hard_fraction\n",
    "        \n",
    "        if use_hard:\n",
    "            # Sample one from top quartile, one from bottom quartile\n",
    "            winner_orig = np.random.choice(groups[\"top\"])\n",
    "            loser_orig = np.random.choice(groups[\"bottom\"])\n",
    "        else:\n",
    "            # Random pair from all news rows that day\n",
    "            if len(groups[\"all\"]) < 2:\n",
    "                winner_orig = loser_orig = groups[\"all\"][0]\n",
    "            else:\n",
    "                winner_orig, loser_orig = np.random.choice(groups[\"all\"], size=2, replace=False)\n",
    "        \n",
    "        # Map to filtered array indices\n",
    "        i = self.idx_map[winner_orig]\n",
    "        j = self.idx_map[loser_orig]\n",
    "        \n",
    "        # Get features\n",
    "        price_i = self.price_arr[i]\n",
    "        price_j = self.price_arr[j]\n",
    "        fund_i = self.fund_arr[i]\n",
    "        fund_j = self.fund_arr[j]\n",
    "        emb_i = self.emb_arr[i]\n",
    "        emb_j = self.emb_arr[j]\n",
    "        \n",
    "        # Label based on actual returns\n",
    "        actual_label = 1.0 if self.target_arr[i] > self.target_arr[j] else 0.0\n",
    "        \n",
    "        # Randomly swap to balance\n",
    "        if np.random.random() < 0.5:\n",
    "            price_i, price_j = price_j, price_i\n",
    "            fund_i, fund_j = fund_j, fund_i\n",
    "            emb_i, emb_j = emb_j, emb_i\n",
    "            label = 1.0 - actual_label\n",
    "        else:\n",
    "            label = actual_label\n",
    "        \n",
    "        return {\n",
    "            \"price_i\": torch.tensor(price_i),\n",
    "            \"price_j\": torch.tensor(price_j),\n",
    "            \"fund_i\": torch.tensor(fund_i),\n",
    "            \"fund_j\": torch.tensor(fund_j),\n",
    "            \"emb_i\": torch.tensor(emb_i),\n",
    "            \"emb_j\": torch.tensor(emb_j),\n",
    "            \"label\": torch.tensor(label),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b2c3d4-0001-0001-0001-000000000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointwiseDataset(Dataset):\n",
    "    \"\"\"Dataset for inference - one sample per stock-day.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        price_cols: list[str],\n",
    "        fund_cols: list[str],\n",
    "        emb_cols: list[str],\n",
    "    ):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.price_arr = df[price_cols].values.astype(np.float32)\n",
    "        self.fund_arr = df[fund_cols].values.astype(np.float32)\n",
    "        self.emb_arr = df[emb_cols].values.astype(np.float32)\n",
    "        self.target_arr = df[\"target_return\"].values.astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"price\": torch.tensor(self.price_arr[idx]),\n",
    "            \"fund\": torch.tensor(self.fund_arr[idx]),\n",
    "            \"emb\": torch.tensor(self.emb_arr[idx]),\n",
    "            \"target\": torch.tensor(self.target_arr[idx]),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b2c3d4-0001-0001-0001-000000000010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: ModelConfig(n_fundamental_features=19, n_price_features=9, n_embedding_dim=768, fundamental_latent=32, price_latent=16, news_latent=32, fundamental_dropout=0.2, price_dropout=0.2, news_dropout=0.3, news_alpha=0.7, batch_size=512, learning_rate=0.001, weight_decay=0.001, n_epochs=20, pairs_per_day=5000)\n"
     ]
    }
   ],
   "source": [
    "# Create config with actual dimensions\n",
    "config = ModelConfig(\n",
    "    n_fundamental_features=len(fund_feat_cols),\n",
    "    n_price_features=len(price_feat_cols),\n",
    "    n_embedding_dim=len(emb_cols),\n",
    ")\n",
    "\n",
    "print(f\"Config: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1b2c3d4-0001-0001-0001-000000000011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to news-only: 85,952 / 1,418,494 rows (6.1%)\n",
      "Days with sufficient news coverage: 830\n",
      "Filtered to news-only: 22,098 / 322,222 rows (6.9%)\n",
      "Days with sufficient news coverage: 178\n",
      "Train pairs: 4,150,000\n",
      "Val pairs: 890,000\n"
     ]
    }
   ],
   "source": [
    "# Create datasets (both use same hard pair ratio for consistent distribution)\n",
    "train_dataset = PairwiseRankingDataset(\n",
    "    train_df, price_feat_cols, fund_feat_cols, emb_cols,\n",
    "    pairs_per_day=config.pairs_per_day,\n",
    "    hard_fraction=0.7,\n",
    ")\n",
    "val_dataset = PairwiseRankingDataset(\n",
    "    val_df, price_feat_cols, fund_feat_cols, emb_cols,\n",
    "    pairs_per_day=config.pairs_per_day,\n",
    "    hard_fraction=0.7,  # Same distribution as training\n",
    ")\n",
    "\n",
    "# For evaluation (pointwise)\n",
    "val_pointwise = PointwiseDataset(val_df, price_feat_cols, fund_feat_cols, emb_cols)\n",
    "test_pointwise = PointwiseDataset(test_df, price_feat_cols, fund_feat_cols, emb_cols)\n",
    "\n",
    "print(f\"Train pairs: {len(train_dataset):,}\")\n",
    "print(f\"Val pairs: {len(val_dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1b2c3d4-0001-0001-0001-000000000012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "# num_workers=0 for Jupyter compatibility (avoids multiprocessing issues)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000013",
   "metadata": {},
   "source": [
    "## 3. Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1b2c3d4-0001-0001-0001-000000000014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBranchRanker(nn.Module):\n",
    "    \"\"\"Multi-branch model with influence-controlled news embedding.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Fundamentals encoder\n",
    "        self.fund_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_fundamental_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.fundamental_dropout),\n",
    "            nn.Linear(64, config.fundamental_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Price encoder\n",
    "        self.price_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_price_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.price_dropout),\n",
    "            nn.Linear(32, config.price_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # News encoder\n",
    "        self.news_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.news_dropout),\n",
    "            nn.Linear(128, config.news_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Output head\n",
    "        fused_dim = config.fundamental_latent + config.price_latent + config.news_latent\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "    \n",
    "    def encode(self, price: torch.Tensor, fund: torch.Tensor, emb: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Encode features and return fused representation.\"\"\"\n",
    "        h_f = self.fund_encoder(fund)\n",
    "        h_p = self.price_encoder(price)\n",
    "        h_n = self.news_encoder(emb)\n",
    "        \n",
    "        # Apply news influence cap (α * h_n)\n",
    "        h_n_scaled = self.config.news_alpha * h_n\n",
    "        \n",
    "        # Fuse\n",
    "        h = torch.cat([h_f, h_p, h_n_scaled], dim=-1)\n",
    "        return h\n",
    "    \n",
    "    def forward(self, price: torch.Tensor, fund: torch.Tensor, emb: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass returning score.\"\"\"\n",
    "        h = self.encode(price, fund, emb)\n",
    "        score = self.output_head(h)\n",
    "        return score.squeeze(-1)\n",
    "    \n",
    "    def forward_pair(\n",
    "        self,\n",
    "        price_i: torch.Tensor, fund_i: torch.Tensor, emb_i: torch.Tensor,\n",
    "        price_j: torch.Tensor, fund_j: torch.Tensor, emb_j: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass for pair, returning P(i > j).\"\"\"\n",
    "        score_i = self.forward(price_i, fund_i, emb_i)\n",
    "        score_j = self.forward(price_j, fund_j, emb_j)\n",
    "        return torch.sigmoid(score_i - score_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1b2c3d4-0001-0001-0001-000000000015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 109,393\n",
      "MultiBranchRanker(\n",
      "  (fund_encoder): Sequential(\n",
      "    (0): Linear(in_features=19, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (price_encoder): Sequential(\n",
      "    (0): Linear(in_features=9, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (news_encoder): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (output_head): Sequential(\n",
      "    (0): Linear(in_features=80, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MultiBranchRanker(config).to(device)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params:,}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000016",
   "metadata": {},
   "source": [
    "## 4. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1b2c3d4-0001-0001-0001-000000000017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_ranking_loss(pred_prob: torch.Tensor, label: torch.Tensor, smoothing: float = 0.2) -> torch.Tensor:\n",
    "    \"\"\"Binary cross-entropy for pairwise ranking with label smoothing.\"\"\"\n",
    "    # Smooth labels: 0 -> smoothing/2, 1 -> 1 - smoothing/2\n",
    "    smoothed_label = label * (1 - smoothing) + 0.5 * smoothing\n",
    "    return F.binary_cross_entropy(pred_prob, smoothed_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1b2c3d4-0001-0001-0001-000000000018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        price_i = batch[\"price_i\"].to(device)\n",
    "        price_j = batch[\"price_j\"].to(device)\n",
    "        fund_i = batch[\"fund_i\"].to(device)\n",
    "        fund_j = batch[\"fund_j\"].to(device)\n",
    "        emb_i = batch[\"emb_i\"].to(device)\n",
    "        emb_j = batch[\"emb_j\"].to(device)\n",
    "        label = batch[\"label\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred_prob = model.forward_pair(price_i, fund_i, emb_i, price_j, fund_j, emb_j)\n",
    "        loss = pairwise_ranking_loss(pred_prob, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(label)\n",
    "        total_correct += ((pred_prob > 0.5) == (label > 0.5)).sum().item()\n",
    "        total_samples += len(label)\n",
    "    \n",
    "    return total_loss / total_samples, total_correct / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1b2c3d4-0001-0001-0001-000000000019",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        price_i = batch[\"price_i\"].to(device)\n",
    "        price_j = batch[\"price_j\"].to(device)\n",
    "        fund_i = batch[\"fund_i\"].to(device)\n",
    "        fund_j = batch[\"fund_j\"].to(device)\n",
    "        emb_i = batch[\"emb_i\"].to(device)\n",
    "        emb_j = batch[\"emb_j\"].to(device)\n",
    "        label = batch[\"label\"].to(device)\n",
    "        \n",
    "        pred_prob = model.forward_pair(price_i, fund_i, emb_i, price_j, fund_j, emb_j)\n",
    "        loss = pairwise_ranking_loss(pred_prob, label)\n",
    "        \n",
    "        total_loss += loss.item() * len(label)\n",
    "        total_correct += ((pred_prob > 0.5) == (label > 0.5)).sum().item()\n",
    "        total_samples += len(label)\n",
    "    \n",
    "    return total_loss / total_samples, total_correct / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1b2c3d4-0001-0001-0001-000000000020",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config.n_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1b2c3d4-0001-0001-0001-000000000021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/8106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: train_loss=0.6068, train_acc=0.7044, val_loss=0.7881, val_acc=0.5016\n",
      "  -> New best model saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/8106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: train_loss=0.5404, train_acc=0.7975, val_loss=0.8077, val_acc=0.5021\n",
      "  -> New best model saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/8106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: train_loss=0.5239, train_acc=0.8168, val_loss=0.8331, val_acc=0.5038\n",
      "  -> New best model saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/8106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: train_loss=0.5149, train_acc=0.8272, val_loss=0.8339, val_acc=0.4996\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/8106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: train_loss=0.5078, train_acc=0.8351, val_loss=0.8351, val_acc=0.5023\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/8106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: train_loss=0.5030, train_acc=0.8404, val_loss=0.8374, val_acc=0.5035\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/8106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: train_loss=0.4984, train_acc=0.8454, val_loss=0.8407, val_acc=0.5030\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/8106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: train_loss=0.4946, train_acc=0.8495, val_loss=0.8494, val_acc=0.5032\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/8106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: train_loss=0.4916, train_acc=0.8526, val_loss=0.8475, val_acc=0.5007\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/8106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: train_loss=0.4886, train_acc=0.8556, val_loss=0.8491, val_acc=0.5019\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/8106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1739 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: train_loss=0.4862, train_acc=0.8583, val_loss=0.8477, val_acc=0.5031\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace54316e9254ffa9e59394b8bd7fd01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/8106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m history \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mn_epochs):\n\u001b[0;32m----> 6\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m eval_epoch(model, val_loader, device)\n\u001b[1;32m      8\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m total_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      8\u001b[0m     price_i \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_i\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m     price_j \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_j\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    738\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:788\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    787\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 788\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    790\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[6], line 116\u001b[0m, in \u001b[0;36mPairwiseRankingDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     label \u001b[38;5;241m=\u001b[39m actual_label\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_i\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(price_i),\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_j\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(price_j),\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfund_i\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(fund_i),\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfund_j\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(fund_j),\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memb_i\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(emb_i),\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memb_j\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb_j\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(label),\n\u001b[1;32m    118\u001b[0m }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "best_val_acc = 0\n",
    "history = []\n",
    "\n",
    "for epoch in range(config.n_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss, val_acc = eval_epoch(model, val_loader, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    history.append({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "    })\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{config.n_epochs}: \"\n",
    "          f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, \"\n",
    "          f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"data/model_best.pt\")\n",
    "        print(f\"  -> New best model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000022",
   "metadata": {},
   "source": [
    "## 5. Evaluation: Rank IC and Basket Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1b2c3d4-0001-0001-0001-000000000023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiBranchRanker(\n",
       "  (fund_encoder): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       "  (price_encoder): Sequential(\n",
       "    (0): Linear(in_features=9, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       "  (news_encoder): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       "  (output_head): Sequential(\n",
       "    (0): Linear(in_features=80, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"data/model_best.pt\", weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1b2c3d4-0001-0001-0001-000000000024",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_scores(model, df, price_cols, fund_cols, emb_cols, device, batch_size=1024):\n",
    "    \"\"\"Get model scores for all rows.\"\"\"\n",
    "    model.eval()\n",
    "    dataset = PointwiseDataset(df, price_cols, fund_cols, emb_cols)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    scores = []\n",
    "    for batch in tqdm(loader, desc=\"Scoring\", leave=False):\n",
    "        price = batch[\"price\"].to(device)\n",
    "        fund = batch[\"fund\"].to(device)\n",
    "        emb = batch[\"emb\"].to(device)\n",
    "        \n",
    "        score = model(price, fund, emb)\n",
    "        scores.append(score.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1b2c3d4-0001-0001-0001-000000000025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 352,213 test samples\n"
     ]
    }
   ],
   "source": [
    "# Score test set\n",
    "test_df = test_df.copy()\n",
    "test_df[\"score\"] = get_scores(model, test_df, price_feat_cols, fund_feat_cols, emb_cols, device)\n",
    "\n",
    "print(f\"Scored {len(test_df):,} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1b2c3d4-0001-0001-0001-000000000026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def compute_daily_ic(df):\n",
    "    \"\"\"Compute Spearman rank IC per day.\"\"\"\n",
    "    ics = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < 10:\n",
    "            continue\n",
    "        ic, _ = spearmanr(group[\"score\"], group[\"target_return\"])\n",
    "        if not np.isnan(ic):\n",
    "            ics.append({\"date\": date, \"ic\": ic})\n",
    "    return pd.DataFrame(ics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1b2c3d4-0001-0001-0001-000000000027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Rank IC (Spearman):\n",
      "  Mean: 0.0171\n",
      "  Std: 0.1017\n",
      "  Sharpe (IC): 2.66\n",
      "  % Positive: 54.5%\n"
     ]
    }
   ],
   "source": [
    "ic_df = compute_daily_ic(test_df)\n",
    "\n",
    "print(f\"Daily Rank IC (Spearman):\")\n",
    "print(f\"  Mean: {ic_df['ic'].mean():.4f}\")\n",
    "print(f\"  Std: {ic_df['ic'].std():.4f}\")\n",
    "print(f\"  Sharpe (IC): {ic_df['ic'].mean() / ic_df['ic'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"  % Positive: {(ic_df['ic'] > 0).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1b2c3d4-0001-0001-0001-000000000028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_basket_returns(df, top_k=20):\n",
    "    \"\"\"Compute daily returns of top-K basket (equal weight).\"\"\"\n",
    "    returns = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < top_k:\n",
    "            continue\n",
    "        # Select top-K by score\n",
    "        top = group.nlargest(top_k, \"score\")\n",
    "        # Equal weight return\n",
    "        ret = top[\"target_return\"].mean()\n",
    "        # Market return (all stocks)\n",
    "        mkt_ret = group[\"target_return\"].mean()\n",
    "        returns.append({\n",
    "            \"date\": date,\n",
    "            \"basket_return\": ret,\n",
    "            \"market_return\": mkt_ret,\n",
    "            \"excess_return\": ret - mkt_ret,\n",
    "        })\n",
    "    return pd.DataFrame(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1b2c3d4-0001-0001-0001-000000000029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-20 Basket Performance (Test Set):\n",
      "  Daily mean return: -0.157%\n",
      "  Daily mean excess: -0.013%\n",
      "  Sharpe (basket): -1.35\n",
      "  Sharpe (excess): -0.19\n"
     ]
    }
   ],
   "source": [
    "basket_df = compute_basket_returns(test_df, top_k=20)\n",
    "\n",
    "print(f\"\\nTop-20 Basket Performance (Test Set):\")\n",
    "print(f\"  Daily mean return: {basket_df['basket_return'].mean()*100:.3f}%\")\n",
    "print(f\"  Daily mean excess: {basket_df['excess_return'].mean()*100:.3f}%\")\n",
    "print(f\"  Sharpe (basket): {basket_df['basket_return'].mean() / basket_df['basket_return'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"  Sharpe (excess): {basket_df['excess_return'].mean() / basket_df['excess_return'].std() * np.sqrt(252):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1b2c3d4-0001-0001-0001-000000000030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cumulative Returns:\n",
      "  Basket: -26.7%\n",
      "  Market: -23.7%\n",
      "  Excess: -3.3%\n"
     ]
    }
   ],
   "source": [
    "# Cumulative returns\n",
    "basket_df[\"cum_basket\"] = (1 + basket_df[\"basket_return\"]).cumprod()\n",
    "basket_df[\"cum_market\"] = (1 + basket_df[\"market_return\"]).cumprod()\n",
    "basket_df[\"cum_excess\"] = (1 + basket_df[\"excess_return\"]).cumprod()\n",
    "\n",
    "print(f\"\\nCumulative Returns:\")\n",
    "print(f\"  Basket: {(basket_df['cum_basket'].iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"  Market: {(basket_df['cum_market'].iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"  Excess: {(basket_df['cum_excess'].iloc[-1] - 1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "o5vcmhiugh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long-Short Portfolio Performance (Test Set):\n",
      "\n",
      "  Long (top-20):\n",
      "    Daily mean: -0.157%\n",
      "    Cumulative: -26.7%\n",
      "\n",
      "  Short (bottom-20):\n",
      "    Daily mean: -1.543%\n",
      "    Cumulative: -94.6%\n",
      "    Short P&L (inverted): 1237.0%\n",
      "\n",
      "  Long-Short (market neutral):\n",
      "    Daily mean: 1.386%\n",
      "    Std: 3.558%\n",
      "    Sharpe: 6.18\n",
      "    Cumulative: 939.8%\n",
      "\n",
      "  Market:\n",
      "    Cumulative: -23.7%\n"
     ]
    }
   ],
   "source": [
    "# Long-short analysis: long top-K, short bottom-K\n",
    "def compute_long_short_returns(df, top_k=20):\n",
    "    \"\"\"Compute daily returns of long-short portfolio.\"\"\"\n",
    "    returns = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < top_k * 2:\n",
    "            continue\n",
    "        # Long top-K\n",
    "        top = group.nlargest(top_k, \"score\")\n",
    "        long_ret = top[\"target_return\"].mean()\n",
    "        # Short bottom-K\n",
    "        bottom = group.nsmallest(top_k, \"score\")\n",
    "        short_ret = bottom[\"target_return\"].mean()\n",
    "        # Long-short return (long winners, short losers)\n",
    "        ls_ret = long_ret - short_ret\n",
    "        # Market\n",
    "        mkt_ret = group[\"target_return\"].mean()\n",
    "        returns.append({\n",
    "            \"date\": date,\n",
    "            \"long_return\": long_ret,\n",
    "            \"short_return\": short_ret,\n",
    "            \"long_short_return\": ls_ret,\n",
    "            \"market_return\": mkt_ret,\n",
    "        })\n",
    "    return pd.DataFrame(returns)\n",
    "\n",
    "ls_df = compute_long_short_returns(test_df, top_k=20)\n",
    "\n",
    "print(\"Long-Short Portfolio Performance (Test Set):\")\n",
    "print(f\"\\n  Long (top-20):\")\n",
    "print(f\"    Daily mean: {ls_df['long_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Cumulative: {((1 + ls_df['long_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Short (bottom-20):\")\n",
    "print(f\"    Daily mean: {ls_df['short_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Cumulative: {((1 + ls_df['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"    Short P&L (inverted): {((1 - ls_df['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Long-Short (market neutral):\")\n",
    "print(f\"    Daily mean: {ls_df['long_short_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Std: {ls_df['long_short_return'].std()*100:.3f}%\")\n",
    "print(f\"    Sharpe: {ls_df['long_short_return'].mean() / ls_df['long_short_return'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"    Cumulative: {((1 + ls_df['long_short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Market:\")\n",
    "print(f\"    Cumulative: {((1 + ls_df['market_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "av5cknegqmf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom-20 basket analysis:\n",
      "==================================================\n",
      "\n",
      "Most frequent bottom-20 stocks:\n",
      "symbol\n",
      "QUBT    55\n",
      "NRIX    48\n",
      "FIG     47\n",
      "KOF     44\n",
      "ALNY    36\n",
      "OKLO    35\n",
      "JACK    33\n",
      "NBIS    32\n",
      "UI      31\n",
      "HTZ     31\n",
      "SERV    30\n",
      "MLGO    29\n",
      "DXYZ    26\n",
      "RAY     24\n",
      "ABTC    23\n",
      "MRUS    22\n",
      "HCWB    22\n",
      "NEGG    22\n",
      "DGNX    21\n",
      "FTAI    21\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Worst single-day returns in bottom basket:\n",
      "        feature_date symbol  target_return\n",
      "1793247   2025-03-31   GDHG      -2.161439\n",
      "2088179   2025-12-16   MENS      -1.662784\n",
      "1746731   2025-02-14   INLF      -1.572174\n",
      "1799271   2025-04-04   SUNE      -1.463255\n",
      "1906158   2025-07-17    YHC      -1.015413\n",
      "1921292   2025-08-05   ELWS      -0.895174\n",
      "2079979   2025-12-05    SMX      -0.893744\n",
      "1952070   2025-08-26    RAY      -0.887303\n",
      "1804732   2025-04-17   AREB      -0.769643\n",
      "1754490   2025-02-21   PPCB      -0.769074\n",
      "2019382   2025-10-14   AQMS      -0.753687\n",
      "2001078   2025-10-01   NVNI      -0.749597\n",
      "1801380   2025-04-15   SUNE      -0.738043\n",
      "1865197   2025-06-12    KWM      -0.705144\n",
      "1970499   2025-09-09   QMMM      -0.636794\n",
      "2071511   2025-11-19   CMCT      -0.627057\n",
      "1799476   2025-04-04   AREB      -0.608618\n",
      "1883407   2025-06-30   SONM      -0.584513\n",
      "2086614   2025-12-15   RADX      -0.573589\n",
      "1825681   2025-05-09   NVVE      -0.565026\n",
      "\n",
      "\n",
      "Return distribution of bottom-20 picks:\n",
      "count    3560.000000\n",
      "mean       -0.015429\n",
      "std         0.139892\n",
      "min        -2.161439\n",
      "25%        -0.051346\n",
      "50%        -0.008242\n",
      "75%         0.027207\n",
      "max         2.910574\n",
      "Name: target_return, dtype: float64\n",
      "\n",
      "\n",
      "Days with extreme short returns (< -10%):\n",
      "Number of extreme days: 5\n",
      "          date  short_return  long_return  long_short_return\n",
      "3   2025-02-14     -0.134601     0.004039           0.138641\n",
      "27  2025-03-31     -0.123888     0.003612           0.127500\n",
      "30  2025-04-04     -0.107224    -0.013600           0.093623\n",
      "142 2025-10-14     -0.106261     0.009988           0.116249\n",
      "175 2025-12-16     -0.118620    -0.002967           0.115653\n"
     ]
    }
   ],
   "source": [
    "# Investigate the short basket - are these real shortable stocks or distressed junk?\n",
    "print(\"Bottom-20 basket analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get all bottom-20 selections\n",
    "bottom_picks = []\n",
    "for date, group in test_df.groupby(\"feature_date\"):\n",
    "    if len(group) < 40:\n",
    "        continue\n",
    "    bottom = group.nsmallest(20, \"score\")\n",
    "    bottom_picks.append(bottom[[\"symbol\", \"feature_date\", \"target_return\", \"score\"]])\n",
    "\n",
    "bottom_df = pd.concat(bottom_picks)\n",
    "\n",
    "print(f\"\\nMost frequent bottom-20 stocks:\")\n",
    "freq = bottom_df[\"symbol\"].value_counts().head(20)\n",
    "print(freq)\n",
    "\n",
    "print(f\"\\n\\nWorst single-day returns in bottom basket:\")\n",
    "print(bottom_df.nsmallest(20, \"target_return\")[[\"feature_date\", \"symbol\", \"target_return\"]])\n",
    "\n",
    "print(f\"\\n\\nReturn distribution of bottom-20 picks:\")\n",
    "print(bottom_df[\"target_return\"].describe())\n",
    "\n",
    "# Check for extreme outliers driving the result\n",
    "print(f\"\\n\\nDays with extreme short returns (< -10%):\")\n",
    "extreme_days = ls_df[ls_df[\"short_return\"] < -0.10]\n",
    "print(f\"Number of extreme days: {len(extreme_days)}\")\n",
    "if len(extreme_days) > 0:\n",
    "    print(extreme_days[[\"date\", \"short_return\", \"long_return\", \"long_short_return\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eltrur3kw8q",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality check:\n",
      "==================================================\n",
      "\n",
      "Rows with impossible returns (< -100%): 28\n",
      "        symbol feature_date  target_return\n",
      "1743903   PPCB   2025-02-12      -1.287354\n",
      "1746731   INLF   2025-02-14      -1.572174\n",
      "1764290   ACON   2025-03-05      -1.474477\n",
      "1767070    HIT   2025-03-11      -1.731924\n",
      "1793247   GDHG   2025-03-31      -2.161439\n",
      "1799271   SUNE   2025-04-04      -1.463255\n",
      "1863749   SBET   2025-06-12      -1.261873\n",
      "1877320    OST   2025-06-25      -2.797281\n",
      "1888515   SKBL   2025-07-03      -2.085914\n",
      "1904441   PTNM   2025-07-17      -1.132462\n",
      "1906158    YHC   2025-07-17      -1.015413\n",
      "1910322   PTHL   2025-07-28      -2.931921\n",
      "1922868    SMX   2025-08-06      -2.019265\n",
      "1935021   FLYE   2025-08-14      -2.048982\n",
      "1951089   TRIB   2025-08-26      -1.602658\n",
      "1970120   EPSM   2025-09-09      -1.228176\n",
      "1993179    SDM   2025-09-25      -1.995619\n",
      "1995860   MLTX   2025-09-26      -2.294392\n",
      "1999569    UFG   2025-10-01      -1.245843\n",
      "2006898   YYAI   2025-10-06      -2.635081\n",
      "\n",
      "Rows with extreme positive returns (> 100%): 12\n",
      "        symbol feature_date  target_return\n",
      "1752155   MLGO   2025-02-20       1.710266\n",
      "1781585   MLGO   2025-03-21       1.714084\n",
      "1796471   AREB   2025-04-03       1.594170\n",
      "1866290    RGC   2025-06-13       1.343235\n",
      "1921629    SMX   2025-08-05       1.802700\n",
      "1929360   ETHZ   2025-08-11       1.120331\n",
      "1966571   EPSM   2025-09-08       1.632158\n",
      "1968035   QMMM   2025-09-08       2.910574\n",
      "1990613   QURE   2025-09-23       1.246258\n",
      "2016467   AQMS   2025-10-13       1.094905\n"
     ]
    }
   ],
   "source": [
    "# Check for data quality issues - returns < -100% are impossible\n",
    "print(\"Data quality check:\")\n",
    "print(\"=\" * 50)\n",
    "impossible_returns = test_df[test_df[\"target_return\"] < -1.0]\n",
    "print(f\"\\nRows with impossible returns (< -100%): {len(impossible_returns)}\")\n",
    "if len(impossible_returns) > 0:\n",
    "    print(impossible_returns[[\"symbol\", \"feature_date\", \"target_return\"]].head(20))\n",
    "\n",
    "# Also check extreme positive returns\n",
    "extreme_positive = test_df[test_df[\"target_return\"] > 1.0]\n",
    "print(f\"\\nRows with extreme positive returns (> 100%): {len(extreme_positive)}\")\n",
    "if len(extreme_positive) > 0:\n",
    "    print(extreme_positive[[\"symbol\", \"feature_date\", \"target_return\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4qzxhrlc0yq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long-Short with clipped returns (max ±50%):\n",
      "==================================================\n",
      "\n",
      "  Long (top-20):\n",
      "    Cumulative: -28.4%\n",
      "\n",
      "  Short (bottom-20):\n",
      "    Cumulative: -93.4%\n",
      "    Short P&L: 1099.8%\n",
      "\n",
      "  Long-Short:\n",
      "    Daily mean: 1.289%\n",
      "    Sharpe: 7.24\n",
      "    Cumulative: 812.6%\n"
     ]
    }
   ],
   "source": [
    "# Re-evaluate with winsorized returns (clip extreme values)\n",
    "def compute_long_short_returns_clipped(df, top_k=20, clip_pct=0.50):\n",
    "    \"\"\"Compute long-short returns with clipped extreme returns.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"target_return_clipped\"] = df[\"target_return\"].clip(-clip_pct, clip_pct)\n",
    "    \n",
    "    returns = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < top_k * 2:\n",
    "            continue\n",
    "        # Use original score for ranking, clipped returns for P&L\n",
    "        top = group.nlargest(top_k, \"score\")\n",
    "        long_ret = top[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        bottom = group.nsmallest(top_k, \"score\")\n",
    "        short_ret = bottom[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        ls_ret = long_ret - short_ret\n",
    "        mkt_ret = group[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        returns.append({\n",
    "            \"date\": date,\n",
    "            \"long_return\": long_ret,\n",
    "            \"short_return\": short_ret,\n",
    "            \"long_short_return\": ls_ret,\n",
    "            \"market_return\": mkt_ret,\n",
    "        })\n",
    "    return pd.DataFrame(returns)\n",
    "\n",
    "ls_clipped = compute_long_short_returns_clipped(test_df, top_k=20, clip_pct=0.50)\n",
    "\n",
    "print(\"Long-Short with clipped returns (max ±50%):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n  Long (top-20):\")\n",
    "print(f\"    Cumulative: {((1 + ls_clipped['long_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Short (bottom-20):\")\n",
    "print(f\"    Cumulative: {((1 + ls_clipped['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"    Short P&L: {((1 - ls_clipped['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Long-Short:\")\n",
    "print(f\"    Daily mean: {ls_clipped['long_short_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Sharpe: {ls_clipped['long_short_return'].mean() / ls_clipped['long_short_return'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"    Cumulative: {((1 + ls_clipped['long_short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcxjwdafeg",
   "metadata": {},
   "source": [
    "### 5.4 Shortability Analysis\n",
    "\n",
    "Filter to stocks that are practically shortable:\n",
    "- Market cap > $500M (institutional threshold)\n",
    "- Exclude penny stocks and micro-caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fxlrizjqnqk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market cap data for 5,564 symbols\n",
      "\n",
      "Market cap distribution:\n",
      "count    5.564000e+03\n",
      "mean     2.053745e+11\n",
      "std      5.247073e+12\n",
      "min      0.000000e+00\n",
      "25%      8.948057e+07\n",
      "50%      5.611468e+08\n",
      "75%      4.123179e+09\n",
      "max      3.111042e+14\n",
      "Name: marketCap, dtype: float64\n",
      "\n",
      "Market cap tiers:\n",
      "tier\n",
      "Micro (<$300M)       1941\n",
      "Small ($300M-$2B)    1414\n",
      "Mid ($2B-$10B)        979\n",
      "Large (>$10B)         879\n",
      "Unknown               351\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load fundamentals to get market cap\n",
    "key_metrics = pd.read_parquet(\"data/key_metrics.pqt\")\n",
    "\n",
    "# Get latest market cap per symbol (most recent quarter)\n",
    "key_metrics[\"date\"] = pd.to_datetime(key_metrics[\"date\"])\n",
    "latest_mcap = (\n",
    "    key_metrics[[\"symbol\", \"date\", \"marketCap\"]]\n",
    "    .sort_values(\"date\")\n",
    "    .groupby(\"symbol\")\n",
    "    .last()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"Market cap data for {len(latest_mcap):,} symbols\")\n",
    "print(f\"\\nMarket cap distribution:\")\n",
    "print(latest_mcap[\"marketCap\"].describe())\n",
    "\n",
    "# Define market cap tiers\n",
    "def mcap_tier(mcap):\n",
    "    if pd.isna(mcap) or mcap <= 0:\n",
    "        return \"Unknown\"\n",
    "    elif mcap < 300_000_000:\n",
    "        return \"Micro (<$300M)\"\n",
    "    elif mcap < 2_000_000_000:\n",
    "        return \"Small ($300M-$2B)\"\n",
    "    elif mcap < 10_000_000_000:\n",
    "        return \"Mid ($2B-$10B)\"\n",
    "    else:\n",
    "        return \"Large (>$10B)\"\n",
    "\n",
    "latest_mcap[\"tier\"] = latest_mcap[\"marketCap\"].apply(mcap_tier)\n",
    "print(f\"\\nMarket cap tiers:\")\n",
    "print(latest_mcap[\"tier\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cilmh1wzhwf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom-20 picks by market cap tier:\n",
      "tier\n",
      "Micro (<$300M)       1013\n",
      "Small ($300M-$2B)     934\n",
      "Mid ($2B-$10B)        896\n",
      "Large (>$10B)         533\n",
      "Unknown               184\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Most frequent bottom-20 stocks with their market caps:\n",
      "  QUBT: 55 days, mcap=$3.0B\n",
      "  NRIX: 48 days, mcap=$0.8B\n",
      "  FIG: 47 days, mcap=$47.4B\n",
      "  KOF: 44 days, mcap=$319.6B\n",
      "  ALNY: 36 days, mcap=$59.9B\n",
      "  OKLO: 35 days, mcap=$7.8B\n",
      "  JACK: 33 days, mcap=$0.4B\n",
      "  NBIS: 32 days, mcap=$28.3B\n",
      "  UI: 31 days, mcap=$24.9B\n",
      "  HTZ: 31 days, mcap=$2.1B\n",
      "  SERV: 30 days, mcap=$0.7B\n",
      "  MLGO: 29 days, mcap=$0.0B\n",
      "  DXYZ: 26 days, mcap=$0.4B\n",
      "  RAY: 24 days, mcap=$4.3B\n",
      "  ABTC: 23 days, mcap=$4.0B\n",
      "  MRUS: 22 days, mcap=$7.1B\n",
      "  HCWB: 22 days, mcap=$0.0B\n",
      "  NEGG: 22 days, mcap=$0.3B\n",
      "  DGNX: 21 days, mcap=N/A\n",
      "  FTAI: 21 days, mcap=$17.1B\n"
     ]
    }
   ],
   "source": [
    "# Merge market cap into test_df\n",
    "test_df_mcap = test_df.merge(\n",
    "    latest_mcap[[\"symbol\", \"marketCap\"]], \n",
    "    on=\"symbol\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Check how many of our bottom-20 picks have market cap data\n",
    "bottom_with_mcap = bottom_df.merge(latest_mcap[[\"symbol\", \"marketCap\"]], on=\"symbol\", how=\"left\")\n",
    "print(\"Bottom-20 picks by market cap tier:\")\n",
    "bottom_with_mcap[\"tier\"] = bottom_with_mcap[\"marketCap\"].apply(mcap_tier)\n",
    "print(bottom_with_mcap[\"tier\"].value_counts())\n",
    "\n",
    "print(\"\\n\\nMost frequent bottom-20 stocks with their market caps:\")\n",
    "freq_symbols = bottom_df[\"symbol\"].value_counts().head(20).index.tolist()\n",
    "for sym in freq_symbols:\n",
    "    mcap = latest_mcap[latest_mcap[\"symbol\"] == sym][\"marketCap\"].values\n",
    "    mcap_str = f\"${mcap[0]/1e9:.1f}B\" if len(mcap) > 0 and mcap[0] > 0 else \"N/A\"\n",
    "    count = bottom_df[bottom_df[\"symbol\"] == sym].shape[0]\n",
    "    print(f\"  {sym}: {count} days, mcap={mcap_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "qk7ud97hd6p",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortable universe (mcap >= $500M): 2,861 symbols\n",
      "Test rows: 352,213 -> 334,455 (95.0%)\n",
      "\n",
      "Long-Short on Shortable Universe (mcap >= $500M):\n",
      "==================================================\n",
      "\n",
      "  Trading days: 178\n",
      "\n",
      "  Long (top-20):\n",
      "    Cumulative: -10.4%\n",
      "\n",
      "  Short (bottom-20):\n",
      "    Cumulative: -36.7%\n",
      "    Short P&L: 40.0%\n",
      "\n",
      "  Long-Short:\n",
      "    Daily mean: 0.173%\n",
      "    Sharpe: 1.41\n",
      "    Cumulative: 31.5%\n",
      "    Win rate: 51.7%\n"
     ]
    }
   ],
   "source": [
    "# Re-evaluate long-short with market cap filter\n",
    "MIN_MCAP = 500_000_000  # $500M minimum\n",
    "\n",
    "# Filter to shortable universe\n",
    "shortable_symbols = set(latest_mcap[latest_mcap[\"marketCap\"] >= MIN_MCAP][\"symbol\"])\n",
    "test_shortable = test_df_mcap[test_df_mcap[\"symbol\"].isin(shortable_symbols)].copy()\n",
    "\n",
    "print(f\"Shortable universe (mcap >= ${MIN_MCAP/1e6:.0f}M): {len(shortable_symbols):,} symbols\")\n",
    "print(f\"Test rows: {len(test_df_mcap):,} -> {len(test_shortable):,} ({len(test_shortable)/len(test_df_mcap)*100:.1f}%)\")\n",
    "\n",
    "# Re-run long-short on shortable universe\n",
    "def compute_long_short_returns_filtered(df, top_k=20, clip_pct=0.50):\n",
    "    \"\"\"Long-short with clipped returns on filtered universe.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"target_return_clipped\"] = df[\"target_return\"].clip(-clip_pct, clip_pct)\n",
    "    \n",
    "    returns = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < top_k * 2:\n",
    "            continue\n",
    "        \n",
    "        top = group.nlargest(top_k, \"score\")\n",
    "        long_ret = top[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        bottom = group.nsmallest(top_k, \"score\")\n",
    "        short_ret = bottom[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        ls_ret = long_ret - short_ret\n",
    "        mkt_ret = group[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        returns.append({\n",
    "            \"date\": date,\n",
    "            \"long_return\": long_ret,\n",
    "            \"short_return\": short_ret,\n",
    "            \"long_short_return\": ls_ret,\n",
    "            \"market_return\": mkt_ret,\n",
    "        })\n",
    "    return pd.DataFrame(returns)\n",
    "\n",
    "ls_shortable = compute_long_short_returns_filtered(test_shortable, top_k=20, clip_pct=0.50)\n",
    "\n",
    "print(f\"\\nLong-Short on Shortable Universe (mcap >= $500M):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n  Trading days: {len(ls_shortable)}\")\n",
    "\n",
    "print(f\"\\n  Long (top-20):\")\n",
    "print(f\"    Cumulative: {((1 + ls_shortable['long_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Short (bottom-20):\")\n",
    "print(f\"    Cumulative: {((1 + ls_shortable['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"    Short P&L: {((1 - ls_shortable['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Long-Short:\")\n",
    "print(f\"    Daily mean: {ls_shortable['long_short_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Sharpe: {ls_shortable['long_short_return'].mean() / ls_shortable['long_short_return'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"    Cumulative: {((1 + ls_shortable['long_short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"    Win rate: {(ls_shortable['long_short_return'] > 0).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "higo5py96vg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent bottom-20 stocks (shortable universe):\n",
      "  QUBT: 72 days, mcap=$3.0B\n",
      "  NBIS: 59 days, mcap=$28.3B\n",
      "  NRIX: 56 days, mcap=$0.8B\n",
      "  FIG: 53 days, mcap=$47.4B\n",
      "  KOF: 53 days, mcap=$319.6B\n",
      "  OKLO: 51 days, mcap=$7.8B\n",
      "  ALNY: 46 days, mcap=$59.9B\n",
      "  UI: 39 days, mcap=$24.9B\n",
      "  SERV: 38 days, mcap=$0.7B\n",
      "  NNE: 37 days, mcap=$1.3B\n",
      "  HTZ: 36 days, mcap=$2.1B\n",
      "  IE: 35 days, mcap=$1.2B\n",
      "  STX: 34 days, mcap=$53.8B\n",
      "  PCT: 33 days, mcap=$2.4B\n",
      "  RGTI: 32 days, mcap=$3.5B\n",
      "  PGY: 32 days, mcap=$2.3B\n",
      "  RAY: 32 days, mcap=$4.3B\n",
      "  LCID: 31 days, mcap=$7.4B\n",
      "  TLRY: 31 days, mcap=$1.5B\n",
      "  MRUS: 29 days, mcap=$7.1B\n",
      "\n",
      "\n",
      "Return distribution of shortable bottom-20:\n",
      "count    3560.000000\n",
      "mean       -0.002193\n",
      "std         0.068118\n",
      "min        -0.887303\n",
      "25%        -0.030394\n",
      "50%        -0.002638\n",
      "75%         0.024212\n",
      "max         0.956265\n",
      "Name: target_return, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# What stocks end up in the shortable bottom-20?\n",
    "bottom_shortable = []\n",
    "for date, group in test_shortable.groupby(\"feature_date\"):\n",
    "    if len(group) < 40:\n",
    "        continue\n",
    "    bottom = group.nsmallest(20, \"score\")\n",
    "    bottom_shortable.append(bottom[[\"symbol\", \"feature_date\", \"target_return\", \"score\", \"marketCap\"]])\n",
    "\n",
    "bottom_shortable_df = pd.concat(bottom_shortable)\n",
    "\n",
    "print(\"Most frequent bottom-20 stocks (shortable universe):\")\n",
    "freq = bottom_shortable_df[\"symbol\"].value_counts().head(20)\n",
    "for sym, count in freq.items():\n",
    "    mcap = bottom_shortable_df[bottom_shortable_df[\"symbol\"] == sym][\"marketCap\"].iloc[0]\n",
    "    print(f\"  {sym}: {count} days, mcap=${mcap/1e9:.1f}B\")\n",
    "\n",
    "print(f\"\\n\\nReturn distribution of shortable bottom-20:\")\n",
    "print(bottom_shortable_df[\"target_return\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and config\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"config\": config,\n",
    "    \"price_cols\": price_feat_cols,\n",
    "    \"fund_cols\": fund_feat_cols,\n",
    "    \"emb_cols\": emb_cols,\n",
    "}, \"data/model_final.pt\")\n",
    "\n",
    "print(\"Model saved to data/model_final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history_df = pd.DataFrame(history)\n",
    "history_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cg5bdgdi4ov",
   "metadata": {},
   "source": [
    "## 6. News Ablation Analysis\n",
    "\n",
    "Evaluate the incremental value of news embeddings by comparing:\n",
    "1. Model performance on rows with vs without news\n",
    "2. Full model vs ablated model (zeroed news embeddings)\n",
    "3. Statistical significance of the news signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f5d19f-bcb2-4eed-b58c-c20b2628b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows with news (non-zero embeddings)\n",
    "test_df[\"has_news\"] = (test_df[emb_cols].abs().sum(axis=1) > 0).astype(int)\n",
    "print(f\"Test set news coverage: {test_df['has_news'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kywyplishsp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare IC for rows with news vs without news\n",
    "test_with_news = test_df[test_df[\"has_news\"] == 1]\n",
    "test_no_news = test_df[test_df[\"has_news\"] == 0]\n",
    "\n",
    "ic_with_news = compute_daily_ic(test_with_news)\n",
    "ic_no_news = compute_daily_ic(test_no_news)\n",
    "\n",
    "print(\"Rank IC comparison:\")\n",
    "print(f\"  With news:    mean={ic_with_news['ic'].mean():.4f}, std={ic_with_news['ic'].std():.4f}, n_days={len(ic_with_news)}\")\n",
    "print(f\"  Without news: mean={ic_no_news['ic'].mean():.4f}, std={ic_no_news['ic'].std():.4f}, n_days={len(ic_no_news)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v08p2y9gh8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basket returns: compare top-K from news vs no-news subsets\n",
    "basket_with_news = compute_basket_returns(test_with_news, top_k=20)\n",
    "basket_no_news = compute_basket_returns(test_no_news, top_k=20)\n",
    "\n",
    "print(\"\\nBasket performance comparison (top-20):\")\n",
    "print(f\"  With news:\")\n",
    "print(f\"    Daily excess: {basket_with_news['excess_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Sharpe (excess): {basket_with_news['excess_return'].mean() / basket_with_news['excess_return'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"  Without news:\")\n",
    "print(f\"    Daily excess: {basket_no_news['excess_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Sharpe (excess): {basket_no_news['excess_return'].mean() / basket_no_news['excess_return'].std() * np.sqrt(252):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ls475exl4w",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation: Score with zeroed-out news embeddings\n",
    "@torch.no_grad()\n",
    "def get_scores_no_news(model, df, price_cols, fund_cols, emb_cols, device, batch_size=1024):\n",
    "    \"\"\"Get model scores with news embeddings zeroed out.\"\"\"\n",
    "    model.eval()\n",
    "    dataset = PointwiseDataset(df, price_cols, fund_cols, emb_cols)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    scores = []\n",
    "    for batch in tqdm(loader, desc=\"Scoring (no news)\", leave=False):\n",
    "        price = batch[\"price\"].to(device)\n",
    "        fund = batch[\"fund\"].to(device)\n",
    "        # Zero out embeddings\n",
    "        emb = torch.zeros_like(batch[\"emb\"]).to(device)\n",
    "        \n",
    "        score = model(price, fund, emb)\n",
    "        scores.append(score.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(scores)\n",
    "\n",
    "# Score with ablated news\n",
    "test_df[\"score_no_news\"] = get_scores_no_news(model, test_df, price_feat_cols, fund_feat_cols, emb_cols, device)\n",
    "\n",
    "# Compare IC\n",
    "ic_full = compute_daily_ic(test_df)  # Uses \"score\" column\n",
    "test_df_ablated = test_df.copy()\n",
    "test_df_ablated[\"score\"] = test_df_ablated[\"score_no_news\"]\n",
    "ic_ablated = compute_daily_ic(test_df_ablated)\n",
    "\n",
    "print(\"\\nAblation study (zeroed news embeddings):\")\n",
    "print(f\"  Full model IC:    {ic_full['ic'].mean():.4f}\")\n",
    "print(f\"  No-news model IC: {ic_ablated['ic'].mean():.4f}\")\n",
    "print(f\"  Delta:            {(ic_full['ic'].mean() - ic_ablated['ic'].mean()):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9gmzhhf5ra6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basket performance: full model vs ablated model\n",
    "basket_full = compute_basket_returns(test_df, top_k=20)\n",
    "basket_ablated = compute_basket_returns(test_df_ablated, top_k=20)\n",
    "\n",
    "print(\"\\nBasket performance: Full vs Ablated model:\")\n",
    "print(f\"  Full model:\")\n",
    "print(f\"    Daily excess: {basket_full['excess_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Sharpe (excess): {basket_full['excess_return'].mean() / basket_full['excess_return'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"  Ablated (no news):\")\n",
    "print(f\"    Daily excess: {basket_ablated['excess_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Sharpe (excess): {basket_ablated['excess_return'].mean() / basket_ablated['excess_return'].std() * np.sqrt(252):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6zeo3ydn2o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on rows WITH news: does the model do better with real embeddings vs zeros?\n",
    "test_news_only = test_df[test_df[\"has_news\"] == 1].copy()\n",
    "\n",
    "# Full model scores (already have)\n",
    "ic_news_full = compute_daily_ic(test_news_only)\n",
    "\n",
    "# Ablated scores for news rows\n",
    "test_news_ablated = test_news_only.copy()\n",
    "test_news_ablated[\"score\"] = test_news_only[\"score_no_news\"]\n",
    "ic_news_ablated = compute_daily_ic(test_news_ablated)\n",
    "\n",
    "print(\"\\nNews rows only - Full vs Ablated:\")\n",
    "print(f\"  Full model IC:    {ic_news_full['ic'].mean():.4f}\")\n",
    "print(f\"  Ablated model IC: {ic_news_ablated['ic'].mean():.4f}\")\n",
    "print(f\"  Delta (news benefit): {(ic_news_full['ic'].mean() - ic_news_ablated['ic'].mean()):.4f}\")\n",
    "\n",
    "# Is the delta significant?\n",
    "from scipy.stats import ttest_rel\n",
    "if len(ic_news_full) == len(ic_news_ablated):\n",
    "    merged_ic = ic_news_full.merge(ic_news_ablated, on=\"date\", suffixes=(\"_full\", \"_ablated\"))\n",
    "    t_stat, p_val = ttest_rel(merged_ic[\"ic_full\"], merged_ic[\"ic_ablated\"])\n",
    "    print(f\"  Paired t-test: t={t_stat:.2f}, p={p_val:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
