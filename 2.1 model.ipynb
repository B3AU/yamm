{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0000-0001-0001-000000000000",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "Multi-branch neural model with influence-controlled news embedding.\n",
    "\n",
    "**Table of Contents:**\n",
    "1. Load and split data\n",
    "2. Dataset and DataLoader\n",
    "3. Model architecture\n",
    "4. Training loop\n",
    "5. Evaluation: Rank IC and Basket Returns\n",
    "   - 5.1 Long-Short Analysis\n",
    "   - 5.2 Data Quality Check\n",
    "   - 5.3 Clipped Returns Evaluation\n",
    "6. News Ablation Analysis\n",
    "\n",
    "**Architecture** (per README section 9):\n",
    "- Fundamentals encoder → h_f (32 dims)\n",
    "- Price encoder → h_p (16 dims)\n",
    "- News encoder → h_n (32 dims)\n",
    "- Fusion: h = concat([h_f, h_p, α * h_n]), α=0.3\n",
    "- Output: scalar score for ranking\n",
    "\n",
    "**Training objective** (per README section 10):\n",
    "- Pairwise ranking loss within each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1b2c3d4-0001-0001-0001-000000000001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b2c3d4-0001-0001-0001-000000000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelConfig:\n",
    "    # Feature dimensions (set after loading data)\n",
    "    n_fundamental_features: int = 19\n",
    "    n_price_features: int = 9\n",
    "    n_embedding_dim: int = 768\n",
    "    \n",
    "    # Encoder latent dimensions\n",
    "    fundamental_latent: int = 32\n",
    "    price_latent: int = 16\n",
    "    news_latent: int = 32\n",
    "    \n",
    "    # Dropout\n",
    "    fundamental_dropout: float = 0.2\n",
    "    price_dropout: float = 0.2\n",
    "    news_dropout: float = 0.3\n",
    "    \n",
    "    # News influence cap\n",
    "    news_alpha: float = 0.8\n",
    "    \n",
    "    # Training\n",
    "    batch_size: int = 512\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-3\n",
    "    n_epochs: int = 20\n",
    "    pairs_per_day: int = 1000\n",
    "\n",
    "    #hard fraction over-sampling\n",
    "    hard_fraction = 0.0    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000003",
   "metadata": {},
   "source": [
    "## 1. Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1b2c3d4-0001-0001-0001-000000000004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 2,092,929 rows\n",
      "Date range: 2021-01-13 to 2025-12-18\n",
      "Symbols: 3,506\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"data/ml_dataset.pqt\")\n",
    "df[\"feature_date\"] = pd.to_datetime(df[\"feature_date\"])\n",
    "\n",
    "print(f\"Dataset: {len(df):,} rows\")\n",
    "print(f\"Date range: {df['feature_date'].min().date()} to {df['feature_date'].max().date()}\")\n",
    "print(f\"Symbols: {df['symbol'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b2c3d4-0001-0001-0001-000000000005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price features: 9\n",
      "Fundamental features: 19\n",
      "Embedding dims: 768\n"
     ]
    }
   ],
   "source": [
    "# Identify feature columns\n",
    "price_feat_cols = [\n",
    "    \"overnight_gap_z\", \"intraday_ret_z\",\n",
    "    \"ret_1d_z\", \"ret_2d_z\", \"ret_3d_z\", \"ret_5d_z\",\n",
    "    \"vol_5d_z\", \"dist_from_high_5d_z\", \"dist_from_low_5d_z\"\n",
    "]\n",
    "\n",
    "# Fundamental feature columns (normalized)\n",
    "fund_feat_cols = [c for c in df.columns if c.endswith(\"_z\") and c not in price_feat_cols and c != \"news_count_z\"]\n",
    "\n",
    "# Embedding columns\n",
    "emb_cols = [c for c in df.columns if c.startswith(\"emb_\")]\n",
    "\n",
    "print(f\"Price features: {len(price_feat_cols)}\")\n",
    "print(f\"Fundamental features: {len(fund_feat_cols)}\")\n",
    "print(f\"Embedding dims: {len(emb_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b2c3d4-0001-0001-0001-000000000006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1,418,494 rows, 830 days\n",
      "Val: 322,222 rows, 178 days\n",
      "Test: 352,213 rows, 178 days\n"
     ]
    }
   ],
   "source": [
    "# Time-based split (per README section 13)\n",
    "# Use last 20% of dates as test\n",
    "dates = sorted(df[\"feature_date\"].unique())\n",
    "n_dates = len(dates)\n",
    "train_end_idx = int(n_dates * 0.7)\n",
    "val_end_idx = int(n_dates * 0.85)\n",
    "\n",
    "train_dates = set(dates[:train_end_idx])\n",
    "val_dates = set(dates[train_end_idx:val_end_idx])\n",
    "test_dates = set(dates[val_end_idx:])\n",
    "\n",
    "train_df = df[df[\"feature_date\"].isin(train_dates)].copy()\n",
    "val_df = df[df[\"feature_date\"].isin(val_dates)].copy()\n",
    "test_df = df[df[\"feature_date\"].isin(test_dates)].copy()\n",
    "\n",
    "print(f\"Train: {len(train_df):,} rows, {len(train_dates)} days\")\n",
    "print(f\"Val: {len(val_df):,} rows, {len(val_dates)} days\")\n",
    "print(f\"Test: {len(test_df):,} rows, {len(test_dates)} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000007",
   "metadata": {},
   "source": [
    "## 2. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1b2c3d4-0001-0001-0001-000000000008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairwiseRankingDataset(Dataset):\n",
    "    \"\"\"Dataset that generates hard pairs from same day for ranking loss.\n",
    "\n",
    "    Only uses rows with news, and samples from top vs bottom quartiles\n",
    "    for harder training signal.\n",
    "\n",
    "    Call resample_pairs() at the start of each epoch to generate fresh pairs.\n",
    "    This prevents overfitting to specific pair combinations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        price_cols: list[str],\n",
    "        fund_cols: list[str],\n",
    "        emb_cols: list[str],\n",
    "        pairs_per_day: int = 100,\n",
    "        hard_fraction: float = 0.7,  # Mix of hard vs random pairs\n",
    "    ):\n",
    "        # Filter to rows with news only\n",
    "        has_news = (df[emb_cols].abs().sum(axis=1) > 0)\n",
    "        df_news = df[has_news].copy()\n",
    "        print(f\"Filtered to news-only: {len(df_news):,} / {len(df):,} rows ({len(df_news)/len(df)*100:.1f}%)\")\n",
    "\n",
    "        self.df = df_news.reset_index(drop=True)\n",
    "        self.price_cols = price_cols\n",
    "        self.fund_cols = fund_cols\n",
    "        self.emb_cols = emb_cols\n",
    "        self.pairs_per_day = pairs_per_day\n",
    "        self.hard_fraction = hard_fraction\n",
    "\n",
    "        # Group by date and precompute quartile indices\n",
    "        self.date_groups = {}\n",
    "        for date, group in df_news.groupby(\"feature_date\"):\n",
    "            indices = group.index.tolist()\n",
    "            if len(indices) < 4:  # Need at least 4 for quartiles\n",
    "                continue\n",
    "\n",
    "            # Sort by target return and get quartile indices\n",
    "            sorted_idx = group[\"target_return\"].sort_values().index.tolist()\n",
    "            q_size = len(sorted_idx) // 4\n",
    "            if q_size < 1:\n",
    "                continue\n",
    "\n",
    "            bottom_q = sorted_idx[:q_size]  # Losers\n",
    "            top_q = sorted_idx[-q_size:]    # Winners\n",
    "\n",
    "            self.date_groups[date] = {\n",
    "                \"all\": np.array(indices),\n",
    "                \"top\": np.array(top_q),\n",
    "                \"bottom\": np.array(bottom_q),\n",
    "            }\n",
    "\n",
    "        self.dates = list(self.date_groups.keys())\n",
    "        print(f\"Days with sufficient news coverage: {len(self.dates)}\")\n",
    "\n",
    "        # Precompute arrays for speed\n",
    "        self.price_arr = df_news[price_cols].values.astype(np.float32)\n",
    "        self.fund_arr = df_news[fund_cols].values.astype(np.float32)\n",
    "        self.emb_arr = df_news[emb_cols].values.astype(np.float32)\n",
    "        self.target_arr = df_news[\"target_return\"].values.astype(np.float32)\n",
    "\n",
    "        # Map original index to position in filtered df\n",
    "        self.idx_map = {old_idx: new_idx for new_idx, old_idx in enumerate(df_news.index)}\n",
    "\n",
    "        # Pre-generate pairs for first epoch\n",
    "        self.pairs = []\n",
    "        self.resample_pairs()\n",
    "\n",
    "    def resample_pairs(self):\n",
    "        \"\"\"Generate fresh pairs for a new epoch (vectorized for speed).\"\"\"\n",
    "        pairs = []\n",
    "\n",
    "        for date in self.dates:\n",
    "            groups = self.date_groups[date]\n",
    "            n_hard = int(self.pairs_per_day * self.hard_fraction)\n",
    "            n_random = self.pairs_per_day - n_hard\n",
    "\n",
    "            # Hard pairs: top quartile vs bottom quartile (vectorized)\n",
    "            if n_hard > 0:\n",
    "                winners = np.random.choice(groups[\"top\"], size=n_hard, replace=True)\n",
    "                losers = np.random.choice(groups[\"bottom\"], size=n_hard, replace=True)\n",
    "                for w, l in zip(winners, losers):\n",
    "                    pairs.append((self.idx_map[w], self.idx_map[l]))\n",
    "\n",
    "            # Random pairs from all stocks that day\n",
    "            if n_random > 0 and len(groups[\"all\"]) >= 2:\n",
    "                all_idx = groups[\"all\"]\n",
    "                idx1 = np.random.choice(all_idx, size=n_random, replace=True)\n",
    "                idx2 = np.random.choice(all_idx, size=n_random, replace=True)\n",
    "                for i1, i2 in zip(idx1, idx2):\n",
    "                    if i1 != i2:\n",
    "                        pairs.append((self.idx_map[i1], self.idx_map[i2]))\n",
    "\n",
    "        self.pairs = pairs\n",
    "        # Shuffle pairs so batches mix different days\n",
    "        np.random.shuffle(self.pairs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i, j = self.pairs[idx]\n",
    "\n",
    "        # Get features\n",
    "        price_i = self.price_arr[i]\n",
    "        price_j = self.price_arr[j]\n",
    "        fund_i = self.fund_arr[i]\n",
    "        fund_j = self.fund_arr[j]\n",
    "        emb_i = self.emb_arr[i]\n",
    "        emb_j = self.emb_arr[j]\n",
    "\n",
    "        # Label based on actual returns\n",
    "        actual_label = 1.0 if self.target_arr[i] > self.target_arr[j] else 0.0\n",
    "\n",
    "        # Randomly swap to balance labels\n",
    "        if np.random.random() < 0.5:\n",
    "            price_i, price_j = price_j, price_i\n",
    "            fund_i, fund_j = fund_j, fund_i\n",
    "            emb_i, emb_j = emb_j, emb_i\n",
    "            label = 1.0 - actual_label\n",
    "        else:\n",
    "            label = actual_label\n",
    "\n",
    "        return {\n",
    "            \"price_i\": torch.tensor(price_i),\n",
    "            \"price_j\": torch.tensor(price_j),\n",
    "            \"fund_i\": torch.tensor(fund_i),\n",
    "            \"fund_j\": torch.tensor(fund_j),\n",
    "            \"emb_i\": torch.tensor(emb_i),\n",
    "            \"emb_j\": torch.tensor(emb_j),\n",
    "            \"label\": torch.tensor(label),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b2c3d4-0001-0001-0001-000000000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointwiseDataset(Dataset):\n",
    "    \"\"\"Dataset for inference - one sample per stock-day.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        price_cols: list[str],\n",
    "        fund_cols: list[str],\n",
    "        emb_cols: list[str],\n",
    "    ):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.price_arr = df[price_cols].values.astype(np.float32)\n",
    "        self.fund_arr = df[fund_cols].values.astype(np.float32)\n",
    "        self.emb_arr = df[emb_cols].values.astype(np.float32)\n",
    "        self.target_arr = df[\"target_return\"].values.astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"price\": torch.tensor(self.price_arr[idx]),\n",
    "            \"fund\": torch.tensor(self.fund_arr[idx]),\n",
    "            \"emb\": torch.tensor(self.emb_arr[idx]),\n",
    "            \"target\": torch.tensor(self.target_arr[idx]),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b2c3d4-0001-0001-0001-000000000010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: ModelConfig(n_fundamental_features=19, n_price_features=9, n_embedding_dim=768, fundamental_latent=32, price_latent=16, news_latent=32, fundamental_dropout=0.2, price_dropout=0.2, news_dropout=0.3, news_alpha=0.8, batch_size=512, learning_rate=0.001, weight_decay=0.001, n_epochs=20, pairs_per_day=1000)\n"
     ]
    }
   ],
   "source": [
    "# Create config with actual dimensions\n",
    "config = ModelConfig(\n",
    "    n_fundamental_features=len(fund_feat_cols),\n",
    "    n_price_features=len(price_feat_cols),\n",
    "    n_embedding_dim=len(emb_cols),\n",
    ")\n",
    "\n",
    "print(f\"Config: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1b2c3d4-0001-0001-0001-000000000011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to news-only: 339,872 / 1,418,494 rows (24.0%)\n",
      "Days with sufficient news coverage: 830\n",
      "Filtered to news-only: 88,579 / 322,222 rows (27.5%)\n",
      "Days with sufficient news coverage: 178\n",
      "Train pairs: 828,008\n",
      "Val pairs: 177,668\n"
     ]
    }
   ],
   "source": [
    "# Create datasets (both use same hard pair ratio for consistent distribution)\n",
    "train_dataset = PairwiseRankingDataset(\n",
    "    train_df, price_feat_cols, fund_feat_cols, emb_cols,\n",
    "    pairs_per_day=config.pairs_per_day,\n",
    "    hard_fraction=config.hard_fraction,\n",
    ")\n",
    "val_dataset = PairwiseRankingDataset(\n",
    "    val_df, price_feat_cols, fund_feat_cols, emb_cols,\n",
    "    pairs_per_day=config.pairs_per_day,\n",
    "    hard_fraction=config.hard_fraction,  # Same distribution as training\n",
    ")\n",
    "\n",
    "# For evaluation (pointwise)\n",
    "val_pointwise = PointwiseDataset(val_df, price_feat_cols, fund_feat_cols, emb_cols)\n",
    "test_pointwise = PointwiseDataset(test_df, price_feat_cols, fund_feat_cols, emb_cols)\n",
    "\n",
    "print(f\"Train pairs: {len(train_dataset):,}\")\n",
    "print(f\"Val pairs: {len(val_dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1b2c3d4-0001-0001-0001-000000000012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "# num_workers=0 for Jupyter compatibility (avoids multiprocessing issues)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000013",
   "metadata": {},
   "source": [
    "## 3. Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1b2c3d4-0001-0001-0001-000000000014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBranchRanker(nn.Module):\n",
    "    \"\"\"Multi-branch model with influence-controlled news embedding.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Fundamentals encoder\n",
    "        self.fund_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_fundamental_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.fundamental_dropout),\n",
    "            nn.Linear(64, config.fundamental_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Price encoder\n",
    "        self.price_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_price_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.price_dropout),\n",
    "            nn.Linear(32, config.price_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # News encoder\n",
    "        self.news_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.news_dropout),\n",
    "            nn.Linear(128, config.news_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Output head\n",
    "        fused_dim = config.fundamental_latent + config.price_latent + config.news_latent\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "    \n",
    "    def encode(self, price: torch.Tensor, fund: torch.Tensor, emb: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Encode features and return fused representation.\"\"\"\n",
    "        h_f = self.fund_encoder(fund)\n",
    "        h_p = self.price_encoder(price)\n",
    "        h_n = self.news_encoder(emb)\n",
    "        \n",
    "        # Apply news influence cap (α * h_n)\n",
    "        h_n_scaled = self.config.news_alpha * h_n\n",
    "        \n",
    "        # Fuse\n",
    "        h = torch.cat([h_f, h_p, h_n_scaled], dim=-1)\n",
    "        return h\n",
    "    \n",
    "    def forward(self, price: torch.Tensor, fund: torch.Tensor, emb: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass returning score.\"\"\"\n",
    "        h = self.encode(price, fund, emb)\n",
    "        score = self.output_head(h)\n",
    "        return score.squeeze(-1)\n",
    "    \n",
    "    def forward_pair(\n",
    "        self,\n",
    "        price_i: torch.Tensor, fund_i: torch.Tensor, emb_i: torch.Tensor,\n",
    "        price_j: torch.Tensor, fund_j: torch.Tensor, emb_j: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass for pair, returning P(i > j).\"\"\"\n",
    "        score_i = self.forward(price_i, fund_i, emb_i)\n",
    "        score_j = self.forward(price_j, fund_j, emb_j)\n",
    "        return torch.sigmoid(score_i - score_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1b2c3d4-0001-0001-0001-000000000015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 109,393\n",
      "MultiBranchRanker(\n",
      "  (fund_encoder): Sequential(\n",
      "    (0): Linear(in_features=19, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (price_encoder): Sequential(\n",
      "    (0): Linear(in_features=9, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (news_encoder): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (output_head): Sequential(\n",
      "    (0): Linear(in_features=80, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MultiBranchRanker(config).to(device)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params:,}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000016",
   "metadata": {},
   "source": [
    "## 4. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1b2c3d4-0001-0001-0001-000000000017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_ranking_loss(pred_prob: torch.Tensor, label: torch.Tensor, smoothing: float = 0.2) -> torch.Tensor:\n",
    "    \"\"\"Binary cross-entropy for pairwise ranking with label smoothing.\"\"\"\n",
    "    # Smooth labels: 0 -> smoothing/2, 1 -> 1 - smoothing/2\n",
    "    smoothed_label = label * (1 - smoothing) + 0.5 * smoothing\n",
    "    return F.binary_cross_entropy(pred_prob, smoothed_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1b2c3d4-0001-0001-0001-000000000018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        price_i = batch[\"price_i\"].to(device)\n",
    "        price_j = batch[\"price_j\"].to(device)\n",
    "        fund_i = batch[\"fund_i\"].to(device)\n",
    "        fund_j = batch[\"fund_j\"].to(device)\n",
    "        emb_i = batch[\"emb_i\"].to(device)\n",
    "        emb_j = batch[\"emb_j\"].to(device)\n",
    "        label = batch[\"label\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred_prob = model.forward_pair(price_i, fund_i, emb_i, price_j, fund_j, emb_j)\n",
    "        loss = pairwise_ranking_loss(pred_prob, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(label)\n",
    "        total_correct += ((pred_prob > 0.5) == (label > 0.5)).sum().item()\n",
    "        total_samples += len(label)\n",
    "    \n",
    "    return total_loss / total_samples, total_correct / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1b2c3d4-0001-0001-0001-000000000019",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        price_i = batch[\"price_i\"].to(device)\n",
    "        price_j = batch[\"price_j\"].to(device)\n",
    "        fund_i = batch[\"fund_i\"].to(device)\n",
    "        fund_j = batch[\"fund_j\"].to(device)\n",
    "        emb_i = batch[\"emb_i\"].to(device)\n",
    "        emb_j = batch[\"emb_j\"].to(device)\n",
    "        label = batch[\"label\"].to(device)\n",
    "        \n",
    "        pred_prob = model.forward_pair(price_i, fund_i, emb_i, price_j, fund_j, emb_j)\n",
    "        loss = pairwise_ranking_loss(pred_prob, label)\n",
    "        \n",
    "        total_loss += loss.item() * len(label)\n",
    "        total_correct += ((pred_prob > 0.5) == (label > 0.5)).sum().item()\n",
    "        total_samples += len(label)\n",
    "    \n",
    "    return total_loss / total_samples, total_correct / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1b2c3d4-0001-0001-0001-000000000020",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config.n_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1b2c3d4-0001-0001-0001-000000000021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: train_loss=0.6926, train_acc=0.5131, val_loss=0.6930, val_acc=0.5043\n",
      "  -> New best model saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: train_loss=0.6925, train_acc=0.5160, val_loss=0.6927, val_acc=0.5100\n",
      "  -> New best model saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: train_loss=0.6922, train_acc=0.5178, val_loss=0.6929, val_acc=0.5068\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: train_loss=0.6919, train_acc=0.5199, val_loss=0.6929, val_acc=0.5111\n",
      "  -> New best model saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: train_loss=0.6918, train_acc=0.5216, val_loss=0.6933, val_acc=0.5065\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: train_loss=0.6915, train_acc=0.5240, val_loss=0.6931, val_acc=0.5054\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: train_loss=0.6911, train_acc=0.5269, val_loss=0.6934, val_acc=0.5064\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: train_loss=0.6906, train_acc=0.5284, val_loss=0.6937, val_acc=0.5033\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: train_loss=0.6901, train_acc=0.5319, val_loss=0.6937, val_acc=0.5045\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: train_loss=0.6896, train_acc=0.5345, val_loss=0.6939, val_acc=0.5045\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: train_loss=0.6891, train_acc=0.5369, val_loss=0.6938, val_acc=0.5063\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: train_loss=0.6885, train_acc=0.5393, val_loss=0.6946, val_acc=0.5032\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: train_loss=0.6882, train_acc=0.5412, val_loss=0.6941, val_acc=0.5046\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: train_loss=0.6877, train_acc=0.5438, val_loss=0.6948, val_acc=0.5046\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: train_loss=0.6874, train_acc=0.5452, val_loss=0.6947, val_acc=0.5034\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: train_loss=0.6871, train_acc=0.5465, val_loss=0.6949, val_acc=0.5043\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: train_loss=0.6869, train_acc=0.5465, val_loss=0.6952, val_acc=0.5028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9abe2944b684bb79e89cd1f735a43ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2e20723cc1418d9aabef3ecaef73d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: train_loss=0.6868, train_acc=0.5476, val_loss=0.6949, val_acc=0.5033\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f97965e27f34e788ef550ad65e62788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb89ce5657546e09f9ae43b8a2462ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: train_loss=0.6866, train_acc=0.5478, val_loss=0.6949, val_acc=0.5032\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389aae7a3f1f4be7ab33b5f20703539f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1618 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0da805947a141958c9564f334891d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/347 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: train_loss=0.6864, train_acc=0.5483, val_loss=0.6952, val_acc=0.5014\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "best_val_acc = 0\n",
    "history = []\n",
    "\n",
    "for epoch in range(config.n_epochs):\n",
    "    # Resample pairs each epoch for fresh training data\n",
    "    train_dataset.resample_pairs()\n",
    "    val_dataset.resample_pairs()\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss, val_acc = eval_epoch(model, val_loader, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    history.append({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "    })\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{config.n_epochs}: \"\n",
    "          f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, \"\n",
    "          f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"data/model_best.pt\")\n",
    "        print(f\"  -> New best model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000022",
   "metadata": {},
   "source": [
    "## 5. Evaluation: Rank IC and Basket Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1b2c3d4-0001-0001-0001-000000000023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiBranchRanker(\n",
       "  (fund_encoder): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       "  (price_encoder): Sequential(\n",
       "    (0): Linear(in_features=9, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       "  (news_encoder): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       "  (output_head): Sequential(\n",
       "    (0): Linear(in_features=80, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"data/model_best.pt\", weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1b2c3d4-0001-0001-0001-000000000024",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_scores(model, df, price_cols, fund_cols, emb_cols, device, batch_size=1024):\n",
    "    \"\"\"Get model scores for all rows.\"\"\"\n",
    "    model.eval()\n",
    "    dataset = PointwiseDataset(df, price_cols, fund_cols, emb_cols)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    scores = []\n",
    "    for batch in tqdm(loader, desc=\"Scoring\", leave=False):\n",
    "        price = batch[\"price\"].to(device)\n",
    "        fund = batch[\"fund\"].to(device)\n",
    "        emb = batch[\"emb\"].to(device)\n",
    "        \n",
    "        score = model(price, fund, emb)\n",
    "        scores.append(score.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1b2c3d4-0001-0001-0001-000000000025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 352,213 test samples\n"
     ]
    }
   ],
   "source": [
    "# Score test set\n",
    "test_df = test_df.copy()\n",
    "test_df[\"score\"] = get_scores(model, test_df, price_feat_cols, fund_feat_cols, emb_cols, device)\n",
    "\n",
    "print(f\"Scored {len(test_df):,} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1b2c3d4-0001-0001-0001-000000000026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def compute_daily_ic(df):\n",
    "    \"\"\"Compute Spearman rank IC per day.\"\"\"\n",
    "    ics = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < 10:\n",
    "            continue\n",
    "        ic, _ = spearmanr(group[\"score\"], group[\"target_return\"])\n",
    "        if not np.isnan(ic):\n",
    "            ics.append({\"date\": date, \"ic\": ic})\n",
    "    return pd.DataFrame(ics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1b2c3d4-0001-0001-0001-000000000027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Rank IC (Spearman):\n",
      "  Mean: 0.0251\n",
      "  Std: 0.0963\n",
      "  Sharpe (IC): 4.13\n",
      "  % Positive: 59.6%\n"
     ]
    }
   ],
   "source": [
    "ic_df = compute_daily_ic(test_df)\n",
    "\n",
    "print(f\"Daily Rank IC (Spearman):\")\n",
    "print(f\"  Mean: {ic_df['ic'].mean():.4f}\")\n",
    "print(f\"  Std: {ic_df['ic'].std():.4f}\")\n",
    "print(f\"  Sharpe (IC): {ic_df['ic'].mean() / ic_df['ic'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"  % Positive: {(ic_df['ic'] > 0).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1b2c3d4-0001-0001-0001-000000000028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_basket_returns(df, top_k=20):\n",
    "    \"\"\"Compute daily returns of top-K basket (equal weight).\"\"\"\n",
    "    returns = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < top_k:\n",
    "            continue\n",
    "        # Select top-K by score\n",
    "        top = group.nlargest(top_k, \"score\")\n",
    "        # Equal weight return\n",
    "        ret = top[\"target_return\"].mean()\n",
    "        # Market return (all stocks)\n",
    "        mkt_ret = group[\"target_return\"].mean()\n",
    "        returns.append({\n",
    "            \"date\": date,\n",
    "            \"basket_return\": ret,\n",
    "            \"market_return\": mkt_ret,\n",
    "            \"excess_return\": ret - mkt_ret,\n",
    "        })\n",
    "    return pd.DataFrame(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1b2c3d4-0001-0001-0001-000000000029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-20 Basket Performance (Test Set):\n",
      "  Daily mean return: -0.137%\n",
      "  Daily mean excess: 0.007%\n",
      "  Sharpe (basket): -1.68\n",
      "  Sharpe (excess): 0.20\n"
     ]
    }
   ],
   "source": [
    "basket_df = compute_basket_returns(test_df, top_k=20)\n",
    "\n",
    "print(f\"\\nTop-20 Basket Performance (Test Set):\")\n",
    "print(f\"  Daily mean return: {basket_df['basket_return'].mean()*100:.3f}%\")\n",
    "print(f\"  Daily mean excess: {basket_df['excess_return'].mean()*100:.3f}%\")\n",
    "print(f\"  Sharpe (basket): {basket_df['basket_return'].mean() / basket_df['basket_return'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"  Sharpe (excess): {basket_df['excess_return'].mean() / basket_df['excess_return'].std() * np.sqrt(252):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1b2c3d4-0001-0001-0001-000000000030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cumulative Returns:\n",
      "  Basket: -22.8%\n",
      "  Market: -23.7%\n",
      "  Excess: 1.0%\n"
     ]
    }
   ],
   "source": [
    "# Cumulative returns\n",
    "basket_df[\"cum_basket\"] = (1 + basket_df[\"basket_return\"]).cumprod()\n",
    "basket_df[\"cum_market\"] = (1 + basket_df[\"market_return\"]).cumprod()\n",
    "basket_df[\"cum_excess\"] = (1 + basket_df[\"excess_return\"]).cumprod()\n",
    "\n",
    "print(f\"\\nCumulative Returns:\")\n",
    "print(f\"  Basket: {(basket_df['cum_basket'].iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"  Market: {(basket_df['cum_market'].iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"  Excess: {(basket_df['cum_excess'].iloc[-1] - 1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "o5vcmhiugh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long-Short Portfolio Performance (Test Set):\n",
      "\n",
      "  Long (top-20):\n",
      "    Daily mean: -0.137%\n",
      "    Cumulative: -22.8%\n",
      "\n",
      "  Short (bottom-20):\n",
      "    Daily mean: -2.412%\n",
      "    Cumulative: -99.0%\n",
      "    Short P&L (inverted): 5449.1%\n",
      "\n",
      "  Long-Short (market neutral):\n",
      "    Daily mean: 2.275%\n",
      "    Std: 4.761%\n",
      "    Sharpe: 7.59\n",
      "    Cumulative: 4430.5%\n",
      "\n",
      "  Market:\n",
      "    Cumulative: -23.7%\n"
     ]
    }
   ],
   "source": [
    "# Long-short analysis: long top-K, short bottom-K\n",
    "def compute_long_short_returns(df, top_k=20):\n",
    "    \"\"\"Compute daily returns of long-short portfolio.\"\"\"\n",
    "    returns = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < top_k * 2:\n",
    "            continue\n",
    "        # Long top-K\n",
    "        top = group.nlargest(top_k, \"score\")\n",
    "        long_ret = top[\"target_return\"].mean()\n",
    "        # Short bottom-K\n",
    "        bottom = group.nsmallest(top_k, \"score\")\n",
    "        short_ret = bottom[\"target_return\"].mean()\n",
    "        # Long-short return (long winners, short losers)\n",
    "        ls_ret = long_ret - short_ret\n",
    "        # Market\n",
    "        mkt_ret = group[\"target_return\"].mean()\n",
    "        returns.append({\n",
    "            \"date\": date,\n",
    "            \"long_return\": long_ret,\n",
    "            \"short_return\": short_ret,\n",
    "            \"long_short_return\": ls_ret,\n",
    "            \"market_return\": mkt_ret,\n",
    "        })\n",
    "    return pd.DataFrame(returns)\n",
    "\n",
    "ls_df = compute_long_short_returns(test_df, top_k=20)\n",
    "\n",
    "print(\"Long-Short Portfolio Performance (Test Set):\")\n",
    "print(f\"\\n  Long (top-20):\")\n",
    "print(f\"    Daily mean: {ls_df['long_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Cumulative: {((1 + ls_df['long_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Short (bottom-20):\")\n",
    "print(f\"    Daily mean: {ls_df['short_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Cumulative: {((1 + ls_df['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"    Short P&L (inverted): {((1 - ls_df['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Long-Short (market neutral):\")\n",
    "print(f\"    Daily mean: {ls_df['long_short_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Std: {ls_df['long_short_return'].std()*100:.3f}%\")\n",
    "print(f\"    Sharpe: {ls_df['long_short_return'].mean() / ls_df['long_short_return'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"    Cumulative: {((1 + ls_df['long_short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Market:\")\n",
    "print(f\"    Cumulative: {((1 + ls_df['market_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "av5cknegqmf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom-20 basket analysis:\n",
      "==================================================\n",
      "\n",
      "Most frequent bottom-20 stocks:\n",
      "symbol\n",
      "SBET    63\n",
      "FIG     61\n",
      "RGC     56\n",
      "DFDV    36\n",
      "TLRY    36\n",
      "CHAI    35\n",
      "MLGO    33\n",
      "NEGG    31\n",
      "RUN     31\n",
      "QUBT    30\n",
      "UPXI    29\n",
      "ABTC    28\n",
      "ACON    27\n",
      "MRNA    27\n",
      "NCNA    26\n",
      "NUKK    26\n",
      "CHYM    25\n",
      "SPRB    25\n",
      "SMX     25\n",
      "WBUY    23\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Worst single-day returns in bottom basket:\n",
      "        feature_date symbol  target_return\n",
      "2050954   2025-11-05   MGNI      -3.540930\n",
      "2092018   2025-12-18   DFLI      -2.164435\n",
      "1793247   2025-03-31   GDHG      -2.161439\n",
      "1922868   2025-08-06    SMX      -2.019265\n",
      "2088179   2025-12-16   MENS      -1.662784\n",
      "1764290   2025-03-05   ACON      -1.474477\n",
      "1799271   2025-04-04   SUNE      -1.463255\n",
      "1743903   2025-02-12   PPCB      -1.287354\n",
      "1863749   2025-06-12   SBET      -1.261873\n",
      "1921292   2025-08-05   ELWS      -0.895174\n",
      "2079979   2025-12-05    SMX      -0.893744\n",
      "1973584   2025-09-11   NXTT      -0.853514\n",
      "1825836   2025-05-12   DEVS      -0.832146\n",
      "1882126   2025-06-27    OST      -0.791666\n",
      "1823738   2025-05-08   VEEE      -0.772743\n",
      "1804732   2025-04-17   AREB      -0.769643\n",
      "1754490   2025-02-21   PPCB      -0.769074\n",
      "1850291   2025-06-03   TJGC      -0.759105\n",
      "2019382   2025-10-14   AQMS      -0.753687\n",
      "1801380   2025-04-15   SUNE      -0.738043\n",
      "\n",
      "\n",
      "Return distribution of bottom-20 picks:\n",
      "count    3560.000000\n",
      "mean       -0.024115\n",
      "std         0.178606\n",
      "min        -3.540930\n",
      "25%        -0.067794\n",
      "50%        -0.014354\n",
      "75%         0.030491\n",
      "max         2.910574\n",
      "Name: target_return, dtype: float64\n",
      "\n",
      "\n",
      "Days with extreme short returns (< -10%):\n",
      "Number of extreme days: 12\n",
      "          date  short_return  long_return  long_short_return\n",
      "7   2025-02-21     -0.111275    -0.008033           0.103242\n",
      "12  2025-03-05     -0.110020    -0.002649           0.107371\n",
      "27  2025-03-31     -0.126192     0.000875           0.127067\n",
      "28  2025-04-02     -0.103430    -0.069123           0.034306\n",
      "30  2025-04-04     -0.137372    -0.011144           0.126228\n",
      "31  2025-04-15     -0.102900    -0.011153           0.091747\n",
      "45  2025-05-12     -0.148647     0.000489           0.149137\n",
      "65  2025-06-12     -0.183685    -0.011674           0.172012\n",
      "95  2025-08-06     -0.123636    -0.012262           0.111375\n",
      "142 2025-10-14     -0.107729     0.000520           0.108250\n"
     ]
    }
   ],
   "source": [
    "# Investigate the short basket - are these real shortable stocks or distressed junk?\n",
    "print(\"Bottom-20 basket analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get all bottom-20 selections\n",
    "bottom_picks = []\n",
    "for date, group in test_df.groupby(\"feature_date\"):\n",
    "    if len(group) < 40:\n",
    "        continue\n",
    "    bottom = group.nsmallest(20, \"score\")\n",
    "    bottom_picks.append(bottom[[\"symbol\", \"feature_date\", \"target_return\", \"score\"]])\n",
    "\n",
    "bottom_df = pd.concat(bottom_picks)\n",
    "\n",
    "print(f\"\\nMost frequent bottom-20 stocks:\")\n",
    "freq = bottom_df[\"symbol\"].value_counts().head(20)\n",
    "print(freq)\n",
    "\n",
    "print(f\"\\n\\nWorst single-day returns in bottom basket:\")\n",
    "print(bottom_df.nsmallest(20, \"target_return\")[[\"feature_date\", \"symbol\", \"target_return\"]])\n",
    "\n",
    "print(f\"\\n\\nReturn distribution of bottom-20 picks:\")\n",
    "print(bottom_df[\"target_return\"].describe())\n",
    "\n",
    "# Check for extreme outliers driving the result\n",
    "print(f\"\\n\\nDays with extreme short returns (< -10%):\")\n",
    "extreme_days = ls_df[ls_df[\"short_return\"] < -0.10]\n",
    "print(f\"Number of extreme days: {len(extreme_days)}\")\n",
    "if len(extreme_days) > 0:\n",
    "    print(extreme_days[[\"date\", \"short_return\", \"long_return\", \"long_short_return\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eltrur3kw8q",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality check:\n",
      "==================================================\n",
      "\n",
      "Rows with impossible returns (< -100%): 28\n",
      "        symbol feature_date  target_return\n",
      "1743903   PPCB   2025-02-12      -1.287354\n",
      "1746731   INLF   2025-02-14      -1.572174\n",
      "1764290   ACON   2025-03-05      -1.474477\n",
      "1767070    HIT   2025-03-11      -1.731924\n",
      "1793247   GDHG   2025-03-31      -2.161439\n",
      "1799271   SUNE   2025-04-04      -1.463255\n",
      "1863749   SBET   2025-06-12      -1.261873\n",
      "1877320    OST   2025-06-25      -2.797281\n",
      "1888515   SKBL   2025-07-03      -2.085914\n",
      "1904441   PTNM   2025-07-17      -1.132462\n",
      "1906158    YHC   2025-07-17      -1.015413\n",
      "1910322   PTHL   2025-07-28      -2.931921\n",
      "1922868    SMX   2025-08-06      -2.019265\n",
      "1935021   FLYE   2025-08-14      -2.048982\n",
      "1951089   TRIB   2025-08-26      -1.602658\n",
      "1970120   EPSM   2025-09-09      -1.228176\n",
      "1993179    SDM   2025-09-25      -1.995619\n",
      "1995860   MLTX   2025-09-26      -2.294392\n",
      "1999569    UFG   2025-10-01      -1.245843\n",
      "2006898   YYAI   2025-10-06      -2.635081\n",
      "\n",
      "Rows with extreme positive returns (> 100%): 12\n",
      "        symbol feature_date  target_return\n",
      "1752155   MLGO   2025-02-20       1.710266\n",
      "1781585   MLGO   2025-03-21       1.714084\n",
      "1796471   AREB   2025-04-03       1.594170\n",
      "1866290    RGC   2025-06-13       1.343235\n",
      "1921629    SMX   2025-08-05       1.802700\n",
      "1929360   ETHZ   2025-08-11       1.120331\n",
      "1966571   EPSM   2025-09-08       1.632158\n",
      "1968035   QMMM   2025-09-08       2.910574\n",
      "1990613   QURE   2025-09-23       1.246258\n",
      "2016467   AQMS   2025-10-13       1.094905\n"
     ]
    }
   ],
   "source": [
    "# Check for data quality issues - returns < -100% are impossible\n",
    "print(\"Data quality check:\")\n",
    "print(\"=\" * 50)\n",
    "impossible_returns = test_df[test_df[\"target_return\"] < -1.0]\n",
    "print(f\"\\nRows with impossible returns (< -100%): {len(impossible_returns)}\")\n",
    "if len(impossible_returns) > 0:\n",
    "    print(impossible_returns[[\"symbol\", \"feature_date\", \"target_return\"]].head(20))\n",
    "\n",
    "# Also check extreme positive returns\n",
    "extreme_positive = test_df[test_df[\"target_return\"] > 1.0]\n",
    "print(f\"\\nRows with extreme positive returns (> 100%): {len(extreme_positive)}\")\n",
    "if len(extreme_positive) > 0:\n",
    "    print(extreme_positive[[\"symbol\", \"feature_date\", \"target_return\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4qzxhrlc0yq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long-Short with clipped returns (max ±50%):\n",
      "==================================================\n",
      "\n",
      "  Long (top-20):\n",
      "    Cumulative: -22.8%\n",
      "\n",
      "  Short (bottom-20):\n",
      "    Cumulative: -98.2%\n",
      "    Short P&L: 3765.4%\n",
      "\n",
      "  Long-Short:\n",
      "    Daily mean: 2.020%\n",
      "    Sharpe: 8.86\n",
      "    Cumulative: 3042.7%\n"
     ]
    }
   ],
   "source": [
    "# Re-evaluate with winsorized returns (clip extreme values)\n",
    "def compute_long_short_returns_clipped(df, top_k=20, clip_pct=0.50):\n",
    "    \"\"\"Compute long-short returns with clipped extreme returns.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"target_return_clipped\"] = df[\"target_return\"].clip(-clip_pct, clip_pct)\n",
    "    \n",
    "    returns = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < top_k * 2:\n",
    "            continue\n",
    "        # Use original score for ranking, clipped returns for P&L\n",
    "        top = group.nlargest(top_k, \"score\")\n",
    "        long_ret = top[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        bottom = group.nsmallest(top_k, \"score\")\n",
    "        short_ret = bottom[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        ls_ret = long_ret - short_ret\n",
    "        mkt_ret = group[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        returns.append({\n",
    "            \"date\": date,\n",
    "            \"long_return\": long_ret,\n",
    "            \"short_return\": short_ret,\n",
    "            \"long_short_return\": ls_ret,\n",
    "            \"market_return\": mkt_ret,\n",
    "        })\n",
    "    return pd.DataFrame(returns)\n",
    "\n",
    "ls_clipped = compute_long_short_returns_clipped(test_df, top_k=20, clip_pct=0.50)\n",
    "\n",
    "print(\"Long-Short with clipped returns (max ±50%):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n  Long (top-20):\")\n",
    "print(f\"    Cumulative: {((1 + ls_clipped['long_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Short (bottom-20):\")\n",
    "print(f\"    Cumulative: {((1 + ls_clipped['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"    Short P&L: {((1 - ls_clipped['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Long-Short:\")\n",
    "print(f\"    Daily mean: {ls_clipped['long_short_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Sharpe: {ls_clipped['long_short_return'].mean() / ls_clipped['long_short_return'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"    Cumulative: {((1 + ls_clipped['long_short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcxjwdafeg",
   "metadata": {},
   "source": [
    "### 5.4 Shortability Analysis\n",
    "\n",
    "Filter to stocks that are practically shortable:\n",
    "- Market cap > $500M (institutional threshold)\n",
    "- Exclude penny stocks and micro-caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fxlrizjqnqk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market cap data for 5,564 symbols\n",
      "\n",
      "Market cap distribution:\n",
      "count    5.564000e+03\n",
      "mean     2.053745e+11\n",
      "std      5.247073e+12\n",
      "min      0.000000e+00\n",
      "25%      8.948057e+07\n",
      "50%      5.611468e+08\n",
      "75%      4.123179e+09\n",
      "max      3.111042e+14\n",
      "Name: marketCap, dtype: float64\n",
      "\n",
      "Market cap tiers:\n",
      "tier\n",
      "Micro (<$300M)       1941\n",
      "Small ($300M-$2B)    1414\n",
      "Mid ($2B-$10B)        979\n",
      "Large (>$10B)         879\n",
      "Unknown               351\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load fundamentals to get market cap\n",
    "key_metrics = pd.read_parquet(\"data/key_metrics.pqt\")\n",
    "\n",
    "# Get latest market cap per symbol (most recent quarter)\n",
    "key_metrics[\"date\"] = pd.to_datetime(key_metrics[\"date\"])\n",
    "latest_mcap = (\n",
    "    key_metrics[[\"symbol\", \"date\", \"marketCap\"]]\n",
    "    .sort_values(\"date\")\n",
    "    .groupby(\"symbol\")\n",
    "    .last()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"Market cap data for {len(latest_mcap):,} symbols\")\n",
    "print(f\"\\nMarket cap distribution:\")\n",
    "print(latest_mcap[\"marketCap\"].describe())\n",
    "\n",
    "# Define market cap tiers\n",
    "def mcap_tier(mcap):\n",
    "    if pd.isna(mcap) or mcap <= 0:\n",
    "        return \"Unknown\"\n",
    "    elif mcap < 300_000_000:\n",
    "        return \"Micro (<$300M)\"\n",
    "    elif mcap < 2_000_000_000:\n",
    "        return \"Small ($300M-$2B)\"\n",
    "    elif mcap < 10_000_000_000:\n",
    "        return \"Mid ($2B-$10B)\"\n",
    "    else:\n",
    "        return \"Large (>$10B)\"\n",
    "\n",
    "latest_mcap[\"tier\"] = latest_mcap[\"marketCap\"].apply(mcap_tier)\n",
    "print(f\"\\nMarket cap tiers:\")\n",
    "print(latest_mcap[\"tier\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cilmh1wzhwf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom-20 picks by market cap tier:\n",
      "tier\n",
      "Micro (<$300M)       1488\n",
      "Small ($300M-$2B)     780\n",
      "Mid ($2B-$10B)        745\n",
      "Large (>$10B)         319\n",
      "Unknown               228\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Most frequent bottom-20 stocks with their market caps:\n",
      "  SBET: 63 days, mcap=$2.5B\n",
      "  FIG: 61 days, mcap=$47.4B\n",
      "  RGC: 56 days, mcap=$8.4B\n",
      "  DFDV: 36 days, mcap=$0.4B\n",
      "  TLRY: 36 days, mcap=$1.5B\n",
      "  CHAI: 35 days, mcap=$0.0B\n",
      "  MLGO: 33 days, mcap=$0.0B\n",
      "  NEGG: 31 days, mcap=$0.3B\n",
      "  RUN: 31 days, mcap=$4.0B\n",
      "  QUBT: 30 days, mcap=$3.0B\n",
      "  UPXI: 29 days, mcap=$0.3B\n",
      "  ABTC: 28 days, mcap=$4.0B\n",
      "  ACON: 27 days, mcap=$0.0B\n",
      "  MRNA: 27 days, mcap=$10.1B\n",
      "  NCNA: 26 days, mcap=$0.0B\n",
      "  NUKK: 26 days, mcap=$0.1B\n",
      "  CHYM: 25 days, mcap=$7.5B\n",
      "  SPRB: 25 days, mcap=$0.0B\n",
      "  SMX: 25 days, mcap=N/A\n",
      "  WBUY: 23 days, mcap=$0.0B\n"
     ]
    }
   ],
   "source": [
    "# Merge market cap into test_df\n",
    "test_df_mcap = test_df.merge(\n",
    "    latest_mcap[[\"symbol\", \"marketCap\"]], \n",
    "    on=\"symbol\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Check how many of our bottom-20 picks have market cap data\n",
    "bottom_with_mcap = bottom_df.merge(latest_mcap[[\"symbol\", \"marketCap\"]], on=\"symbol\", how=\"left\")\n",
    "print(\"Bottom-20 picks by market cap tier:\")\n",
    "bottom_with_mcap[\"tier\"] = bottom_with_mcap[\"marketCap\"].apply(mcap_tier)\n",
    "print(bottom_with_mcap[\"tier\"].value_counts())\n",
    "\n",
    "print(\"\\n\\nMost frequent bottom-20 stocks with their market caps:\")\n",
    "freq_symbols = bottom_df[\"symbol\"].value_counts().head(20).index.tolist()\n",
    "for sym in freq_symbols:\n",
    "    mcap = latest_mcap[latest_mcap[\"symbol\"] == sym][\"marketCap\"].values\n",
    "    mcap_str = f\"${mcap[0]/1e9:.1f}B\" if len(mcap) > 0 and mcap[0] > 0 else \"N/A\"\n",
    "    count = bottom_df[bottom_df[\"symbol\"] == sym].shape[0]\n",
    "    print(f\"  {sym}: {count} days, mcap={mcap_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "qk7ud97hd6p",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortable universe (mcap >= $500M): 2,861 symbols\n",
      "Test rows: 352,213 -> 334,455 (95.0%)\n",
      "\n",
      "Long-Short on Shortable Universe (mcap >= $500M):\n",
      "==================================================\n",
      "\n",
      "  Trading days: 178\n",
      "\n",
      "  Long (top-20):\n",
      "    Cumulative: -20.6%\n",
      "\n",
      "  Short (bottom-20):\n",
      "    Cumulative: -49.4%\n",
      "    Short P&L: 72.2%\n",
      "\n",
      "  Long-Short:\n",
      "    Daily mean: 0.222%\n",
      "    Sharpe: 1.62\n",
      "    Cumulative: 42.4%\n",
      "    Win rate: 53.4%\n"
     ]
    }
   ],
   "source": [
    "# Re-evaluate long-short with market cap filter\n",
    "MIN_MCAP = 500_000_000  # $500M minimum\n",
    "\n",
    "# Filter to shortable universe\n",
    "shortable_symbols = set(latest_mcap[latest_mcap[\"marketCap\"] >= MIN_MCAP][\"symbol\"])\n",
    "test_shortable = test_df_mcap[test_df_mcap[\"symbol\"].isin(shortable_symbols)].copy()\n",
    "\n",
    "print(f\"Shortable universe (mcap >= ${MIN_MCAP/1e6:.0f}M): {len(shortable_symbols):,} symbols\")\n",
    "print(f\"Test rows: {len(test_df_mcap):,} -> {len(test_shortable):,} ({len(test_shortable)/len(test_df_mcap)*100:.1f}%)\")\n",
    "\n",
    "# Re-run long-short on shortable universe\n",
    "def compute_long_short_returns_filtered(df, top_k=20, clip_pct=0.50):\n",
    "    \"\"\"Long-short with clipped returns on filtered universe.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"target_return_clipped\"] = df[\"target_return\"].clip(-clip_pct, clip_pct)\n",
    "    \n",
    "    returns = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < top_k * 2:\n",
    "            continue\n",
    "        \n",
    "        top = group.nlargest(top_k, \"score\")\n",
    "        long_ret = top[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        bottom = group.nsmallest(top_k, \"score\")\n",
    "        short_ret = bottom[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        ls_ret = long_ret - short_ret\n",
    "        mkt_ret = group[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        returns.append({\n",
    "            \"date\": date,\n",
    "            \"long_return\": long_ret,\n",
    "            \"short_return\": short_ret,\n",
    "            \"long_short_return\": ls_ret,\n",
    "            \"market_return\": mkt_ret,\n",
    "        })\n",
    "    return pd.DataFrame(returns)\n",
    "\n",
    "ls_shortable = compute_long_short_returns_filtered(test_shortable, top_k=20, clip_pct=0.50)\n",
    "\n",
    "print(f\"\\nLong-Short on Shortable Universe (mcap >= $500M):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n  Trading days: {len(ls_shortable)}\")\n",
    "\n",
    "print(f\"\\n  Long (top-20):\")\n",
    "print(f\"    Cumulative: {((1 + ls_shortable['long_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Short (bottom-20):\")\n",
    "print(f\"    Cumulative: {((1 + ls_shortable['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"    Short P&L: {((1 - ls_shortable['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Long-Short:\")\n",
    "print(f\"    Daily mean: {ls_shortable['long_short_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Sharpe: {ls_shortable['long_short_return'].mean() / ls_shortable['long_short_return'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"    Cumulative: {((1 + ls_shortable['long_short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"    Win rate: {(ls_shortable['long_short_return'] > 0).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "higo5py96vg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent bottom-20 stocks (shortable universe):\n",
      "  SBET: 81 days, mcap=$2.5B\n",
      "  FIG: 61 days, mcap=$47.4B\n",
      "  RGC: 56 days, mcap=$8.4B\n",
      "  TLRY: 53 days, mcap=$1.5B\n",
      "  QUBT: 49 days, mcap=$3.0B\n",
      "  AEVA: 43 days, mcap=$0.8B\n",
      "  RUN: 40 days, mcap=$4.0B\n",
      "  MRNA: 38 days, mcap=$10.1B\n",
      "  ABTC: 38 days, mcap=$4.0B\n",
      "  ARQQ: 36 days, mcap=$0.6B\n",
      "  BEPC: 35 days, mcap=$5.9B\n",
      "  PTON: 32 days, mcap=$3.7B\n",
      "  PSKY: 31 days, mcap=$20.8B\n",
      "  FTRE: 30 days, mcap=$0.8B\n",
      "  ALAB: 30 days, mcap=$32.8B\n",
      "  GRAL: 29 days, mcap=$2.1B\n",
      "  AMN: 28 days, mcap=$0.7B\n",
      "  PL: 27 days, mcap=$1.9B\n",
      "  ASTS: 26 days, mcap=$16.6B\n",
      "  CHYM: 26 days, mcap=$7.5B\n",
      "\n",
      "\n",
      "Return distribution of shortable bottom-20:\n",
      "count    3560.000000\n",
      "mean       -0.004362\n",
      "std         0.102958\n",
      "min        -3.540930\n",
      "25%        -0.031859\n",
      "50%        -0.003293\n",
      "75%         0.024884\n",
      "max         1.343235\n",
      "Name: target_return, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# What stocks end up in the shortable bottom-20?\n",
    "bottom_shortable = []\n",
    "for date, group in test_shortable.groupby(\"feature_date\"):\n",
    "    if len(group) < 40:\n",
    "        continue\n",
    "    bottom = group.nsmallest(20, \"score\")\n",
    "    bottom_shortable.append(bottom[[\"symbol\", \"feature_date\", \"target_return\", \"score\", \"marketCap\"]])\n",
    "\n",
    "bottom_shortable_df = pd.concat(bottom_shortable)\n",
    "\n",
    "print(\"Most frequent bottom-20 stocks (shortable universe):\")\n",
    "freq = bottom_shortable_df[\"symbol\"].value_counts().head(20)\n",
    "for sym, count in freq.items():\n",
    "    mcap = bottom_shortable_df[bottom_shortable_df[\"symbol\"] == sym][\"marketCap\"].iloc[0]\n",
    "    print(f\"  {sym}: {count} days, mcap=${mcap/1e9:.1f}B\")\n",
    "\n",
    "print(f\"\\n\\nReturn distribution of shortable bottom-20:\")\n",
    "print(bottom_shortable_df[\"target_return\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1b2c3d4-0001-0001-0001-000000000031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to data/model_final.pt\n"
     ]
    }
   ],
   "source": [
    "# Save model and config\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"config\": config,\n",
    "    \"price_cols\": price_feat_cols,\n",
    "    \"fund_cols\": fund_feat_cols,\n",
    "    \"emb_cols\": emb_cols,\n",
    "}, \"data/model_final.pt\")\n",
    "\n",
    "print(\"Model saved to data/model_final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1b2c3d4-0001-0001-0001-000000000032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.692637</td>\n",
       "      <td>0.513108</td>\n",
       "      <td>0.692965</td>\n",
       "      <td>0.504324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.692459</td>\n",
       "      <td>0.516030</td>\n",
       "      <td>0.692730</td>\n",
       "      <td>0.509986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.692179</td>\n",
       "      <td>0.517811</td>\n",
       "      <td>0.692919</td>\n",
       "      <td>0.506770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.691949</td>\n",
       "      <td>0.519894</td>\n",
       "      <td>0.692892</td>\n",
       "      <td>0.511132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.691778</td>\n",
       "      <td>0.521649</td>\n",
       "      <td>0.693288</td>\n",
       "      <td>0.506512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.691481</td>\n",
       "      <td>0.523973</td>\n",
       "      <td>0.693141</td>\n",
       "      <td>0.505377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.691127</td>\n",
       "      <td>0.526862</td>\n",
       "      <td>0.693427</td>\n",
       "      <td>0.506365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.690606</td>\n",
       "      <td>0.528435</td>\n",
       "      <td>0.693693</td>\n",
       "      <td>0.503291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.690073</td>\n",
       "      <td>0.531941</td>\n",
       "      <td>0.693685</td>\n",
       "      <td>0.504475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.689611</td>\n",
       "      <td>0.534544</td>\n",
       "      <td>0.693882</td>\n",
       "      <td>0.504464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.689147</td>\n",
       "      <td>0.536911</td>\n",
       "      <td>0.693835</td>\n",
       "      <td>0.506290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.688536</td>\n",
       "      <td>0.539270</td>\n",
       "      <td>0.694643</td>\n",
       "      <td>0.503181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.688243</td>\n",
       "      <td>0.541222</td>\n",
       "      <td>0.694102</td>\n",
       "      <td>0.504588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.687659</td>\n",
       "      <td>0.543814</td>\n",
       "      <td>0.694815</td>\n",
       "      <td>0.504625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.687389</td>\n",
       "      <td>0.545180</td>\n",
       "      <td>0.694729</td>\n",
       "      <td>0.503394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.687114</td>\n",
       "      <td>0.546473</td>\n",
       "      <td>0.694885</td>\n",
       "      <td>0.504298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.686938</td>\n",
       "      <td>0.546455</td>\n",
       "      <td>0.695191</td>\n",
       "      <td>0.502764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.686799</td>\n",
       "      <td>0.547550</td>\n",
       "      <td>0.694904</td>\n",
       "      <td>0.503322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.686610</td>\n",
       "      <td>0.547800</td>\n",
       "      <td>0.694853</td>\n",
       "      <td>0.503248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.686383</td>\n",
       "      <td>0.548271</td>\n",
       "      <td>0.695185</td>\n",
       "      <td>0.501354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  train_loss  train_acc  val_loss   val_acc\n",
       "0       1    0.692637   0.513108  0.692965  0.504324\n",
       "1       2    0.692459   0.516030  0.692730  0.509986\n",
       "2       3    0.692179   0.517811  0.692919  0.506770\n",
       "3       4    0.691949   0.519894  0.692892  0.511132\n",
       "4       5    0.691778   0.521649  0.693288  0.506512\n",
       "5       6    0.691481   0.523973  0.693141  0.505377\n",
       "6       7    0.691127   0.526862  0.693427  0.506365\n",
       "7       8    0.690606   0.528435  0.693693  0.503291\n",
       "8       9    0.690073   0.531941  0.693685  0.504475\n",
       "9      10    0.689611   0.534544  0.693882  0.504464\n",
       "10     11    0.689147   0.536911  0.693835  0.506290\n",
       "11     12    0.688536   0.539270  0.694643  0.503181\n",
       "12     13    0.688243   0.541222  0.694102  0.504588\n",
       "13     14    0.687659   0.543814  0.694815  0.504625\n",
       "14     15    0.687389   0.545180  0.694729  0.503394\n",
       "15     16    0.687114   0.546473  0.694885  0.504298\n",
       "16     17    0.686938   0.546455  0.695191  0.502764\n",
       "17     18    0.686799   0.547550  0.694904  0.503322\n",
       "18     19    0.686610   0.547800  0.694853  0.503248\n",
       "19     20    0.686383   0.548271  0.695185  0.501354"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training history\n",
    "history_df = pd.DataFrame(history)\n",
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7544d91-00f8-4a56-aed7-f881bcb4e4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6120efb-3e45-4704-9ab3-a3cc1e3ee810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
