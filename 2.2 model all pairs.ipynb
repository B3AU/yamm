{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0000-0001-0001-000000000000",
   "metadata": {},
   "source": [
    "# Model Training (All Pairs)\n",
    "\n",
    "Multi-branch neural model with influence-controlled news embedding.\n",
    "\n",
    "**Difference from 2.0**: This version generates ALL possible pairs per day rather than sampling. With ~160 stocks/day with news, that's ~12,700 pairs/day.\n",
    "\n",
    "**Table of Contents:**\n",
    "1. Load and split data\n",
    "2. Dataset and DataLoader\n",
    "3. Model architecture\n",
    "4. Training loop\n",
    "5. Evaluation: Rank IC and Basket Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1b2c3d4-0001-0001-0001-000000000001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b2c3d4-0001-0001-0001-000000000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelConfig:\n",
    "    # Feature dimensions (set after loading data)\n",
    "    n_fundamental_features: int = 19\n",
    "    n_price_features: int = 9\n",
    "    n_embedding_dim: int = 768\n",
    "    \n",
    "    # Encoder latent dimensions\n",
    "    fundamental_latent: int = 32\n",
    "    price_latent: int = 16\n",
    "    news_latent: int = 32\n",
    "    \n",
    "    # Dropout\n",
    "    fundamental_dropout: float = 0.2\n",
    "    price_dropout: float = 0.2\n",
    "    news_dropout: float = 0.3\n",
    "    \n",
    "    # News influence cap\n",
    "    news_alpha: float = 0.8\n",
    "    \n",
    "    # Training\n",
    "    batch_size: int = 512\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-3\n",
    "    n_epochs: int = 20\n",
    "    pairs_per_day: int = 5000\n",
    "\n",
    "    # Random pairs only (no hard pair oversampling)\n",
    "    hard_fraction: float = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000003",
   "metadata": {},
   "source": [
    "## 1. Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1b2c3d4-0001-0001-0001-000000000004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 2,092,929 rows\n",
      "Date range: 2021-01-13 to 2025-12-18\n",
      "Symbols: 3,506\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"data/ml_dataset.pqt\")\n",
    "df[\"feature_date\"] = pd.to_datetime(df[\"feature_date\"])\n",
    "\n",
    "print(f\"Dataset: {len(df):,} rows\")\n",
    "print(f\"Date range: {df['feature_date'].min().date()} to {df['feature_date'].max().date()}\")\n",
    "print(f\"Symbols: {df['symbol'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b2c3d4-0001-0001-0001-000000000005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price features: 9\n",
      "Fundamental features: 19\n",
      "Embedding dims: 768\n"
     ]
    }
   ],
   "source": [
    "# Identify feature columns\n",
    "price_feat_cols = [\n",
    "    \"overnight_gap_z\", \"intraday_ret_z\",\n",
    "    \"ret_1d_z\", \"ret_2d_z\", \"ret_3d_z\", \"ret_5d_z\",\n",
    "    \"vol_5d_z\", \"dist_from_high_5d_z\", \"dist_from_low_5d_z\"\n",
    "]\n",
    "\n",
    "# Fundamental feature columns (normalized)\n",
    "fund_feat_cols = [c for c in df.columns if c.endswith(\"_z\") and c not in price_feat_cols and c != \"news_count_z\"]\n",
    "\n",
    "# Embedding columns\n",
    "emb_cols = [c for c in df.columns if c.startswith(\"emb_\")]\n",
    "\n",
    "print(f\"Price features: {len(price_feat_cols)}\")\n",
    "print(f\"Fundamental features: {len(fund_feat_cols)}\")\n",
    "print(f\"Embedding dims: {len(emb_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b2c3d4-0001-0001-0001-000000000006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1,418,494 rows, 830 days\n",
      "Val: 322,222 rows, 178 days\n",
      "Test: 352,213 rows, 178 days\n"
     ]
    }
   ],
   "source": [
    "# Time-based split (per README section 13)\n",
    "# Use last 20% of dates as test\n",
    "dates = sorted(df[\"feature_date\"].unique())\n",
    "n_dates = len(dates)\n",
    "train_end_idx = int(n_dates * 0.7)\n",
    "val_end_idx = int(n_dates * 0.85)\n",
    "\n",
    "train_dates = set(dates[:train_end_idx])\n",
    "val_dates = set(dates[train_end_idx:val_end_idx])\n",
    "test_dates = set(dates[val_end_idx:])\n",
    "\n",
    "train_df = df[df[\"feature_date\"].isin(train_dates)].copy()\n",
    "val_df = df[df[\"feature_date\"].isin(val_dates)].copy()\n",
    "test_df = df[df[\"feature_date\"].isin(test_dates)].copy()\n",
    "\n",
    "print(f\"Train: {len(train_df):,} rows, {len(train_dates)} days\")\n",
    "print(f\"Val: {len(val_df):,} rows, {len(val_dates)} days\")\n",
    "print(f\"Test: {len(test_df):,} rows, {len(test_dates)} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000007",
   "metadata": {},
   "source": [
    "## 2. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1b2c3d4-0001-0001-0001-000000000008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairwiseRankingDataset(Dataset):\n",
    "    \"\"\"Dataset that generates ALL pairs from same day for ranking loss.\n",
    "\n",
    "    Only uses rows with news. Generates all N*(N-1)/2 unique pairs per day.\n",
    "    \n",
    "    Call resample_pairs() at the start of each epoch to shuffle the order.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        price_cols: list[str],\n",
    "        fund_cols: list[str],\n",
    "        emb_cols: list[str],\n",
    "    ):\n",
    "        # Filter to rows with news only\n",
    "        has_news = (df[emb_cols].abs().sum(axis=1) > 0)\n",
    "        df_news = df[has_news].copy()\n",
    "        print(f\"Filtered to news-only: {len(df_news):,} / {len(df):,} rows ({len(df_news)/len(df)*100:.1f}%)\")\n",
    "\n",
    "        self.df = df_news.reset_index(drop=True)\n",
    "        self.price_cols = price_cols\n",
    "        self.fund_cols = fund_cols\n",
    "        self.emb_cols = emb_cols\n",
    "\n",
    "        # Group by date\n",
    "        self.date_groups = {}\n",
    "        for date, group in df_news.groupby(\"feature_date\"):\n",
    "            indices = group.index.tolist()\n",
    "            if len(indices) < 2:  # Need at least 2 for pairs\n",
    "                continue\n",
    "            self.date_groups[date] = np.array(indices)\n",
    "\n",
    "        self.dates = list(self.date_groups.keys())\n",
    "        print(f\"Days with sufficient news coverage: {len(self.dates)}\")\n",
    "\n",
    "        # Precompute arrays for speed\n",
    "        self.price_arr = df_news[price_cols].values.astype(np.float32)\n",
    "        self.fund_arr = df_news[fund_cols].values.astype(np.float32)\n",
    "        self.emb_arr = df_news[emb_cols].values.astype(np.float32)\n",
    "        self.target_arr = df_news[\"target_return\"].values.astype(np.float32)\n",
    "\n",
    "        # Map original index to position in filtered df\n",
    "        self.idx_map = {old_idx: new_idx for new_idx, old_idx in enumerate(df_news.index)}\n",
    "\n",
    "        # Generate all pairs\n",
    "        self.pairs = []\n",
    "        self._generate_all_pairs()\n",
    "        \n",
    "    def _generate_all_pairs(self):\n",
    "        \"\"\"Generate ALL unique pairs for each day.\"\"\"\n",
    "        pairs = []\n",
    "        total_possible = 0\n",
    "\n",
    "        for date in self.dates:\n",
    "            indices = self.date_groups[date]\n",
    "            n = len(indices)\n",
    "            total_possible += n * (n - 1) // 2\n",
    "            \n",
    "            # Generate all unique pairs (i, j) where i < j\n",
    "            for i in range(n):\n",
    "                for j in range(i + 1, n):\n",
    "                    idx_i = self.idx_map[indices[i]]\n",
    "                    idx_j = self.idx_map[indices[j]]\n",
    "                    pairs.append((idx_i, idx_j))\n",
    "\n",
    "        self.pairs = pairs\n",
    "        print(f\"Generated {len(self.pairs):,} pairs (all combinations)\")\n",
    "        \n",
    "    def resample_pairs(self):\n",
    "        \"\"\"Shuffle pairs for new epoch (no resampling needed since we use all).\"\"\"\n",
    "        np.random.shuffle(self.pairs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i, j = self.pairs[idx]\n",
    "\n",
    "        # Get features\n",
    "        price_i = self.price_arr[i]\n",
    "        price_j = self.price_arr[j]\n",
    "        fund_i = self.fund_arr[i]\n",
    "        fund_j = self.fund_arr[j]\n",
    "        emb_i = self.emb_arr[i]\n",
    "        emb_j = self.emb_arr[j]\n",
    "\n",
    "        # Label based on actual returns\n",
    "        actual_label = 1.0 if self.target_arr[i] > self.target_arr[j] else 0.0\n",
    "\n",
    "        # Randomly swap to balance labels\n",
    "        if np.random.random() < 0.5:\n",
    "            price_i, price_j = price_j, price_i\n",
    "            fund_i, fund_j = fund_j, fund_i\n",
    "            emb_i, emb_j = emb_j, emb_i\n",
    "            label = 1.0 - actual_label\n",
    "        else:\n",
    "            label = actual_label\n",
    "\n",
    "        return {\n",
    "            \"price_i\": torch.tensor(price_i),\n",
    "            \"price_j\": torch.tensor(price_j),\n",
    "            \"fund_i\": torch.tensor(fund_i),\n",
    "            \"fund_j\": torch.tensor(fund_j),\n",
    "            \"emb_i\": torch.tensor(emb_i),\n",
    "            \"emb_j\": torch.tensor(emb_j),\n",
    "            \"label\": torch.tensor(label),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b2c3d4-0001-0001-0001-000000000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointwiseDataset(Dataset):\n",
    "    \"\"\"Dataset for inference - one sample per stock-day.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        price_cols: list[str],\n",
    "        fund_cols: list[str],\n",
    "        emb_cols: list[str],\n",
    "    ):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.price_arr = df[price_cols].values.astype(np.float32)\n",
    "        self.fund_arr = df[fund_cols].values.astype(np.float32)\n",
    "        self.emb_arr = df[emb_cols].values.astype(np.float32)\n",
    "        self.target_arr = df[\"target_return\"].values.astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"price\": torch.tensor(self.price_arr[idx]),\n",
    "            \"fund\": torch.tensor(self.fund_arr[idx]),\n",
    "            \"emb\": torch.tensor(self.emb_arr[idx]),\n",
    "            \"target\": torch.tensor(self.target_arr[idx]),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b2c3d4-0001-0001-0001-000000000010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: ModelConfig(n_fundamental_features=19, n_price_features=9, n_embedding_dim=768, fundamental_latent=32, price_latent=16, news_latent=32, fundamental_dropout=0.2, price_dropout=0.2, news_dropout=0.3, news_alpha=0.8, batch_size=512, learning_rate=0.001, weight_decay=0.001, n_epochs=20, pairs_per_day=5000, hard_fraction=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Create config with actual dimensions\n",
    "config = ModelConfig(\n",
    "    n_fundamental_features=len(fund_feat_cols),\n",
    "    n_price_features=len(price_feat_cols),\n",
    "    n_embedding_dim=len(emb_cols),\n",
    ")\n",
    "\n",
    "print(f\"Config: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1b2c3d4-0001-0001-0001-000000000011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to news-only: 125,880 / 1,418,494 rows (8.9%)\n",
      "Days with sufficient news coverage: 830\n",
      "Generated 9,742,161 pairs (all combinations)\n",
      "Filtered to news-only: 32,360 / 322,222 rows (10.0%)\n",
      "Days with sufficient news coverage: 178\n",
      "Generated 2,981,143 pairs (all combinations)\n",
      "\n",
      "Train pairs: 9,742,161\n",
      "Val pairs: 2,981,143\n"
     ]
    }
   ],
   "source": [
    "# Create datasets with ALL pairs\n",
    "train_dataset = PairwiseRankingDataset(\n",
    "    train_df, price_feat_cols, fund_feat_cols, emb_cols,\n",
    ")\n",
    "val_dataset = PairwiseRankingDataset(\n",
    "    val_df, price_feat_cols, fund_feat_cols, emb_cols,\n",
    ")\n",
    "\n",
    "# For evaluation (pointwise)\n",
    "val_pointwise = PointwiseDataset(val_df, price_feat_cols, fund_feat_cols, emb_cols)\n",
    "test_pointwise = PointwiseDataset(test_df, price_feat_cols, fund_feat_cols, emb_cols)\n",
    "\n",
    "print(f\"\\nTrain pairs: {len(train_dataset):,}\")\n",
    "print(f\"Val pairs: {len(val_dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1b2c3d4-0001-0001-0001-000000000012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "# num_workers=0 for Jupyter compatibility (avoids multiprocessing issues)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000013",
   "metadata": {},
   "source": [
    "## 3. Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1b2c3d4-0001-0001-0001-000000000014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBranchRanker(nn.Module):\n",
    "    \"\"\"Multi-branch model with influence-controlled news embedding.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Fundamentals encoder\n",
    "        self.fund_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_fundamental_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.fundamental_dropout),\n",
    "            nn.Linear(64, config.fundamental_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Price encoder\n",
    "        self.price_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_price_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.price_dropout),\n",
    "            nn.Linear(32, config.price_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # News encoder\n",
    "        self.news_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.news_dropout),\n",
    "            nn.Linear(128, config.news_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Output head\n",
    "        fused_dim = config.fundamental_latent + config.price_latent + config.news_latent\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "    \n",
    "    def encode(self, price: torch.Tensor, fund: torch.Tensor, emb: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Encode features and return fused representation.\"\"\"\n",
    "        h_f = self.fund_encoder(fund)\n",
    "        h_p = self.price_encoder(price)\n",
    "        h_n = self.news_encoder(emb)\n",
    "        \n",
    "        # Apply news influence cap (Î± * h_n)\n",
    "        h_n_scaled = self.config.news_alpha * h_n\n",
    "        \n",
    "        # Fuse\n",
    "        h = torch.cat([h_f, h_p, h_n_scaled], dim=-1)\n",
    "        return h\n",
    "    \n",
    "    def forward(self, price: torch.Tensor, fund: torch.Tensor, emb: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass returning score.\"\"\"\n",
    "        h = self.encode(price, fund, emb)\n",
    "        score = self.output_head(h)\n",
    "        return score.squeeze(-1)\n",
    "    \n",
    "    def forward_pair(\n",
    "        self,\n",
    "        price_i: torch.Tensor, fund_i: torch.Tensor, emb_i: torch.Tensor,\n",
    "        price_j: torch.Tensor, fund_j: torch.Tensor, emb_j: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass for pair, returning P(i > j).\"\"\"\n",
    "        score_i = self.forward(price_i, fund_i, emb_i)\n",
    "        score_j = self.forward(price_j, fund_j, emb_j)\n",
    "        return torch.sigmoid(score_i - score_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1b2c3d4-0001-0001-0001-000000000015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 109,393\n",
      "MultiBranchRanker(\n",
      "  (fund_encoder): Sequential(\n",
      "    (0): Linear(in_features=19, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (price_encoder): Sequential(\n",
      "    (0): Linear(in_features=9, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (news_encoder): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (output_head): Sequential(\n",
      "    (0): Linear(in_features=80, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MultiBranchRanker(config).to(device)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params:,}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000016",
   "metadata": {},
   "source": [
    "## 4. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1b2c3d4-0001-0001-0001-000000000017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_ranking_loss(pred_prob: torch.Tensor, label: torch.Tensor, smoothing: float = 0.2) -> torch.Tensor:\n",
    "    \"\"\"Binary cross-entropy for pairwise ranking with label smoothing.\"\"\"\n",
    "    # Smooth labels: 0 -> smoothing/2, 1 -> 1 - smoothing/2\n",
    "    smoothed_label = label * (1 - smoothing) + 0.5 * smoothing\n",
    "    return F.binary_cross_entropy(pred_prob, smoothed_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1b2c3d4-0001-0001-0001-000000000018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        price_i = batch[\"price_i\"].to(device)\n",
    "        price_j = batch[\"price_j\"].to(device)\n",
    "        fund_i = batch[\"fund_i\"].to(device)\n",
    "        fund_j = batch[\"fund_j\"].to(device)\n",
    "        emb_i = batch[\"emb_i\"].to(device)\n",
    "        emb_j = batch[\"emb_j\"].to(device)\n",
    "        label = batch[\"label\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred_prob = model.forward_pair(price_i, fund_i, emb_i, price_j, fund_j, emb_j)\n",
    "        loss = pairwise_ranking_loss(pred_prob, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(label)\n",
    "        total_correct += ((pred_prob > 0.5) == (label > 0.5)).sum().item()\n",
    "        total_samples += len(label)\n",
    "    \n",
    "    return total_loss / total_samples, total_correct / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1b2c3d4-0001-0001-0001-000000000019",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        price_i = batch[\"price_i\"].to(device)\n",
    "        price_j = batch[\"price_j\"].to(device)\n",
    "        fund_i = batch[\"fund_i\"].to(device)\n",
    "        fund_j = batch[\"fund_j\"].to(device)\n",
    "        emb_i = batch[\"emb_i\"].to(device)\n",
    "        emb_j = batch[\"emb_j\"].to(device)\n",
    "        label = batch[\"label\"].to(device)\n",
    "        \n",
    "        pred_prob = model.forward_pair(price_i, fund_i, emb_i, price_j, fund_j, emb_j)\n",
    "        loss = pairwise_ranking_loss(pred_prob, label)\n",
    "        \n",
    "        total_loss += loss.item() * len(label)\n",
    "        total_correct += ((pred_prob > 0.5) == (label > 0.5)).sum().item()\n",
    "        total_samples += len(label)\n",
    "    \n",
    "    return total_loss / total_samples, total_correct / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1b2c3d4-0001-0001-0001-000000000020",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config.n_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc486e7-228a-45ed-a3f8-73aba931429e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87d90e02-417b-4a37-a46e-c266aa0f1fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"data/model_best_all_pairs_ep3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1b2c3d4-0001-0001-0001-000000000021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/19028 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5823 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: train_loss=0.6823, train_acc=0.5603, val_loss=0.7039, val_acc=0.5036\n",
      "  -> New best model saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/19028 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5823 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: train_loss=0.6647, train_acc=0.6064, val_loss=0.7180, val_acc=0.5022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/19028 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5823 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: train_loss=0.6569, train_acc=0.6224, val_loss=0.7219, val_acc=0.5020\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470f474695e447818d24a1e28e58d116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/19028 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m train_dataset\u001b[38;5;241m.\u001b[39mresample_pairs()\n\u001b[1;32m      8\u001b[0m val_dataset\u001b[38;5;241m.\u001b[39mresample_pairs()\n\u001b[0;32m---> 10\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m eval_epoch(model, val_loader, device)\n\u001b[1;32m     12\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m total_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      8\u001b[0m     price_i \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_i\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m     price_j \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_j\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    738\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:788\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    787\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 788\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    790\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[6], line 105\u001b[0m, in \u001b[0;36mPairwiseRankingDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     label \u001b[38;5;241m=\u001b[39m actual_label\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_i\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(price_i),\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_j\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(price_j),\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfund_i\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(fund_i),\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfund_j\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(fund_j),\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memb_i\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb_i\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memb_j\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(emb_j),\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(label),\n\u001b[1;32m    108\u001b[0m }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "best_val_acc = 0\n",
    "history = []\n",
    "\n",
    "for epoch in range(config.n_epochs):\n",
    "    # Resample pairs each epoch for fresh training data\n",
    "    train_dataset.resample_pairs()\n",
    "    val_dataset.resample_pairs()\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss, val_acc = eval_epoch(model, val_loader, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    history.append({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "    })\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{config.n_epochs}: \"\n",
    "          f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, \"\n",
    "          f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"data/model_best_all_pairs.pt\")\n",
    "        print(f\"  -> New best model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000022",
   "metadata": {},
   "source": [
    "## 5. Evaluation: Rank IC and Basket Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"data/model_best.pt\", weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000024",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_scores(model, df, price_cols, fund_cols, emb_cols, device, batch_size=1024):\n",
    "    \"\"\"Get model scores for all rows.\"\"\"\n",
    "    model.eval()\n",
    "    dataset = PointwiseDataset(df, price_cols, fund_cols, emb_cols)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    scores = []\n",
    "    for batch in tqdm(loader, desc=\"Scoring\", leave=False):\n",
    "        price = batch[\"price\"].to(device)\n",
    "        fund = batch[\"fund\"].to(device)\n",
    "        emb = batch[\"emb\"].to(device)\n",
    "        \n",
    "        score = model(price, fund, emb)\n",
    "        scores.append(score.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score test set\n",
    "test_df = test_df.copy()\n",
    "test_df[\"score\"] = get_scores(model, test_df, price_feat_cols, fund_feat_cols, emb_cols, device)\n",
    "\n",
    "print(f\"Scored {len(test_df):,} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def compute_daily_ic(df):\n",
    "    \"\"\"Compute Spearman rank IC per day.\"\"\"\n",
    "    ics = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < 10:\n",
    "            continue\n",
    "        ic, _ = spearmanr(group[\"score\"], group[\"target_return\"])\n",
    "        if not np.isnan(ic):\n",
    "            ics.append({\"date\": date, \"ic\": ic})\n",
    "    return pd.DataFrame(ics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000027",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_df = compute_daily_ic(test_df)\n",
    "\n",
    "print(f\"Daily Rank IC (Spearman):\")\n",
    "print(f\"  Mean: {ic_df['ic'].mean():.4f}\")\n",
    "print(f\"  Std: {ic_df['ic'].std():.4f}\")\n",
    "print(f\"  Sharpe (IC): {ic_df['ic'].mean() / ic_df['ic'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"  % Positive: {(ic_df['ic'] > 0).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_basket_returns(df, top_k=20):\n",
    "    \"\"\"Compute daily returns of top-K basket (equal weight).\"\"\"\n",
    "    returns = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < top_k:\n",
    "            continue\n",
    "        # Select top-K by score\n",
    "        top = group.nlargest(top_k, \"score\")\n",
    "        # Equal weight return\n",
    "        ret = top[\"target_return\"].mean()\n",
    "        # Market return (all stocks)\n",
    "        mkt_ret = group[\"target_return\"].mean()\n",
    "        returns.append({\n",
    "            \"date\": date,\n",
    "            \"basket_return\": ret,\n",
    "            \"market_return\": mkt_ret,\n",
    "            \"excess_return\": ret - mkt_ret,\n",
    "        })\n",
    "    return pd.DataFrame(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000029",
   "metadata": {},
   "outputs": [],
   "source": [
    "basket_df = compute_basket_returns(test_df, top_k=20)\n",
    "\n",
    "print(f\"\\nTop-20 Basket Performance (Test Set):\")\n",
    "print(f\"  Daily mean return: {basket_df['basket_return'].mean()*100:.3f}%\")\n",
    "print(f\"  Daily mean excess: {basket_df['excess_return'].mean()*100:.3f}%\")\n",
    "print(f\"  Sharpe (basket): {basket_df['basket_return'].mean() / basket_df['basket_return'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"  Sharpe (excess): {basket_df['excess_return'].mean() / basket_df['excess_return'].std() * np.sqrt(252):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative returns\n",
    "basket_df[\"cum_basket\"] = (1 + basket_df[\"basket_return\"]).cumprod()\n",
    "basket_df[\"cum_market\"] = (1 + basket_df[\"market_return\"]).cumprod()\n",
    "basket_df[\"cum_excess\"] = (1 + basket_df[\"excess_return\"]).cumprod()\n",
    "\n",
    "print(f\"\\nCumulative Returns:\")\n",
    "print(f\"  Basket: {(basket_df['cum_basket'].iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"  Market: {(basket_df['cum_market'].iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"  Excess: {(basket_df['cum_excess'].iloc[-1] - 1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o5vcmhiugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long-short analysis: long top-K, short bottom-K\n",
    "def compute_long_short_returns(df, top_k=20):\n",
    "    \"\"\"Compute daily returns of long-short portfolio.\"\"\"\n",
    "    returns = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < top_k * 2:\n",
    "            continue\n",
    "        # Long top-K\n",
    "        top = group.nlargest(top_k, \"score\")\n",
    "        long_ret = top[\"target_return\"].mean()\n",
    "        # Short bottom-K\n",
    "        bottom = group.nsmallest(top_k, \"score\")\n",
    "        short_ret = bottom[\"target_return\"].mean()\n",
    "        # Long-short return (long winners, short losers)\n",
    "        ls_ret = long_ret - short_ret\n",
    "        # Market\n",
    "        mkt_ret = group[\"target_return\"].mean()\n",
    "        returns.append({\n",
    "            \"date\": date,\n",
    "            \"long_return\": long_ret,\n",
    "            \"short_return\": short_ret,\n",
    "            \"long_short_return\": ls_ret,\n",
    "            \"market_return\": mkt_ret,\n",
    "        })\n",
    "    return pd.DataFrame(returns)\n",
    "\n",
    "ls_df = compute_long_short_returns(test_df, top_k=20)\n",
    "\n",
    "print(\"Long-Short Portfolio Performance (Test Set):\")\n",
    "print(f\"\\n  Long (top-20):\")\n",
    "print(f\"    Daily mean: {ls_df['long_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Cumulative: {((1 + ls_df['long_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Short (bottom-20):\")\n",
    "print(f\"    Daily mean: {ls_df['short_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Cumulative: {((1 + ls_df['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"    Short P&L (inverted): {((1 - ls_df['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Long-Short (market neutral):\")\n",
    "print(f\"    Daily mean: {ls_df['long_short_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Std: {ls_df['long_short_return'].std()*100:.3f}%\")\n",
    "print(f\"    Sharpe: {ls_df['long_short_return'].mean() / ls_df['long_short_return'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"    Cumulative: {((1 + ls_df['long_short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Market:\")\n",
    "print(f\"    Cumulative: {((1 + ls_df['market_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "av5cknegqmf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the short basket - are these real shortable stocks or distressed junk?\n",
    "print(\"Bottom-20 basket analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get all bottom-20 selections\n",
    "bottom_picks = []\n",
    "for date, group in test_df.groupby(\"feature_date\"):\n",
    "    if len(group) < 40:\n",
    "        continue\n",
    "    bottom = group.nsmallest(20, \"score\")\n",
    "    bottom_picks.append(bottom[[\"symbol\", \"feature_date\", \"target_return\", \"score\"]])\n",
    "\n",
    "bottom_df = pd.concat(bottom_picks)\n",
    "\n",
    "print(f\"\\nMost frequent bottom-20 stocks:\")\n",
    "freq = bottom_df[\"symbol\"].value_counts().head(20)\n",
    "print(freq)\n",
    "\n",
    "print(f\"\\n\\nWorst single-day returns in bottom basket:\")\n",
    "print(bottom_df.nsmallest(20, \"target_return\")[[\"feature_date\", \"symbol\", \"target_return\"]])\n",
    "\n",
    "print(f\"\\n\\nReturn distribution of bottom-20 picks:\")\n",
    "print(bottom_df[\"target_return\"].describe())\n",
    "\n",
    "# Check for extreme outliers driving the result\n",
    "print(f\"\\n\\nDays with extreme short returns (< -10%):\")\n",
    "extreme_days = ls_df[ls_df[\"short_return\"] < -0.10]\n",
    "print(f\"Number of extreme days: {len(extreme_days)}\")\n",
    "if len(extreme_days) > 0:\n",
    "    print(extreme_days[[\"date\", \"short_return\", \"long_return\", \"long_short_return\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eltrur3kw8q",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for data quality issues - returns < -100% are impossible\n",
    "print(\"Data quality check:\")\n",
    "print(\"=\" * 50)\n",
    "impossible_returns = test_df[test_df[\"target_return\"] < -1.0]\n",
    "print(f\"\\nRows with impossible returns (< -100%): {len(impossible_returns)}\")\n",
    "if len(impossible_returns) > 0:\n",
    "    print(impossible_returns[[\"symbol\", \"feature_date\", \"target_return\"]].head(20))\n",
    "\n",
    "# Also check extreme positive returns\n",
    "extreme_positive = test_df[test_df[\"target_return\"] > 1.0]\n",
    "print(f\"\\nRows with extreme positive returns (> 100%): {len(extreme_positive)}\")\n",
    "if len(extreme_positive) > 0:\n",
    "    print(extreme_positive[[\"symbol\", \"feature_date\", \"target_return\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4qzxhrlc0yq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-evaluate with winsorized returns (clip extreme values)\n",
    "def compute_long_short_returns_clipped(df, top_k=20, clip_pct=0.50):\n",
    "    \"\"\"Compute long-short returns with clipped extreme returns.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"target_return_clipped\"] = df[\"target_return\"].clip(-clip_pct, clip_pct)\n",
    "    \n",
    "    returns = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < top_k * 2:\n",
    "            continue\n",
    "        # Use original score for ranking, clipped returns for P&L\n",
    "        top = group.nlargest(top_k, \"score\")\n",
    "        long_ret = top[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        bottom = group.nsmallest(top_k, \"score\")\n",
    "        short_ret = bottom[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        ls_ret = long_ret - short_ret\n",
    "        mkt_ret = group[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        returns.append({\n",
    "            \"date\": date,\n",
    "            \"long_return\": long_ret,\n",
    "            \"short_return\": short_ret,\n",
    "            \"long_short_return\": ls_ret,\n",
    "            \"market_return\": mkt_ret,\n",
    "        })\n",
    "    return pd.DataFrame(returns)\n",
    "\n",
    "ls_clipped = compute_long_short_returns_clipped(test_df, top_k=20, clip_pct=0.50)\n",
    "\n",
    "print(\"Long-Short with clipped returns (max Â±50%):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n  Long (top-20):\")\n",
    "print(f\"    Cumulative: {((1 + ls_clipped['long_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Short (bottom-20):\")\n",
    "print(f\"    Cumulative: {((1 + ls_clipped['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"    Short P&L: {((1 - ls_clipped['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Long-Short:\")\n",
    "print(f\"    Daily mean: {ls_clipped['long_short_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Sharpe: {ls_clipped['long_short_return'].mean() / ls_clipped['long_short_return'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"    Cumulative: {((1 + ls_clipped['long_short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcxjwdafeg",
   "metadata": {},
   "source": [
    "### 5.4 Shortability Analysis\n",
    "\n",
    "Filter to stocks that are practically shortable:\n",
    "- Market cap > $500M (institutional threshold)\n",
    "- Exclude penny stocks and micro-caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fxlrizjqnqk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fundamentals to get market cap\n",
    "key_metrics = pd.read_parquet(\"data/key_metrics.pqt\")\n",
    "\n",
    "# Get latest market cap per symbol (most recent quarter)\n",
    "key_metrics[\"date\"] = pd.to_datetime(key_metrics[\"date\"])\n",
    "latest_mcap = (\n",
    "    key_metrics[[\"symbol\", \"date\", \"marketCap\"]]\n",
    "    .sort_values(\"date\")\n",
    "    .groupby(\"symbol\")\n",
    "    .last()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"Market cap data for {len(latest_mcap):,} symbols\")\n",
    "print(f\"\\nMarket cap distribution:\")\n",
    "print(latest_mcap[\"marketCap\"].describe())\n",
    "\n",
    "# Define market cap tiers\n",
    "def mcap_tier(mcap):\n",
    "    if pd.isna(mcap) or mcap <= 0:\n",
    "        return \"Unknown\"\n",
    "    elif mcap < 300_000_000:\n",
    "        return \"Micro (<$300M)\"\n",
    "    elif mcap < 2_000_000_000:\n",
    "        return \"Small ($300M-$2B)\"\n",
    "    elif mcap < 10_000_000_000:\n",
    "        return \"Mid ($2B-$10B)\"\n",
    "    else:\n",
    "        return \"Large (>$10B)\"\n",
    "\n",
    "latest_mcap[\"tier\"] = latest_mcap[\"marketCap\"].apply(mcap_tier)\n",
    "print(f\"\\nMarket cap tiers:\")\n",
    "print(latest_mcap[\"tier\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cilmh1wzhwf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge market cap into test_df\n",
    "test_df_mcap = test_df.merge(\n",
    "    latest_mcap[[\"symbol\", \"marketCap\"]], \n",
    "    on=\"symbol\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Check how many of our bottom-20 picks have market cap data\n",
    "bottom_with_mcap = bottom_df.merge(latest_mcap[[\"symbol\", \"marketCap\"]], on=\"symbol\", how=\"left\")\n",
    "print(\"Bottom-20 picks by market cap tier:\")\n",
    "bottom_with_mcap[\"tier\"] = bottom_with_mcap[\"marketCap\"].apply(mcap_tier)\n",
    "print(bottom_with_mcap[\"tier\"].value_counts())\n",
    "\n",
    "print(\"\\n\\nMost frequent bottom-20 stocks with their market caps:\")\n",
    "freq_symbols = bottom_df[\"symbol\"].value_counts().head(20).index.tolist()\n",
    "for sym in freq_symbols:\n",
    "    mcap = latest_mcap[latest_mcap[\"symbol\"] == sym][\"marketCap\"].values\n",
    "    mcap_str = f\"${mcap[0]/1e9:.1f}B\" if len(mcap) > 0 and mcap[0] > 0 else \"N/A\"\n",
    "    count = bottom_df[bottom_df[\"symbol\"] == sym].shape[0]\n",
    "    print(f\"  {sym}: {count} days, mcap={mcap_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qk7ud97hd6p",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-evaluate long-short with market cap filter\n",
    "MIN_MCAP = 500_000_000  # $500M minimum\n",
    "\n",
    "# Filter to shortable universe\n",
    "shortable_symbols = set(latest_mcap[latest_mcap[\"marketCap\"] >= MIN_MCAP][\"symbol\"])\n",
    "test_shortable = test_df_mcap[test_df_mcap[\"symbol\"].isin(shortable_symbols)].copy()\n",
    "\n",
    "print(f\"Shortable universe (mcap >= ${MIN_MCAP/1e6:.0f}M): {len(shortable_symbols):,} symbols\")\n",
    "print(f\"Test rows: {len(test_df_mcap):,} -> {len(test_shortable):,} ({len(test_shortable)/len(test_df_mcap)*100:.1f}%)\")\n",
    "\n",
    "# Re-run long-short on shortable universe\n",
    "def compute_long_short_returns_filtered(df, top_k=20, clip_pct=0.50):\n",
    "    \"\"\"Long-short with clipped returns on filtered universe.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"target_return_clipped\"] = df[\"target_return\"].clip(-clip_pct, clip_pct)\n",
    "    \n",
    "    returns = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < top_k * 2:\n",
    "            continue\n",
    "        \n",
    "        top = group.nlargest(top_k, \"score\")\n",
    "        long_ret = top[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        bottom = group.nsmallest(top_k, \"score\")\n",
    "        short_ret = bottom[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        ls_ret = long_ret - short_ret\n",
    "        mkt_ret = group[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        returns.append({\n",
    "            \"date\": date,\n",
    "            \"long_return\": long_ret,\n",
    "            \"short_return\": short_ret,\n",
    "            \"long_short_return\": ls_ret,\n",
    "            \"market_return\": mkt_ret,\n",
    "        })\n",
    "    return pd.DataFrame(returns)\n",
    "\n",
    "ls_shortable = compute_long_short_returns_filtered(test_shortable, top_k=20, clip_pct=0.50)\n",
    "\n",
    "print(f\"\\nLong-Short on Shortable Universe (mcap >= $500M):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n  Trading days: {len(ls_shortable)}\")\n",
    "\n",
    "print(f\"\\n  Long (top-20):\")\n",
    "print(f\"    Cumulative: {((1 + ls_shortable['long_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Short (bottom-20):\")\n",
    "print(f\"    Cumulative: {((1 + ls_shortable['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"    Short P&L: {((1 - ls_shortable['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Long-Short:\")\n",
    "print(f\"    Daily mean: {ls_shortable['long_short_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Sharpe: {ls_shortable['long_short_return'].mean() / ls_shortable['long_short_return'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"    Cumulative: {((1 + ls_shortable['long_short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"    Win rate: {(ls_shortable['long_short_return'] > 0).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higo5py96vg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What stocks end up in the shortable bottom-20?\n",
    "bottom_shortable = []\n",
    "for date, group in test_shortable.groupby(\"feature_date\"):\n",
    "    if len(group) < 40:\n",
    "        continue\n",
    "    bottom = group.nsmallest(20, \"score\")\n",
    "    bottom_shortable.append(bottom[[\"symbol\", \"feature_date\", \"target_return\", \"score\", \"marketCap\"]])\n",
    "\n",
    "bottom_shortable_df = pd.concat(bottom_shortable)\n",
    "\n",
    "print(\"Most frequent bottom-20 stocks (shortable universe):\")\n",
    "freq = bottom_shortable_df[\"symbol\"].value_counts().head(20)\n",
    "for sym, count in freq.items():\n",
    "    mcap = bottom_shortable_df[bottom_shortable_df[\"symbol\"] == sym][\"marketCap\"].iloc[0]\n",
    "    print(f\"  {sym}: {count} days, mcap=${mcap/1e9:.1f}B\")\n",
    "\n",
    "print(f\"\\n\\nReturn distribution of shortable bottom-20:\")\n",
    "print(bottom_shortable_df[\"target_return\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and config\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"config\": config,\n",
    "    \"price_cols\": price_feat_cols,\n",
    "    \"fund_cols\": fund_feat_cols,\n",
    "    \"emb_cols\": emb_cols,\n",
    "}, \"data/model_final.pt\")\n",
    "\n",
    "print(\"Model saved to data/model_final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0001-0001-0001-000000000032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history_df = pd.DataFrame(history)\n",
    "history_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cg5bdgdi4ov",
   "metadata": {},
   "source": [
    "## 6. News Ablation Analysis\n",
    "\n",
    "Evaluate the incremental value of news embeddings by comparing:\n",
    "1. Model performance on rows with vs without news\n",
    "2. Full model vs ablated model (zeroed news embeddings)\n",
    "3. Statistical significance of the news signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f5d19f-bcb2-4eed-b58c-c20b2628b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify rows with news (non-zero embeddings)\n",
    "test_df[\"has_news\"] = (test_df[emb_cols].abs().sum(axis=1) > 0).astype(int)\n",
    "print(f\"Test set news coverage: {test_df['has_news'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kywyplishsp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare IC for rows with news vs without news\n",
    "test_with_news = test_df[test_df[\"has_news\"] == 1]\n",
    "test_no_news = test_df[test_df[\"has_news\"] == 0]\n",
    "\n",
    "ic_with_news = compute_daily_ic(test_with_news)\n",
    "ic_no_news = compute_daily_ic(test_no_news)\n",
    "\n",
    "print(\"Rank IC comparison:\")\n",
    "print(f\"  With news:    mean={ic_with_news['ic'].mean():.4f}, std={ic_with_news['ic'].std():.4f}, n_days={len(ic_with_news)}\")\n",
    "print(f\"  Without news: mean={ic_no_news['ic'].mean():.4f}, std={ic_no_news['ic'].std():.4f}, n_days={len(ic_no_news)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v08p2y9gh8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basket returns: compare top-K from news vs no-news subsets\n",
    "basket_with_news = compute_basket_returns(test_with_news, top_k=20)\n",
    "basket_no_news = compute_basket_returns(test_no_news, top_k=20)\n",
    "\n",
    "print(\"\\nBasket performance comparison (top-20):\")\n",
    "print(f\"  With news:\")\n",
    "print(f\"    Daily excess: {basket_with_news['excess_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Sharpe (excess): {basket_with_news['excess_return'].mean() / basket_with_news['excess_return'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"  Without news:\")\n",
    "print(f\"    Daily excess: {basket_no_news['excess_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Sharpe (excess): {basket_no_news['excess_return'].mean() / basket_no_news['excess_return'].std() * np.sqrt(252):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ls475exl4w",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation: Score with zeroed-out news embeddings\n",
    "@torch.no_grad()\n",
    "def get_scores_no_news(model, df, price_cols, fund_cols, emb_cols, device, batch_size=1024):\n",
    "    \"\"\"Get model scores with news embeddings zeroed out.\"\"\"\n",
    "    model.eval()\n",
    "    dataset = PointwiseDataset(df, price_cols, fund_cols, emb_cols)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    scores = []\n",
    "    for batch in tqdm(loader, desc=\"Scoring (no news)\", leave=False):\n",
    "        price = batch[\"price\"].to(device)\n",
    "        fund = batch[\"fund\"].to(device)\n",
    "        # Zero out embeddings\n",
    "        emb = torch.zeros_like(batch[\"emb\"]).to(device)\n",
    "        \n",
    "        score = model(price, fund, emb)\n",
    "        scores.append(score.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(scores)\n",
    "\n",
    "# Score with ablated news\n",
    "test_df[\"score_no_news\"] = get_scores_no_news(model, test_df, price_feat_cols, fund_feat_cols, emb_cols, device)\n",
    "\n",
    "# Compare IC\n",
    "ic_full = compute_daily_ic(test_df)  # Uses \"score\" column\n",
    "test_df_ablated = test_df.copy()\n",
    "test_df_ablated[\"score\"] = test_df_ablated[\"score_no_news\"]\n",
    "ic_ablated = compute_daily_ic(test_df_ablated)\n",
    "\n",
    "print(\"\\nAblation study (zeroed news embeddings):\")\n",
    "print(f\"  Full model IC:    {ic_full['ic'].mean():.4f}\")\n",
    "print(f\"  No-news model IC: {ic_ablated['ic'].mean():.4f}\")\n",
    "print(f\"  Delta:            {(ic_full['ic'].mean() - ic_ablated['ic'].mean()):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9gmzhhf5ra6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basket performance: full model vs ablated model\n",
    "basket_full = compute_basket_returns(test_df, top_k=20)\n",
    "basket_ablated = compute_basket_returns(test_df_ablated, top_k=20)\n",
    "\n",
    "print(\"\\nBasket performance: Full vs Ablated model:\")\n",
    "print(f\"  Full model:\")\n",
    "print(f\"    Daily excess: {basket_full['excess_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Sharpe (excess): {basket_full['excess_return'].mean() / basket_full['excess_return'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"  Ablated (no news):\")\n",
    "print(f\"    Daily excess: {basket_ablated['excess_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Sharpe (excess): {basket_ablated['excess_return'].mean() / basket_ablated['excess_return'].std() * np.sqrt(252):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6zeo3ydn2o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on rows WITH news: does the model do better with real embeddings vs zeros?\n",
    "test_news_only = test_df[test_df[\"has_news\"] == 1].copy()\n",
    "\n",
    "# Full model scores (already have)\n",
    "ic_news_full = compute_daily_ic(test_news_only)\n",
    "\n",
    "# Ablated scores for news rows\n",
    "test_news_ablated = test_news_only.copy()\n",
    "test_news_ablated[\"score\"] = test_news_only[\"score_no_news\"]\n",
    "ic_news_ablated = compute_daily_ic(test_news_ablated)\n",
    "\n",
    "print(\"\\nNews rows only - Full vs Ablated:\")\n",
    "print(f\"  Full model IC:    {ic_news_full['ic'].mean():.4f}\")\n",
    "print(f\"  Ablated model IC: {ic_news_ablated['ic'].mean():.4f}\")\n",
    "print(f\"  Delta (news benefit): {(ic_news_full['ic'].mean() - ic_news_ablated['ic'].mean()):.4f}\")\n",
    "\n",
    "# Is the delta significant?\n",
    "from scipy.stats import ttest_rel\n",
    "if len(ic_news_full) == len(ic_news_ablated):\n",
    "    merged_ic = ic_news_full.merge(ic_news_ablated, on=\"date\", suffixes=(\"_full\", \"_ablated\"))\n",
    "    t_stat, p_val = ttest_rel(merged_ic[\"ic_full\"], merged_ic[\"ic_ablated\"])\n",
    "    print(f\"  Paired t-test: t={t_stat:.2f}, p={p_val:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
