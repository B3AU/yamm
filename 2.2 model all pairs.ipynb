{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0000-0001-0001-000000000000",
   "metadata": {},
   "source": [
    "# Model Training (All Pairs)\n",
    "\n",
    "Multi-branch neural model with influence-controlled news embedding.\n",
    "\n",
    "**Difference from 2.0**: This version generates ALL possible pairs per day rather than sampling. With ~160 stocks/day with news, that's ~12,700 pairs/day.\n",
    "\n",
    "**Table of Contents:**\n",
    "1. Load and split data\n",
    "2. Dataset and DataLoader\n",
    "3. Model architecture\n",
    "4. Training loop\n",
    "5. Evaluation: Rank IC and Basket Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1b2c3d4-0001-0001-0001-000000000001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b2c3d4-0001-0001-0001-000000000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelConfig:\n",
    "    # Feature dimensions (set after loading data)\n",
    "    n_fundamental_features: int = 19\n",
    "    n_price_features: int = 9\n",
    "    n_embedding_dim: int = 768\n",
    "    \n",
    "    # Encoder latent dimensions\n",
    "    fundamental_latent: int = 32\n",
    "    price_latent: int = 16\n",
    "    news_latent: int = 32\n",
    "    \n",
    "    # Dropout\n",
    "    fundamental_dropout: float = 0.2\n",
    "    price_dropout: float = 0.2\n",
    "    news_dropout: float = 0.3\n",
    "    \n",
    "    # News influence cap\n",
    "    news_alpha: float = 0.8\n",
    "    \n",
    "    # Training\n",
    "    batch_size: int = 512\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-3\n",
    "    n_epochs: int = 20\n",
    "    pairs_per_day: int = 5000\n",
    "\n",
    "    # Random pairs only (no hard pair oversampling)\n",
    "    hard_fraction: float = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000003",
   "metadata": {},
   "source": [
    "## 1. Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1b2c3d4-0001-0001-0001-000000000004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 2,092,929 rows\n",
      "Date range: 2021-01-13 to 2025-12-18\n",
      "Symbols: 3,506\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"data/ml_dataset.pqt\")\n",
    "df[\"feature_date\"] = pd.to_datetime(df[\"feature_date\"])\n",
    "\n",
    "print(f\"Dataset: {len(df):,} rows\")\n",
    "print(f\"Date range: {df['feature_date'].min().date()} to {df['feature_date'].max().date()}\")\n",
    "print(f\"Symbols: {df['symbol'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b2c3d4-0001-0001-0001-000000000005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price features: 9\n",
      "Fundamental features: 19\n",
      "Embedding dims: 768\n"
     ]
    }
   ],
   "source": [
    "# Identify feature columns\n",
    "price_feat_cols = [\n",
    "    \"overnight_gap_z\", \"intraday_ret_z\",\n",
    "    \"ret_1d_z\", \"ret_2d_z\", \"ret_3d_z\", \"ret_5d_z\",\n",
    "    \"vol_5d_z\", \"dist_from_high_5d_z\", \"dist_from_low_5d_z\"\n",
    "]\n",
    "\n",
    "# Fundamental feature columns (normalized)\n",
    "fund_feat_cols = [c for c in df.columns if c.endswith(\"_z\") and c not in price_feat_cols and c != \"news_count_z\"]\n",
    "\n",
    "# Embedding columns\n",
    "emb_cols = [c for c in df.columns if c.startswith(\"emb_\")]\n",
    "\n",
    "print(f\"Price features: {len(price_feat_cols)}\")\n",
    "print(f\"Fundamental features: {len(fund_feat_cols)}\")\n",
    "print(f\"Embedding dims: {len(emb_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b2c3d4-0001-0001-0001-000000000006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1,418,494 rows, 830 days\n",
      "Val: 322,222 rows, 178 days\n",
      "Test: 352,213 rows, 178 days\n"
     ]
    }
   ],
   "source": [
    "# Time-based split (per README section 13)\n",
    "# Use last 20% of dates as test\n",
    "dates = sorted(df[\"feature_date\"].unique())\n",
    "n_dates = len(dates)\n",
    "train_end_idx = int(n_dates * 0.7)\n",
    "val_end_idx = int(n_dates * 0.85)\n",
    "\n",
    "train_dates = set(dates[:train_end_idx])\n",
    "val_dates = set(dates[train_end_idx:val_end_idx])\n",
    "test_dates = set(dates[val_end_idx:])\n",
    "\n",
    "train_df = df[df[\"feature_date\"].isin(train_dates)].copy()\n",
    "val_df = df[df[\"feature_date\"].isin(val_dates)].copy()\n",
    "test_df = df[df[\"feature_date\"].isin(test_dates)].copy()\n",
    "\n",
    "print(f\"Train: {len(train_df):,} rows, {len(train_dates)} days\")\n",
    "print(f\"Val: {len(val_df):,} rows, {len(val_dates)} days\")\n",
    "print(f\"Test: {len(test_df):,} rows, {len(test_dates)} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000007",
   "metadata": {},
   "source": [
    "## 2. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1b2c3d4-0001-0001-0001-000000000008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairwiseRankingDataset(Dataset):\n",
    "    \"\"\"Dataset that generates ALL pairs from same day for ranking loss.\n",
    "\n",
    "    Only uses rows with news. Generates all N*(N-1)/2 unique pairs per day.\n",
    "    \n",
    "    Call resample_pairs() at the start of each epoch to shuffle the order.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        price_cols: list[str],\n",
    "        fund_cols: list[str],\n",
    "        emb_cols: list[str],\n",
    "    ):\n",
    "        # Filter to rows with news only\n",
    "        has_news = (df[emb_cols].abs().sum(axis=1) > 0)\n",
    "        df_news = df[has_news].copy()\n",
    "        print(f\"Filtered to news-only: {len(df_news):,} / {len(df):,} rows ({len(df_news)/len(df)*100:.1f}%)\")\n",
    "\n",
    "        self.df = df_news.reset_index(drop=True)\n",
    "        self.price_cols = price_cols\n",
    "        self.fund_cols = fund_cols\n",
    "        self.emb_cols = emb_cols\n",
    "\n",
    "        # Group by date\n",
    "        self.date_groups = {}\n",
    "        for date, group in df_news.groupby(\"feature_date\"):\n",
    "            indices = group.index.tolist()\n",
    "            if len(indices) < 2:  # Need at least 2 for pairs\n",
    "                continue\n",
    "            self.date_groups[date] = np.array(indices)\n",
    "\n",
    "        self.dates = list(self.date_groups.keys())\n",
    "        print(f\"Days with sufficient news coverage: {len(self.dates)}\")\n",
    "\n",
    "        # Precompute arrays for speed\n",
    "        self.price_arr = df_news[price_cols].values.astype(np.float32)\n",
    "        self.fund_arr = df_news[fund_cols].values.astype(np.float32)\n",
    "        self.emb_arr = df_news[emb_cols].values.astype(np.float32)\n",
    "        self.target_arr = df_news[\"target_return\"].values.astype(np.float32)\n",
    "\n",
    "        # Map original index to position in filtered df\n",
    "        self.idx_map = {old_idx: new_idx for new_idx, old_idx in enumerate(df_news.index)}\n",
    "\n",
    "        # Generate all pairs\n",
    "        self.pairs = []\n",
    "        self._generate_all_pairs()\n",
    "        \n",
    "    def _generate_all_pairs(self):\n",
    "        \"\"\"Generate ALL unique pairs for each day.\"\"\"\n",
    "        pairs = []\n",
    "        total_possible = 0\n",
    "\n",
    "        for date in self.dates:\n",
    "            indices = self.date_groups[date]\n",
    "            n = len(indices)\n",
    "            total_possible += n * (n - 1) // 2\n",
    "            \n",
    "            # Generate all unique pairs (i, j) where i < j\n",
    "            for i in range(n):\n",
    "                for j in range(i + 1, n):\n",
    "                    idx_i = self.idx_map[indices[i]]\n",
    "                    idx_j = self.idx_map[indices[j]]\n",
    "                    pairs.append((idx_i, idx_j))\n",
    "\n",
    "        self.pairs = pairs\n",
    "        print(f\"Generated {len(self.pairs):,} pairs (all combinations)\")\n",
    "        \n",
    "    def resample_pairs(self):\n",
    "        \"\"\"Shuffle pairs for new epoch (no resampling needed since we use all).\"\"\"\n",
    "        np.random.shuffle(self.pairs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i, j = self.pairs[idx]\n",
    "\n",
    "        # Get features\n",
    "        price_i = self.price_arr[i]\n",
    "        price_j = self.price_arr[j]\n",
    "        fund_i = self.fund_arr[i]\n",
    "        fund_j = self.fund_arr[j]\n",
    "        emb_i = self.emb_arr[i]\n",
    "        emb_j = self.emb_arr[j]\n",
    "\n",
    "        # Label based on actual returns\n",
    "        actual_label = 1.0 if self.target_arr[i] > self.target_arr[j] else 0.0\n",
    "\n",
    "        # Randomly swap to balance labels\n",
    "        if np.random.random() < 0.5:\n",
    "            price_i, price_j = price_j, price_i\n",
    "            fund_i, fund_j = fund_j, fund_i\n",
    "            emb_i, emb_j = emb_j, emb_i\n",
    "            label = 1.0 - actual_label\n",
    "        else:\n",
    "            label = actual_label\n",
    "\n",
    "        return {\n",
    "            \"price_i\": torch.tensor(price_i),\n",
    "            \"price_j\": torch.tensor(price_j),\n",
    "            \"fund_i\": torch.tensor(fund_i),\n",
    "            \"fund_j\": torch.tensor(fund_j),\n",
    "            \"emb_i\": torch.tensor(emb_i),\n",
    "            \"emb_j\": torch.tensor(emb_j),\n",
    "            \"label\": torch.tensor(label),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b2c3d4-0001-0001-0001-000000000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointwiseDataset(Dataset):\n",
    "    \"\"\"Dataset for inference - one sample per stock-day.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        price_cols: list[str],\n",
    "        fund_cols: list[str],\n",
    "        emb_cols: list[str],\n",
    "    ):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.price_arr = df[price_cols].values.astype(np.float32)\n",
    "        self.fund_arr = df[fund_cols].values.astype(np.float32)\n",
    "        self.emb_arr = df[emb_cols].values.astype(np.float32)\n",
    "        self.target_arr = df[\"target_return\"].values.astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"price\": torch.tensor(self.price_arr[idx]),\n",
    "            \"fund\": torch.tensor(self.fund_arr[idx]),\n",
    "            \"emb\": torch.tensor(self.emb_arr[idx]),\n",
    "            \"target\": torch.tensor(self.target_arr[idx]),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b2c3d4-0001-0001-0001-000000000010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: ModelConfig(n_fundamental_features=19, n_price_features=9, n_embedding_dim=768, fundamental_latent=32, price_latent=16, news_latent=32, fundamental_dropout=0.2, price_dropout=0.2, news_dropout=0.3, news_alpha=0.8, batch_size=512, learning_rate=0.001, weight_decay=0.001, n_epochs=20, pairs_per_day=5000, hard_fraction=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Create config with actual dimensions\n",
    "config = ModelConfig(\n",
    "    n_fundamental_features=len(fund_feat_cols),\n",
    "    n_price_features=len(price_feat_cols),\n",
    "    n_embedding_dim=len(emb_cols),\n",
    ")\n",
    "\n",
    "print(f\"Config: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1b2c3d4-0001-0001-0001-000000000011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to news-only: 339,872 / 1,418,494 rows (24.0%)\n",
      "Days with sufficient news coverage: 830\n",
      "Generated 71,008,149 pairs (all combinations)\n",
      "Filtered to news-only: 88,579 / 322,222 rows (27.5%)\n",
      "Days with sufficient news coverage: 178\n",
      "Generated 22,370,809 pairs (all combinations)\n",
      "\n",
      "Train pairs: 71,008,149\n",
      "Val pairs: 22,370,809\n"
     ]
    }
   ],
   "source": [
    "# Create datasets with ALL pairs\n",
    "train_dataset = PairwiseRankingDataset(\n",
    "    train_df, price_feat_cols, fund_feat_cols, emb_cols,\n",
    ")\n",
    "val_dataset = PairwiseRankingDataset(\n",
    "    val_df, price_feat_cols, fund_feat_cols, emb_cols,\n",
    ")\n",
    "\n",
    "# For evaluation (pointwise)\n",
    "val_pointwise = PointwiseDataset(val_df, price_feat_cols, fund_feat_cols, emb_cols)\n",
    "test_pointwise = PointwiseDataset(test_df, price_feat_cols, fund_feat_cols, emb_cols)\n",
    "\n",
    "print(f\"\\nTrain pairs: {len(train_dataset):,}\")\n",
    "print(f\"Val pairs: {len(val_dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1b2c3d4-0001-0001-0001-000000000012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "# num_workers=0 for Jupyter compatibility (avoids multiprocessing issues)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config.batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000013",
   "metadata": {},
   "source": [
    "## 3. Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1b2c3d4-0001-0001-0001-000000000014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBranchRanker(nn.Module):\n",
    "    \"\"\"Multi-branch model with influence-controlled news embedding.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Fundamentals encoder\n",
    "        self.fund_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_fundamental_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.fundamental_dropout),\n",
    "            nn.Linear(64, config.fundamental_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Price encoder\n",
    "        self.price_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_price_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.price_dropout),\n",
    "            nn.Linear(32, config.price_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # News encoder\n",
    "        self.news_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.news_dropout),\n",
    "            nn.Linear(128, config.news_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Output head\n",
    "        fused_dim = config.fundamental_latent + config.price_latent + config.news_latent\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "    \n",
    "    def encode(self, price: torch.Tensor, fund: torch.Tensor, emb: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Encode features and return fused representation.\"\"\"\n",
    "        h_f = self.fund_encoder(fund)\n",
    "        h_p = self.price_encoder(price)\n",
    "        h_n = self.news_encoder(emb)\n",
    "        \n",
    "        # Apply news influence cap (Î± * h_n)\n",
    "        h_n_scaled = self.config.news_alpha * h_n\n",
    "        \n",
    "        # Fuse\n",
    "        h = torch.cat([h_f, h_p, h_n_scaled], dim=-1)\n",
    "        return h\n",
    "    \n",
    "    def forward(self, price: torch.Tensor, fund: torch.Tensor, emb: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass returning score.\"\"\"\n",
    "        h = self.encode(price, fund, emb)\n",
    "        score = self.output_head(h)\n",
    "        return score.squeeze(-1)\n",
    "    \n",
    "    def forward_pair(\n",
    "        self,\n",
    "        price_i: torch.Tensor, fund_i: torch.Tensor, emb_i: torch.Tensor,\n",
    "        price_j: torch.Tensor, fund_j: torch.Tensor, emb_j: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Forward pass for pair, returning P(i > j).\"\"\"\n",
    "        score_i = self.forward(price_i, fund_i, emb_i)\n",
    "        score_j = self.forward(price_j, fund_j, emb_j)\n",
    "        return torch.sigmoid(score_i - score_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1b2c3d4-0001-0001-0001-000000000015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 109,393\n",
      "MultiBranchRanker(\n",
      "  (fund_encoder): Sequential(\n",
      "    (0): Linear(in_features=19, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (price_encoder): Sequential(\n",
      "    (0): Linear(in_features=9, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (news_encoder): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "  )\n",
      "  (output_head): Sequential(\n",
      "    (0): Linear(in_features=80, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MultiBranchRanker(config).to(device)\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {n_params:,}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000016",
   "metadata": {},
   "source": [
    "## 4. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1b2c3d4-0001-0001-0001-000000000017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_ranking_loss(pred_prob: torch.Tensor, label: torch.Tensor, smoothing: float = 0.2) -> torch.Tensor:\n",
    "    \"\"\"Binary cross-entropy for pairwise ranking with label smoothing.\"\"\"\n",
    "    # Smooth labels: 0 -> smoothing/2, 1 -> 1 - smoothing/2\n",
    "    smoothed_label = label * (1 - smoothing) + 0.5 * smoothing\n",
    "    return F.binary_cross_entropy(pred_prob, smoothed_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1b2c3d4-0001-0001-0001-000000000018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        price_i = batch[\"price_i\"].to(device)\n",
    "        price_j = batch[\"price_j\"].to(device)\n",
    "        fund_i = batch[\"fund_i\"].to(device)\n",
    "        fund_j = batch[\"fund_j\"].to(device)\n",
    "        emb_i = batch[\"emb_i\"].to(device)\n",
    "        emb_j = batch[\"emb_j\"].to(device)\n",
    "        label = batch[\"label\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred_prob = model.forward_pair(price_i, fund_i, emb_i, price_j, fund_j, emb_j)\n",
    "        loss = pairwise_ranking_loss(pred_prob, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(label)\n",
    "        total_correct += ((pred_prob > 0.5) == (label > 0.5)).sum().item()\n",
    "        total_samples += len(label)\n",
    "    \n",
    "    return total_loss / total_samples, total_correct / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1b2c3d4-0001-0001-0001-000000000019",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        price_i = batch[\"price_i\"].to(device)\n",
    "        price_j = batch[\"price_j\"].to(device)\n",
    "        fund_i = batch[\"fund_i\"].to(device)\n",
    "        fund_j = batch[\"fund_j\"].to(device)\n",
    "        emb_i = batch[\"emb_i\"].to(device)\n",
    "        emb_j = batch[\"emb_j\"].to(device)\n",
    "        label = batch[\"label\"].to(device)\n",
    "        \n",
    "        pred_prob = model.forward_pair(price_i, fund_i, emb_i, price_j, fund_j, emb_j)\n",
    "        loss = pairwise_ranking_loss(pred_prob, label)\n",
    "        \n",
    "        total_loss += loss.item() * len(label)\n",
    "        total_correct += ((pred_prob > 0.5) == (label > 0.5)).sum().item()\n",
    "        total_samples += len(label)\n",
    "    \n",
    "    return total_loss / total_samples, total_correct / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1b2c3d4-0001-0001-0001-000000000020",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=config.n_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1b2c3d4-0001-0001-0001-000000000021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186efedb222b421588bad26fca2d9c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/138688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m train_dataset\u001b[38;5;241m.\u001b[39mresample_pairs()\n\u001b[1;32m      8\u001b[0m val_dataset\u001b[38;5;241m.\u001b[39mresample_pairs()\n\u001b[0;32m---> 10\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m eval_epoch(model, val_loader, device)\n\u001b[1;32m     12\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[14], line 20\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m pairwise_ranking_loss(pred_prob, label)\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 20\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(label)\n\u001b[1;32m     23\u001b[0m total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ((pred_prob \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m==\u001b[39m (label \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m))\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:133\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    132\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:517\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    513\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    514\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    515\u001b[0m             )\n\u001b[0;32m--> 517\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    520\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:82\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 82\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:247\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    235\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    237\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    238\u001b[0m         group,\n\u001b[1;32m    239\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m         state_steps,\n\u001b[1;32m    245\u001b[0m     )\n\u001b[0;32m--> 247\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:150\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:953\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    951\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 953\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:537\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    535\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 537\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "best_val_acc = 0\n",
    "history = []\n",
    "\n",
    "for epoch in range(1):#:range(config.n_epochs):\n",
    "    # Resample pairs each epoch for fresh training data\n",
    "    train_dataset.resample_pairs()\n",
    "    val_dataset.resample_pairs()\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss, val_acc = eval_epoch(model, val_loader, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    history.append({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "    })\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{config.n_epochs}: \"\n",
    "          f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, \"\n",
    "          f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"data/model_best_70M_pairs.pt\")\n",
    "        print(f\"  -> New best model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000022",
   "metadata": {},
   "source": [
    "## 5. Evaluation: Rank IC and Basket Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1b2c3d4-0001-0001-0001-000000000023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiBranchRanker(\n",
       "  (fund_encoder): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       "  (price_encoder): Sequential(\n",
       "    (0): Linear(in_features=9, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       "  (news_encoder): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (4): ReLU()\n",
       "  )\n",
       "  (output_head): Sequential(\n",
       "    (0): Linear(in_features=80, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best model\n",
    "#model.load_state_dict(torch.load(\"data/model_best_all_pairs-best.pt\", weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1b2c3d4-0001-0001-0001-000000000024",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_scores(model, df, price_cols, fund_cols, emb_cols, device, batch_size=1024):\n",
    "    \"\"\"Get model scores for all rows.\"\"\"\n",
    "    model.eval()\n",
    "    dataset = PointwiseDataset(df, price_cols, fund_cols, emb_cols)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    scores = []\n",
    "    for batch in tqdm(loader, desc=\"Scoring\", leave=False):\n",
    "        price = batch[\"price\"].to(device)\n",
    "        fund = batch[\"fund\"].to(device)\n",
    "        emb = batch[\"emb\"].to(device)\n",
    "        \n",
    "        score = model(price, fund, emb)\n",
    "        scores.append(score.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1b2c3d4-0001-0001-0001-000000000025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring:   0%|          | 0/344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 352,213 test samples\n"
     ]
    }
   ],
   "source": [
    "# Score test set\n",
    "test_df = test_df.copy()\n",
    "test_df[\"score\"] = get_scores(model, test_df, price_feat_cols, fund_feat_cols, emb_cols, device)\n",
    "\n",
    "print(f\"Scored {len(test_df):,} test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1b2c3d4-0001-0001-0001-000000000026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "def compute_daily_ic(df):\n",
    "    \"\"\"Compute Spearman rank IC per day.\"\"\"\n",
    "    ics = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < 10:\n",
    "            continue\n",
    "        ic, _ = spearmanr(group[\"score\"], group[\"target_return\"])\n",
    "        if not np.isnan(ic):\n",
    "            ics.append({\"date\": date, \"ic\": ic})\n",
    "    return pd.DataFrame(ics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1b2c3d4-0001-0001-0001-000000000027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Rank IC (Spearman):\n",
      "  Mean: 0.0100\n",
      "  Std: 0.0738\n",
      "  Sharpe (IC): 2.15\n",
      "  % Positive: 55.6%\n"
     ]
    }
   ],
   "source": [
    "ic_df = compute_daily_ic(test_df)\n",
    "\n",
    "print(f\"Daily Rank IC (Spearman):\")\n",
    "print(f\"  Mean: {ic_df['ic'].mean():.4f}\")\n",
    "print(f\"  Std: {ic_df['ic'].std():.4f}\")\n",
    "print(f\"  Sharpe (IC): {ic_df['ic'].mean() / ic_df['ic'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"  % Positive: {(ic_df['ic'] > 0).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1b2c3d4-0001-0001-0001-000000000028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_basket_returns(df, top_k=20):\n",
    "    \"\"\"Compute daily returns of top-K basket (equal weight).\"\"\"\n",
    "    returns = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < top_k:\n",
    "            continue\n",
    "        # Select top-K by score\n",
    "        top = group.nlargest(top_k, \"score\")\n",
    "        # Equal weight return\n",
    "        ret = top[\"target_return\"].mean()\n",
    "        # Market return (all stocks)\n",
    "        mkt_ret = group[\"target_return\"].mean()\n",
    "        returns.append({\n",
    "            \"date\": date,\n",
    "            \"basket_return\": ret,\n",
    "            \"market_return\": mkt_ret,\n",
    "            \"excess_return\": ret - mkt_ret,\n",
    "        })\n",
    "    return pd.DataFrame(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1b2c3d4-0001-0001-0001-000000000029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-20 Basket Performance (Test Set):\n",
      "  Daily mean return: -0.834%\n",
      "  Daily mean excess: -0.690%\n",
      "  Sharpe (basket): -3.98\n",
      "  Sharpe (excess): -4.01\n"
     ]
    }
   ],
   "source": [
    "basket_df = compute_basket_returns(test_df, top_k=20)\n",
    "\n",
    "print(f\"\\nTop-20 Basket Performance (Test Set):\")\n",
    "print(f\"  Daily mean return: {basket_df['basket_return'].mean()*100:.3f}%\")\n",
    "print(f\"  Daily mean excess: {basket_df['excess_return'].mean()*100:.3f}%\")\n",
    "print(f\"  Sharpe (basket): {basket_df['basket_return'].mean() / basket_df['basket_return'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"  Sharpe (excess): {basket_df['excess_return'].mean() / basket_df['excess_return'].std() * np.sqrt(252):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1b2c3d4-0001-0001-0001-000000000030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cumulative Returns:\n",
      "  Basket: -79.7%\n",
      "  Market: -23.7%\n",
      "  Excess: -72.8%\n"
     ]
    }
   ],
   "source": [
    "# Cumulative returns\n",
    "basket_df[\"cum_basket\"] = (1 + basket_df[\"basket_return\"]).cumprod()\n",
    "basket_df[\"cum_market\"] = (1 + basket_df[\"market_return\"]).cumprod()\n",
    "basket_df[\"cum_excess\"] = (1 + basket_df[\"excess_return\"]).cumprod()\n",
    "\n",
    "print(f\"\\nCumulative Returns:\")\n",
    "print(f\"  Basket: {(basket_df['cum_basket'].iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"  Market: {(basket_df['cum_market'].iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"  Excess: {(basket_df['cum_excess'].iloc[-1] - 1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "o5vcmhiugh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long-Short Portfolio Performance (Test Set):\n",
      "\n",
      "  Long (top-20):\n",
      "    Daily mean: -0.834%\n",
      "    Cumulative: -79.7%\n",
      "\n",
      "  Short (bottom-20):\n",
      "    Daily mean: -1.283%\n",
      "    Cumulative: -91.5%\n",
      "    Short P&L (inverted): 727.8%\n",
      "\n",
      "  Long-Short (market neutral):\n",
      "    Daily mean: 0.449%\n",
      "    Std: 3.855%\n",
      "    Sharpe: 1.85\n",
      "    Cumulative: 94.7%\n",
      "\n",
      "  Market:\n",
      "    Cumulative: -23.7%\n"
     ]
    }
   ],
   "source": [
    "# Long-short analysis: long top-K, short bottom-K\n",
    "def compute_long_short_returns(df, top_k=20):\n",
    "    \"\"\"Compute daily returns of long-short portfolio.\"\"\"\n",
    "    returns = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < top_k * 2:\n",
    "            continue\n",
    "        # Long top-K\n",
    "        top = group.nlargest(top_k, \"score\")\n",
    "        long_ret = top[\"target_return\"].mean()\n",
    "        # Short bottom-K\n",
    "        bottom = group.nsmallest(top_k, \"score\")\n",
    "        short_ret = bottom[\"target_return\"].mean()\n",
    "        # Long-short return (long winners, short losers)\n",
    "        ls_ret = long_ret - short_ret\n",
    "        # Market\n",
    "        mkt_ret = group[\"target_return\"].mean()\n",
    "        returns.append({\n",
    "            \"date\": date,\n",
    "            \"long_return\": long_ret,\n",
    "            \"short_return\": short_ret,\n",
    "            \"long_short_return\": ls_ret,\n",
    "            \"market_return\": mkt_ret,\n",
    "        })\n",
    "    return pd.DataFrame(returns)\n",
    "\n",
    "ls_df = compute_long_short_returns(test_df, top_k=20)\n",
    "\n",
    "print(\"Long-Short Portfolio Performance (Test Set):\")\n",
    "print(f\"\\n  Long (top-20):\")\n",
    "print(f\"    Daily mean: {ls_df['long_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Cumulative: {((1 + ls_df['long_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Short (bottom-20):\")\n",
    "print(f\"    Daily mean: {ls_df['short_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Cumulative: {((1 + ls_df['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"    Short P&L (inverted): {((1 - ls_df['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Long-Short (market neutral):\")\n",
    "print(f\"    Daily mean: {ls_df['long_short_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Std: {ls_df['long_short_return'].std()*100:.3f}%\")\n",
    "print(f\"    Sharpe: {ls_df['long_short_return'].mean() / ls_df['long_short_return'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"    Cumulative: {((1 + ls_df['long_short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Market:\")\n",
    "print(f\"    Cumulative: {((1 + ls_df['market_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "av5cknegqmf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom-20 basket analysis:\n",
      "==================================================\n",
      "\n",
      "Most frequent bottom-20 stocks:\n",
      "symbol\n",
      "QBTS    45\n",
      "RGC     34\n",
      "ABTC    34\n",
      "IONQ    31\n",
      "TLRY    31\n",
      "QMCO    30\n",
      "BBAI    30\n",
      "ASTS    28\n",
      "QUBT    28\n",
      "ACON    27\n",
      "SEDG    25\n",
      "JOBY    24\n",
      "FIG     24\n",
      "NBIS    21\n",
      "RGTI    21\n",
      "HSDT    20\n",
      "AEVA    19\n",
      "LCID    18\n",
      "HTZ     18\n",
      "AAOI    18\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Worst single-day returns in bottom basket:\n",
      "        feature_date symbol  target_return\n",
      "2050954   2025-11-05   MGNI      -3.540930\n",
      "1877320   2025-06-25    OST      -2.797281\n",
      "1764290   2025-03-05   ACON      -1.474477\n",
      "1799271   2025-04-04   SUNE      -1.463255\n",
      "1906158   2025-07-17    YHC      -1.015413\n",
      "1825836   2025-05-12   DEVS      -0.832146\n",
      "1882126   2025-06-27    OST      -0.791666\n",
      "1804732   2025-04-17   AREB      -0.769643\n",
      "1754490   2025-02-21   PPCB      -0.769074\n",
      "1865197   2025-06-12    KWM      -0.705144\n",
      "1970499   2025-09-09   QMMM      -0.636794\n",
      "1744581   2025-02-13   BSLK      -0.622530\n",
      "1799476   2025-04-04   AREB      -0.608618\n",
      "2061673   2025-11-13   NVVE      -0.604211\n",
      "1920673   2025-08-05   LFMD      -0.595077\n",
      "1883407   2025-06-30   SONM      -0.584513\n",
      "2086614   2025-12-15   RADX      -0.573589\n",
      "2021426   2025-10-15   GNPX      -0.568825\n",
      "1825681   2025-05-09   NVVE      -0.565026\n",
      "1909652   2025-07-28   BKKT      -0.540579\n",
      "\n",
      "\n",
      "Return distribution of bottom-20 picks:\n",
      "count    3560.000000\n",
      "mean       -0.012825\n",
      "std         0.147779\n",
      "min        -3.540930\n",
      "25%        -0.042280\n",
      "50%        -0.005322\n",
      "75%         0.025375\n",
      "max         2.910574\n",
      "Name: target_return, dtype: float64\n",
      "\n",
      "\n",
      "Days with extreme short returns (< -10%):\n",
      "Number of extreme days: 6\n",
      "          date  short_return  long_return  long_short_return\n",
      "12  2025-03-05     -0.107687    -0.046658           0.061029\n",
      "30  2025-04-04     -0.112555    -0.006004           0.106550\n",
      "72  2025-06-25     -0.108980     0.031026           0.140005\n",
      "86  2025-07-17     -0.108514    -0.017644           0.090869\n",
      "143 2025-10-15     -0.143393    -0.061287           0.082107\n",
      "157 2025-11-05     -0.200940    -0.042427           0.158513\n"
     ]
    }
   ],
   "source": [
    "# Investigate the short basket - are these real shortable stocks or distressed junk?\n",
    "print(\"Bottom-20 basket analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get all bottom-20 selections\n",
    "bottom_picks = []\n",
    "for date, group in test_df.groupby(\"feature_date\"):\n",
    "    if len(group) < 40:\n",
    "        continue\n",
    "    bottom = group.nsmallest(20, \"score\")\n",
    "    bottom_picks.append(bottom[[\"symbol\", \"feature_date\", \"target_return\", \"score\"]])\n",
    "\n",
    "bottom_df = pd.concat(bottom_picks)\n",
    "\n",
    "print(f\"\\nMost frequent bottom-20 stocks:\")\n",
    "freq = bottom_df[\"symbol\"].value_counts().head(20)\n",
    "print(freq)\n",
    "\n",
    "print(f\"\\n\\nWorst single-day returns in bottom basket:\")\n",
    "print(bottom_df.nsmallest(20, \"target_return\")[[\"feature_date\", \"symbol\", \"target_return\"]])\n",
    "\n",
    "print(f\"\\n\\nReturn distribution of bottom-20 picks:\")\n",
    "print(bottom_df[\"target_return\"].describe())\n",
    "\n",
    "# Check for extreme outliers driving the result\n",
    "print(f\"\\n\\nDays with extreme short returns (< -10%):\")\n",
    "extreme_days = ls_df[ls_df[\"short_return\"] < -0.10]\n",
    "print(f\"Number of extreme days: {len(extreme_days)}\")\n",
    "if len(extreme_days) > 0:\n",
    "    print(extreme_days[[\"date\", \"short_return\", \"long_return\", \"long_short_return\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eltrur3kw8q",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality check:\n",
      "==================================================\n",
      "\n",
      "Rows with impossible returns (< -100%): 28\n",
      "        symbol feature_date  target_return\n",
      "1743903   PPCB   2025-02-12      -1.287354\n",
      "1746731   INLF   2025-02-14      -1.572174\n",
      "1764290   ACON   2025-03-05      -1.474477\n",
      "1767070    HIT   2025-03-11      -1.731924\n",
      "1793247   GDHG   2025-03-31      -2.161439\n",
      "1799271   SUNE   2025-04-04      -1.463255\n",
      "1863749   SBET   2025-06-12      -1.261873\n",
      "1877320    OST   2025-06-25      -2.797281\n",
      "1888515   SKBL   2025-07-03      -2.085914\n",
      "1904441   PTNM   2025-07-17      -1.132462\n",
      "1906158    YHC   2025-07-17      -1.015413\n",
      "1910322   PTHL   2025-07-28      -2.931921\n",
      "1922868    SMX   2025-08-06      -2.019265\n",
      "1935021   FLYE   2025-08-14      -2.048982\n",
      "1951089   TRIB   2025-08-26      -1.602658\n",
      "1970120   EPSM   2025-09-09      -1.228176\n",
      "1993179    SDM   2025-09-25      -1.995619\n",
      "1995860   MLTX   2025-09-26      -2.294392\n",
      "1999569    UFG   2025-10-01      -1.245843\n",
      "2006898   YYAI   2025-10-06      -2.635081\n",
      "\n",
      "Rows with extreme positive returns (> 100%): 12\n",
      "        symbol feature_date  target_return\n",
      "1752155   MLGO   2025-02-20       1.710266\n",
      "1781585   MLGO   2025-03-21       1.714084\n",
      "1796471   AREB   2025-04-03       1.594170\n",
      "1866290    RGC   2025-06-13       1.343235\n",
      "1921629    SMX   2025-08-05       1.802700\n",
      "1929360   ETHZ   2025-08-11       1.120331\n",
      "1966571   EPSM   2025-09-08       1.632158\n",
      "1968035   QMMM   2025-09-08       2.910574\n",
      "1990613   QURE   2025-09-23       1.246258\n",
      "2016467   AQMS   2025-10-13       1.094905\n"
     ]
    }
   ],
   "source": [
    "# Check for data quality issues - returns < -100% are impossible\n",
    "print(\"Data quality check:\")\n",
    "print(\"=\" * 50)\n",
    "impossible_returns = test_df[test_df[\"target_return\"] < -1.0]\n",
    "print(f\"\\nRows with impossible returns (< -100%): {len(impossible_returns)}\")\n",
    "if len(impossible_returns) > 0:\n",
    "    print(impossible_returns[[\"symbol\", \"feature_date\", \"target_return\"]].head(20))\n",
    "\n",
    "# Also check extreme positive returns\n",
    "extreme_positive = test_df[test_df[\"target_return\"] > 1.0]\n",
    "print(f\"\\nRows with extreme positive returns (> 100%): {len(extreme_positive)}\")\n",
    "if len(extreme_positive) > 0:\n",
    "    print(extreme_positive[[\"symbol\", \"feature_date\", \"target_return\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4qzxhrlc0yq",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long-Short with clipped returns (max Â±50%):\n",
      "==================================================\n",
      "\n",
      "  Long (top-20):\n",
      "    Cumulative: -72.1%\n",
      "\n",
      "  Short (bottom-20):\n",
      "    Cumulative: -89.1%\n",
      "    Short P&L: 633.2%\n",
      "\n",
      "  Long-Short:\n",
      "    Daily mean: 0.509%\n",
      "    Sharpe: 2.95\n",
      "    Cumulative: 131.1%\n"
     ]
    }
   ],
   "source": [
    "# Re-evaluate with winsorized returns (clip extreme values)\n",
    "def compute_long_short_returns_clipped(df, top_k=20, clip_pct=0.50):\n",
    "    \"\"\"Compute long-short returns with clipped extreme returns.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"target_return_clipped\"] = df[\"target_return\"].clip(-clip_pct, clip_pct)\n",
    "    \n",
    "    returns = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < top_k * 2:\n",
    "            continue\n",
    "        # Use original score for ranking, clipped returns for P&L\n",
    "        top = group.nlargest(top_k, \"score\")\n",
    "        long_ret = top[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        bottom = group.nsmallest(top_k, \"score\")\n",
    "        short_ret = bottom[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        ls_ret = long_ret - short_ret\n",
    "        mkt_ret = group[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        returns.append({\n",
    "            \"date\": date,\n",
    "            \"long_return\": long_ret,\n",
    "            \"short_return\": short_ret,\n",
    "            \"long_short_return\": ls_ret,\n",
    "            \"market_return\": mkt_ret,\n",
    "        })\n",
    "    return pd.DataFrame(returns)\n",
    "\n",
    "ls_clipped = compute_long_short_returns_clipped(test_df, top_k=20, clip_pct=0.50)\n",
    "\n",
    "print(\"Long-Short with clipped returns (max Â±50%):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n  Long (top-20):\")\n",
    "print(f\"    Cumulative: {((1 + ls_clipped['long_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Short (bottom-20):\")\n",
    "print(f\"    Cumulative: {((1 + ls_clipped['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"    Short P&L: {((1 - ls_clipped['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Long-Short:\")\n",
    "print(f\"    Daily mean: {ls_clipped['long_short_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Sharpe: {ls_clipped['long_short_return'].mean() / ls_clipped['long_short_return'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"    Cumulative: {((1 + ls_clipped['long_short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcxjwdafeg",
   "metadata": {},
   "source": [
    "### 5.4 Shortability Analysis\n",
    "\n",
    "Filter to stocks that are practically shortable:\n",
    "- Market cap > $500M (institutional threshold)\n",
    "- Exclude penny stocks and micro-caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fxlrizjqnqk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market cap data for 5,564 symbols\n",
      "\n",
      "Market cap distribution:\n",
      "count    5.564000e+03\n",
      "mean     2.053745e+11\n",
      "std      5.247073e+12\n",
      "min      0.000000e+00\n",
      "25%      8.948057e+07\n",
      "50%      5.611468e+08\n",
      "75%      4.123179e+09\n",
      "max      3.111042e+14\n",
      "Name: marketCap, dtype: float64\n",
      "\n",
      "Market cap tiers:\n",
      "tier\n",
      "Micro (<$300M)       1941\n",
      "Small ($300M-$2B)    1414\n",
      "Mid ($2B-$10B)        979\n",
      "Large (>$10B)         879\n",
      "Unknown               351\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load fundamentals to get market cap\n",
    "key_metrics = pd.read_parquet(\"data/key_metrics.pqt\")\n",
    "\n",
    "# Get latest market cap per symbol (most recent quarter)\n",
    "key_metrics[\"date\"] = pd.to_datetime(key_metrics[\"date\"])\n",
    "latest_mcap = (\n",
    "    key_metrics[[\"symbol\", \"date\", \"marketCap\"]]\n",
    "    .sort_values(\"date\")\n",
    "    .groupby(\"symbol\")\n",
    "    .last()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"Market cap data for {len(latest_mcap):,} symbols\")\n",
    "print(f\"\\nMarket cap distribution:\")\n",
    "print(latest_mcap[\"marketCap\"].describe())\n",
    "\n",
    "# Define market cap tiers\n",
    "def mcap_tier(mcap):\n",
    "    if pd.isna(mcap) or mcap <= 0:\n",
    "        return \"Unknown\"\n",
    "    elif mcap < 300_000_000:\n",
    "        return \"Micro (<$300M)\"\n",
    "    elif mcap < 2_000_000_000:\n",
    "        return \"Small ($300M-$2B)\"\n",
    "    elif mcap < 10_000_000_000:\n",
    "        return \"Mid ($2B-$10B)\"\n",
    "    else:\n",
    "        return \"Large (>$10B)\"\n",
    "\n",
    "latest_mcap[\"tier\"] = latest_mcap[\"marketCap\"].apply(mcap_tier)\n",
    "print(f\"\\nMarket cap tiers:\")\n",
    "print(latest_mcap[\"tier\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cilmh1wzhwf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottom-20 picks by market cap tier:\n",
      "tier\n",
      "Mid ($2B-$10B)       1091\n",
      "Small ($300M-$2B)     979\n",
      "Micro (<$300M)        700\n",
      "Large (>$10B)         670\n",
      "Unknown               120\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Most frequent bottom-20 stocks with their market caps:\n",
      "  QBTS: 45 days, mcap=$8.5B\n",
      "  RGC: 34 days, mcap=$8.4B\n",
      "  ABTC: 34 days, mcap=$4.0B\n",
      "  IONQ: 31 days, mcap=$18.1B\n",
      "  TLRY: 31 days, mcap=$1.5B\n",
      "  QMCO: 30 days, mcap=$0.1B\n",
      "  BBAI: 30 days, mcap=$2.2B\n",
      "  ASTS: 28 days, mcap=$16.6B\n",
      "  QUBT: 28 days, mcap=$3.0B\n",
      "  ACON: 27 days, mcap=$0.0B\n",
      "  SEDG: 25 days, mcap=$2.2B\n",
      "  JOBY: 24 days, mcap=$13.6B\n",
      "  FIG: 24 days, mcap=$47.4B\n",
      "  NBIS: 21 days, mcap=$28.3B\n",
      "  RGTI: 21 days, mcap=$3.5B\n",
      "  HSDT: 20 days, mcap=$0.0B\n",
      "  AEVA: 19 days, mcap=$0.8B\n",
      "  LCID: 18 days, mcap=$7.4B\n",
      "  HTZ: 18 days, mcap=$2.1B\n",
      "  AAOI: 18 days, mcap=$1.6B\n"
     ]
    }
   ],
   "source": [
    "# Merge market cap into test_df\n",
    "test_df_mcap = test_df.merge(\n",
    "    latest_mcap[[\"symbol\", \"marketCap\"]], \n",
    "    on=\"symbol\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Check how many of our bottom-20 picks have market cap data\n",
    "bottom_with_mcap = bottom_df.merge(latest_mcap[[\"symbol\", \"marketCap\"]], on=\"symbol\", how=\"left\")\n",
    "print(\"Bottom-20 picks by market cap tier:\")\n",
    "bottom_with_mcap[\"tier\"] = bottom_with_mcap[\"marketCap\"].apply(mcap_tier)\n",
    "print(bottom_with_mcap[\"tier\"].value_counts())\n",
    "\n",
    "print(\"\\n\\nMost frequent bottom-20 stocks with their market caps:\")\n",
    "freq_symbols = bottom_df[\"symbol\"].value_counts().head(20).index.tolist()\n",
    "for sym in freq_symbols:\n",
    "    mcap = latest_mcap[latest_mcap[\"symbol\"] == sym][\"marketCap\"].values\n",
    "    mcap_str = f\"${mcap[0]/1e9:.1f}B\" if len(mcap) > 0 and mcap[0] > 0 else \"N/A\"\n",
    "    count = bottom_df[bottom_df[\"symbol\"] == sym].shape[0]\n",
    "    print(f\"  {sym}: {count} days, mcap={mcap_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "qk7ud97hd6p",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortable universe (mcap >= $500M): 2,861 symbols\n",
      "Test rows: 352,213 -> 334,455 (95.0%)\n",
      "\n",
      "Long-Short on Shortable Universe (mcap >= $500M):\n",
      "==================================================\n",
      "\n",
      "  Trading days: 178\n",
      "\n",
      "  Long (top-20):\n",
      "    Cumulative: -32.4%\n",
      "\n",
      "  Short (bottom-20):\n",
      "    Cumulative: -33.9%\n",
      "    Short P&L: 33.2%\n",
      "\n",
      "  Long-Short:\n",
      "    Daily mean: 0.004%\n",
      "    Sharpe: 0.03\n",
      "    Cumulative: -2.6%\n",
      "    Win rate: 49.4%\n"
     ]
    }
   ],
   "source": [
    "# Re-evaluate long-short with market cap filter\n",
    "MIN_MCAP = 500_000_000  # $500M minimum\n",
    "\n",
    "# Filter to shortable universe\n",
    "shortable_symbols = set(latest_mcap[latest_mcap[\"marketCap\"] >= MIN_MCAP][\"symbol\"])\n",
    "test_shortable = test_df_mcap[test_df_mcap[\"symbol\"].isin(shortable_symbols)].copy()\n",
    "\n",
    "print(f\"Shortable universe (mcap >= ${MIN_MCAP/1e6:.0f}M): {len(shortable_symbols):,} symbols\")\n",
    "print(f\"Test rows: {len(test_df_mcap):,} -> {len(test_shortable):,} ({len(test_shortable)/len(test_df_mcap)*100:.1f}%)\")\n",
    "\n",
    "# Re-run long-short on shortable universe\n",
    "def compute_long_short_returns_filtered(df, top_k=20, clip_pct=0.50):\n",
    "    \"\"\"Long-short with clipped returns on filtered universe.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"target_return_clipped\"] = df[\"target_return\"].clip(-clip_pct, clip_pct)\n",
    "    \n",
    "    returns = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < top_k * 2:\n",
    "            continue\n",
    "        \n",
    "        top = group.nlargest(top_k, \"score\")\n",
    "        long_ret = top[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        bottom = group.nsmallest(top_k, \"score\")\n",
    "        short_ret = bottom[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        ls_ret = long_ret - short_ret\n",
    "        mkt_ret = group[\"target_return_clipped\"].mean()\n",
    "        \n",
    "        returns.append({\n",
    "            \"date\": date,\n",
    "            \"long_return\": long_ret,\n",
    "            \"short_return\": short_ret,\n",
    "            \"long_short_return\": ls_ret,\n",
    "            \"market_return\": mkt_ret,\n",
    "        })\n",
    "    return pd.DataFrame(returns)\n",
    "\n",
    "ls_shortable = compute_long_short_returns_filtered(test_shortable, top_k=20, clip_pct=0.50)\n",
    "\n",
    "print(f\"\\nLong-Short on Shortable Universe (mcap >= $500M):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\n  Trading days: {len(ls_shortable)}\")\n",
    "\n",
    "print(f\"\\n  Long (top-20):\")\n",
    "print(f\"    Cumulative: {((1 + ls_shortable['long_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Short (bottom-20):\")\n",
    "print(f\"    Cumulative: {((1 + ls_shortable['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"    Short P&L: {((1 - ls_shortable['short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n  Long-Short:\")\n",
    "print(f\"    Daily mean: {ls_shortable['long_short_return'].mean()*100:.3f}%\")\n",
    "print(f\"    Sharpe: {ls_shortable['long_short_return'].mean() / ls_shortable['long_short_return'].std() * np.sqrt(252):.2f}\")\n",
    "print(f\"    Cumulative: {((1 + ls_shortable['long_short_return']).cumprod().iloc[-1] - 1)*100:.1f}%\")\n",
    "print(f\"    Win rate: {(ls_shortable['long_short_return'] > 0).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "higo5py96vg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent bottom-20 stocks (shortable universe):\n",
      "  QBTS: 50 days, mcap=$8.5B\n",
      "  ABTC: 40 days, mcap=$4.0B\n",
      "  RGC: 39 days, mcap=$8.4B\n",
      "  IONQ: 36 days, mcap=$18.1B\n",
      "  TLRY: 35 days, mcap=$1.5B\n",
      "  ASTS: 35 days, mcap=$16.6B\n",
      "  BBAI: 34 days, mcap=$2.2B\n",
      "  QUBT: 34 days, mcap=$3.0B\n",
      "  SEDG: 31 days, mcap=$2.2B\n",
      "  JOBY: 29 days, mcap=$13.6B\n",
      "  APLD: 27 days, mcap=$4.1B\n",
      "  NBIS: 26 days, mcap=$28.3B\n",
      "  FIG: 25 days, mcap=$47.4B\n",
      "  LCID: 24 days, mcap=$7.4B\n",
      "  AEVA: 24 days, mcap=$0.8B\n",
      "  HTZ: 24 days, mcap=$2.1B\n",
      "  RGTI: 23 days, mcap=$3.5B\n",
      "  NVTS: 23 days, mcap=$1.5B\n",
      "  BKSY: 23 days, mcap=$0.7B\n",
      "  SERV: 22 days, mcap=$0.7B\n",
      "\n",
      "\n",
      "Return distribution of shortable bottom-20:\n",
      "count    3560.000000\n",
      "mean       -0.002293\n",
      "std         0.093994\n",
      "min        -3.540930\n",
      "25%        -0.028342\n",
      "50%        -0.002292\n",
      "75%         0.022554\n",
      "max         1.343235\n",
      "Name: target_return, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# What stocks end up in the shortable bottom-20?\n",
    "bottom_shortable = []\n",
    "for date, group in test_shortable.groupby(\"feature_date\"):\n",
    "    if len(group) < 40:\n",
    "        continue\n",
    "    bottom = group.nsmallest(20, \"score\")\n",
    "    bottom_shortable.append(bottom[[\"symbol\", \"feature_date\", \"target_return\", \"score\", \"marketCap\"]])\n",
    "\n",
    "bottom_shortable_df = pd.concat(bottom_shortable)\n",
    "\n",
    "print(\"Most frequent bottom-20 stocks (shortable universe):\")\n",
    "freq = bottom_shortable_df[\"symbol\"].value_counts().head(20)\n",
    "for sym, count in freq.items():\n",
    "    mcap = bottom_shortable_df[bottom_shortable_df[\"symbol\"] == sym][\"marketCap\"].iloc[0]\n",
    "    print(f\"  {sym}: {count} days, mcap=${mcap/1e9:.1f}B\")\n",
    "\n",
    "print(f\"\\n\\nReturn distribution of shortable bottom-20:\")\n",
    "print(bottom_shortable_df[\"target_return\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1b2c3d4-0001-0001-0001-000000000031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to data/model_final_best.pt\n"
     ]
    }
   ],
   "source": [
    "# Save model and config\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"config\": config,\n",
    "    \"price_cols\": price_feat_cols,\n",
    "    \"fund_cols\": fund_feat_cols,\n",
    "    \"emb_cols\": emb_cols,\n",
    "}, \"data/model_final_full.pt\")\n",
    "\n",
    "print(\"Model saved to data/model_final_best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1b2c3d4-0001-0001-0001-000000000032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training history\n",
    "history_df = pd.DataFrame(history)\n",
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3757c51b-7472-486f-9987-065c690c0544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
