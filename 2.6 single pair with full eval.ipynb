{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 2.6 Single Pair Training with Full Eval\n",
    "\n",
    "Based on 2.5, with improvements:\n",
    "1. **25 epochs max** - prevent overfitting\n",
    "2. **Full-pair evaluation** - eval on ALL pairs for stable signal\n",
    "3. **Baseline comparison** - compare to model_final.pt\n",
    "4. **Track strategy metrics** - IC Sharpe and top-K returns per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    # Feature dimensions\n",
    "    n_fundamental_features: int = 19\n",
    "    n_price_features: int = 9\n",
    "    n_embedding_dim: int = 768\n",
    "    \n",
    "    # Encoder latent dimensions\n",
    "    fundamental_latent: int = 32\n",
    "    price_latent: int = 16\n",
    "    news_latent: int = 32\n",
    "    \n",
    "    # Dropout - tunable per encoder\n",
    "    fundamental_dropout: float = 0.95  # HIGH - combat 60x overexposure\n",
    "    price_dropout: float = 0.4         # MEDIUM\n",
    "    news_dropout: float = 0.2          # Standard\n",
    "    \n",
    "    # News influence cap\n",
    "    news_alpha: float = 0.8\n",
    "    \n",
    "    # Training\n",
    "    batch_size: int = 512\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-3\n",
    "    n_epochs: int = 25  # Reduced from 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/ml_dataset.pqt\")\n",
    "df[\"feature_date\"] = pd.to_datetime(df[\"feature_date\"])\n",
    "\n",
    "print(f\"Dataset: {len(df):,} rows\")\n",
    "print(f\"Date range: {df['feature_date'].min().date()} to {df['feature_date'].max().date()}\")\n",
    "print(f\"Symbols: {df['symbol'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns\n",
    "price_feat_cols = [\n",
    "    \"overnight_gap_z\", \"intraday_ret_z\",\n",
    "    \"ret_1d_z\", \"ret_2d_z\", \"ret_3d_z\", \"ret_5d_z\",\n",
    "    \"vol_5d_z\", \"dist_from_high_5d_z\", \"dist_from_low_5d_z\"\n",
    "]\n",
    "fund_feat_cols = [c for c in df.columns if c.endswith(\"_z\") and c not in price_feat_cols and c != \"news_count_z\"]\n",
    "emb_cols = [c for c in df.columns if c.startswith(\"emb_\")]\n",
    "\n",
    "print(f\"Price features: {len(price_feat_cols)}\")\n",
    "print(f\"Fundamental features: {len(fund_feat_cols)}\")\n",
    "print(f\"Embedding dims: {len(emb_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based split\n",
    "dates = sorted(df[\"feature_date\"].unique())\n",
    "n_dates = len(dates)\n",
    "train_end_idx = int(n_dates * 0.7)\n",
    "val_end_idx = int(n_dates * 0.8)\n",
    "\n",
    "train_dates = set(dates[:train_end_idx])\n",
    "val_dates = set(dates[train_end_idx:val_end_idx])\n",
    "test_dates = set(dates[val_end_idx:])\n",
    "\n",
    "train_df = df[df[\"feature_date\"].isin(train_dates)].copy()\n",
    "val_df = df[df[\"feature_date\"].isin(val_dates)].copy()\n",
    "test_df = df[df[\"feature_date\"].isin(test_dates)].copy()\n",
    "\n",
    "print(f\"Train: {len(train_df):,} rows, {len(train_dates)} days\")\n",
    "print(f\"Val: {len(val_df):,} rows, {len(val_dates)} days\")\n",
    "print(f\"Test: {len(test_df):,} rows, {len(test_dates)} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 2. Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinglePairDataset(Dataset):\n",
    "    \"\"\"Dataset where each symbol appears in exactly one pair per day.\n",
    "    \n",
    "    Shuffle symbols each day, pair consecutively: (0,1), (2,3), (4,5)...\n",
    "    This ensures no symbol dominates training through repeated exposure.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        price_cols: list[str],\n",
    "        fund_cols: list[str],\n",
    "        emb_cols: list[str],\n",
    "    ):\n",
    "        # Filter to rows with news only\n",
    "        has_news = (df[emb_cols].abs().sum(axis=1) > 0)\n",
    "        df_news = df[has_news].copy().reset_index(drop=True)\n",
    "        print(f\"Filtered to news-only: {len(df_news):,} / {len(df):,} rows ({len(df_news)/len(df)*100:.1f}%)\")\n",
    "\n",
    "        self.df = df_news\n",
    "        self.price_cols = price_cols\n",
    "        self.fund_cols = fund_cols\n",
    "        self.emb_cols = emb_cols\n",
    "\n",
    "        # Group indices by date - use reset df so indices match arrays\n",
    "        self.date_groups = {}\n",
    "        for date, group in self.df.groupby(\"feature_date\"):\n",
    "            indices = group.index.tolist()\n",
    "            if len(indices) < 2:\n",
    "                continue\n",
    "            self.date_groups[date] = indices\n",
    "\n",
    "        self.dates = list(self.date_groups.keys())\n",
    "        print(f\"Days with sufficient news coverage: {len(self.dates)}\")\n",
    "\n",
    "        # Precompute arrays (indices now 0 to len-1)\n",
    "        self.price_arr = self.df[price_cols].values.astype(np.float32)\n",
    "        self.fund_arr = self.df[fund_cols].values.astype(np.float32)\n",
    "        self.emb_arr = self.df[emb_cols].values.astype(np.float32)\n",
    "        self.target_arr = self.df[\"target_return\"].values.astype(np.float32)\n",
    "\n",
    "        # Generate initial pairs\n",
    "        self.pairs = []\n",
    "        self._generate_pairs()\n",
    "\n",
    "    def _generate_pairs(self):\n",
    "        \"\"\"Generate pairs: shuffle each day, pair consecutively.\"\"\"\n",
    "        pairs = []\n",
    "        \n",
    "        for date in self.dates:\n",
    "            indices = list(self.date_groups[date])  # copy\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            # Pair consecutively: (0,1), (2,3), (4,5)...\n",
    "            for i in range(0, len(indices) - 1, 2):\n",
    "                pairs.append((indices[i], indices[i + 1]))\n",
    "        \n",
    "        self.pairs = pairs\n",
    "        \n",
    "        # Stats\n",
    "        avg_pairs_per_day = len(pairs) / len(self.dates)\n",
    "        print(f\"Generated {len(self.pairs):,} pairs ({avg_pairs_per_day:.0f} per day avg)\")\n",
    "\n",
    "    def resample_pairs(self):\n",
    "        \"\"\"Regenerate pairs with new shuffle.\"\"\"\n",
    "        self._generate_pairs()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i, j = self.pairs[idx]\n",
    "\n",
    "        price_i = self.price_arr[i]\n",
    "        price_j = self.price_arr[j]\n",
    "        fund_i = self.fund_arr[i]\n",
    "        fund_j = self.fund_arr[j]\n",
    "        emb_i = self.emb_arr[i]\n",
    "        emb_j = self.emb_arr[j]\n",
    "\n",
    "        actual_label = 1.0 if self.target_arr[i] > self.target_arr[j] else 0.0\n",
    "\n",
    "        # Random swap for label balance\n",
    "        if np.random.random() < 0.5:\n",
    "            price_i, price_j = price_j, price_i\n",
    "            fund_i, fund_j = fund_j, fund_i\n",
    "            emb_i, emb_j = emb_j, emb_i\n",
    "            label = 1.0 - actual_label\n",
    "        else:\n",
    "            label = actual_label\n",
    "\n",
    "        return {\n",
    "            \"price_i\": torch.tensor(price_i),\n",
    "            \"price_j\": torch.tensor(price_j),\n",
    "            \"fund_i\": torch.tensor(fund_i),\n",
    "            \"fund_j\": torch.tensor(fund_j),\n",
    "            \"emb_i\": torch.tensor(emb_i),\n",
    "            \"emb_j\": torch.tensor(emb_j),\n",
    "            \"label\": torch.tensor(label),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBranchRanker(nn.Module):\n",
    "    \"\"\"Multi-branch model with high fundamental dropout.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # High dropout for fundamentals\n",
    "        self.fund_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_fundamental_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.fundamental_dropout),\n",
    "            nn.Linear(64, config.fundamental_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Medium dropout for price\n",
    "        self.price_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_price_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.price_dropout),\n",
    "            nn.Linear(32, config.price_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Standard dropout for news\n",
    "        self.news_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.news_dropout),\n",
    "            nn.Linear(128, config.news_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        fused_dim = config.fundamental_latent + config.price_latent + config.news_latent\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, price, fund, emb):\n",
    "        h_f = self.fund_encoder(fund)\n",
    "        h_p = self.price_encoder(price)\n",
    "        h_n = self.news_encoder(emb)\n",
    "        h_n_scaled = self.config.news_alpha * h_n\n",
    "        h = torch.cat([h_f, h_p, h_n_scaled], dim=-1)\n",
    "        return self.output_head(h).squeeze(-1)\n",
    "    \n",
    "    def forward_pair(self, price_i, fund_i, emb_i, price_j, fund_j, emb_j):\n",
    "        score_i = self.forward(price_i, fund_i, emb_i)\n",
    "        score_j = self.forward(price_j, fund_j, emb_j)\n",
    "        return torch.sigmoid(score_i - score_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_ranking_loss(pred_prob, label, smoothing=0.1):\n",
    "    smoothed_label = label * (1 - smoothing) + 0.5 * smoothing\n",
    "    return F.binary_cross_entropy(pred_prob, smoothed_label)\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        price_i = batch[\"price_i\"].to(device)\n",
    "        price_j = batch[\"price_j\"].to(device)\n",
    "        fund_i = batch[\"fund_i\"].to(device)\n",
    "        fund_j = batch[\"fund_j\"].to(device)\n",
    "        emb_i = batch[\"emb_i\"].to(device)\n",
    "        emb_j = batch[\"emb_j\"].to(device)\n",
    "        label = batch[\"label\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred_prob = model.forward_pair(price_i, fund_i, emb_i, price_j, fund_j, emb_j)\n",
    "        loss = pairwise_ranking_loss(pred_prob, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(label)\n",
    "        total_correct += ((pred_prob > 0.5) == (label > 0.5)).sum().item()\n",
    "        total_samples += len(label)\n",
    "    \n",
    "    return total_loss / total_samples, total_correct / total_samples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_scores(model, df, price_cols, fund_cols, emb_cols, device, batch_size=1024):\n",
    "    \"\"\"Score all rows in dataframe.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    price_arr = torch.tensor(df[price_cols].values.astype(np.float32))\n",
    "    fund_arr = torch.tensor(df[fund_cols].values.astype(np.float32))\n",
    "    emb_arr = torch.tensor(df[emb_cols].values.astype(np.float32))\n",
    "    \n",
    "    scores = []\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        price = price_arr[i:i+batch_size].to(device)\n",
    "        fund = fund_arr[i:i+batch_size].to(device)\n",
    "        emb = emb_arr[i:i+batch_size].to(device)\n",
    "        score = model(price, fund, emb)\n",
    "        scores.append(score.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(scores)\n",
    "\n",
    "\n",
    "def compute_daily_ic(df):\n",
    "    \"\"\"Compute Spearman IC per day.\"\"\"\n",
    "    ics = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < 10:\n",
    "            continue\n",
    "        ic, _ = spearmanr(group[\"score\"], group[\"target_return\"])\n",
    "        if not np.isnan(ic):\n",
    "            ics.append({\"date\": date, \"ic\": ic})\n",
    "    return pd.DataFrame(ics)\n",
    "\n",
    "\n",
    "def compute_short_returns(df, k=5, clip_return=0.10):\n",
    "    \"\"\"Compute daily short returns for bottom-K with return clipping.\"\"\"\n",
    "    returns = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < k * 2:\n",
    "            continue\n",
    "        bottom = group.nsmallest(k, \"score\")\n",
    "        clipped_returns = bottom[\"target_return\"].clip(-clip_return, clip_return)\n",
    "        short_ret = -clipped_returns.mean()\n",
    "        returns.append({\"date\": date, \"return\": short_ret})\n",
    "    return pd.DataFrame(returns)\n",
    "\n",
    "\n",
    "def evaluate_model(model, df, price_cols, fund_cols, emb_cols, device, k=5):\n",
    "    \"\"\"Full evaluation: IC Sharpe and strategy metrics.\"\"\"\n",
    "    df_eval = df.copy()\n",
    "    df_eval[\"score\"] = get_scores(model, df_eval, price_cols, fund_cols, emb_cols, device)\n",
    "    \n",
    "    # IC metrics\n",
    "    ic_df = compute_daily_ic(df_eval)\n",
    "    mean_ic = ic_df['ic'].mean()\n",
    "    ic_std = ic_df['ic'].std()\n",
    "    ic_sharpe = mean_ic / ic_std * np.sqrt(252) if ic_std > 0 else 0\n",
    "    \n",
    "    # Strategy metrics\n",
    "    short_df = compute_short_returns(df_eval, k=k, clip_return=0.10)\n",
    "    if len(short_df) > 1:\n",
    "        short_mean = short_df['return'].mean()\n",
    "        short_std = short_df['return'].std()\n",
    "        short_sharpe = short_mean / short_std * np.sqrt(252) if short_std > 0 else 0\n",
    "        short_cumret = (1 + short_df['return']).cumprod().iloc[-1] - 1\n",
    "    else:\n",
    "        short_sharpe = 0\n",
    "        short_cumret = 0\n",
    "    \n",
    "    return {\n",
    "        \"mean_ic\": mean_ic,\n",
    "        \"ic_sharpe\": ic_sharpe,\n",
    "        \"short_sharpe\": short_sharpe,\n",
    "        \"short_cumret\": short_cumret,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Load Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline model_final.pt for comparison\n",
    "baseline_path = Path(\"data/model_final.pt\")\n",
    "if baseline_path.exists():\n",
    "    baseline_ckpt = torch.load(baseline_path, map_location=device, weights_only=False)\n",
    "    \n",
    "    # Create baseline model with same config structure\n",
    "    baseline_config = baseline_ckpt.get(\"config\", ModelConfig())\n",
    "    baseline_model = MultiBranchRanker(baseline_config).to(device)\n",
    "    baseline_model.load_state_dict(baseline_ckpt[\"model_state_dict\"])\n",
    "    baseline_model.eval()\n",
    "    \n",
    "    print(\"Loaded baseline model_final.pt\")\n",
    "    print(f\"  Fundamental dropout: {baseline_config.fundamental_dropout}\")\n",
    "    print(f\"  Price dropout: {baseline_config.price_dropout}\")\n",
    "else:\n",
    "    baseline_model = None\n",
    "    print(\"No baseline model found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline on val and test sets\n",
    "if baseline_model is not None:\n",
    "    print(\"Evaluating baseline model...\")\n",
    "    baseline_val = evaluate_model(baseline_model, val_df, price_feat_cols, fund_feat_cols, emb_cols, device)\n",
    "    baseline_test = evaluate_model(baseline_model, test_df, price_feat_cols, fund_feat_cols, emb_cols, device)\n",
    "    \n",
    "    print(\"\\nBaseline Results:\")\n",
    "    print(f\"  Val  IC Sharpe: {baseline_val['ic_sharpe']:.2f}, Short Sharpe: {baseline_val['short_sharpe']:.2f}\")\n",
    "    print(f\"  Test IC Sharpe: {baseline_test['ic_sharpe']:.2f}, Short Sharpe: {baseline_test['short_sharpe']:.2f}\")\n",
    "else:\n",
    "    baseline_val = baseline_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = ModelConfig(\n",
    "    n_fundamental_features=len(fund_feat_cols),\n",
    "    n_price_features=len(price_feat_cols),\n",
    "    n_embedding_dim=len(emb_cols),\n",
    "    fundamental_dropout=0.95,  # HIGH\n",
    "    price_dropout=0.4,         # MEDIUM\n",
    "    news_dropout=0.2,          # Standard\n",
    "    n_epochs=25,               # Reduced\n",
    ")\n",
    "\n",
    "print(f\"Config:\")\n",
    "print(f\"  Epochs: {config.n_epochs}\")\n",
    "print(f\"  Fundamental dropout: {config.fundamental_dropout}\")\n",
    "print(f\"  Price dropout: {config.price_dropout}\")\n",
    "print(f\"  News dropout: {config.news_dropout}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset (single-pair)\n",
    "train_dataset = SinglePairDataset(train_df, price_feat_cols, fund_feat_cols, emb_cols)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = MultiBranchRanker(config).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.n_epochs)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with full evaluation each epoch\n",
    "best_val_ic_sharpe = -float('inf')\n",
    "best_state = None\n",
    "history = []\n",
    "\n",
    "for epoch in range(config.n_epochs):\n",
    "    # Reshuffle pairs each epoch\n",
    "    train_dataset.resample_pairs()\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Full evaluation on val set (ALL rows, not single-pair)\n",
    "    val_metrics = evaluate_model(model, val_df, price_feat_cols, fund_feat_cols, emb_cols, device)\n",
    "    \n",
    "    history.append({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"val_ic_sharpe\": val_metrics[\"ic_sharpe\"],\n",
    "        \"val_short_sharpe\": val_metrics[\"short_sharpe\"],\n",
    "        \"val_mean_ic\": val_metrics[\"mean_ic\"],\n",
    "    })\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{config.n_epochs}: \"\n",
    "          f\"loss={train_loss:.4f}, acc={train_acc:.4f}, \"\n",
    "          f\"val_IC_sharpe={val_metrics['ic_sharpe']:.2f}, \"\n",
    "          f\"val_short_sharpe={val_metrics['short_sharpe']:.2f}\")\n",
    "    \n",
    "    # Track best by IC Sharpe\n",
    "    if val_metrics[\"ic_sharpe\"] > best_val_ic_sharpe:\n",
    "        best_val_ic_sharpe = val_metrics[\"ic_sharpe\"]\n",
    "        best_state = model.state_dict().copy()\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(best_state)\n",
    "print(f\"\\nBest val IC Sharpe: {best_val_ic_sharpe:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "hist_df = pd.DataFrame(history)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Loss\n",
    "ax = axes[0, 0]\n",
    "ax.plot(hist_df['epoch'], hist_df['train_loss'])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Training Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax = axes[0, 1]\n",
    "ax.plot(hist_df['epoch'], hist_df['train_acc'])\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Training Accuracy')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Val IC Sharpe\n",
    "ax = axes[1, 0]\n",
    "ax.plot(hist_df['epoch'], hist_df['val_ic_sharpe'], label='Single-pair model')\n",
    "if baseline_val is not None:\n",
    "    ax.axhline(baseline_val['ic_sharpe'], color='red', linestyle='--', label=f'Baseline: {baseline_val[\"ic_sharpe\"]:.2f}')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('IC Sharpe')\n",
    "ax.set_title('Validation IC Sharpe')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Val Short Sharpe\n",
    "ax = axes[1, 1]\n",
    "ax.plot(hist_df['epoch'], hist_df['val_short_sharpe'], label='Single-pair model')\n",
    "if baseline_val is not None:\n",
    "    ax.axhline(baseline_val['short_sharpe'], color='red', linestyle='--', label=f'Baseline: {baseline_val[\"short_sharpe\"]:.2f}')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Short Sharpe')\n",
    "ax.set_title('Validation Short Strategy Sharpe (K=5)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 7. Final Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set\n",
    "test_metrics = evaluate_model(model, test_df, price_feat_cols, fund_feat_cols, emb_cols, device)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mean IC:       {test_metrics['mean_ic']:.4f}\")\n",
    "print(f\"IC Sharpe:     {test_metrics['ic_sharpe']:.2f}\")\n",
    "print(f\"Short Sharpe:  {test_metrics['short_sharpe']:.2f}\")\n",
    "print(f\"Short Return:  {test_metrics['short_cumret']*100:.1f}%\")\n",
    "\n",
    "if baseline_test is not None:\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(\"BASELINE COMPARISON (model_final.pt)\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Baseline IC Sharpe:     {baseline_test['ic_sharpe']:.2f}\")\n",
    "    print(f\"Baseline Short Sharpe:  {baseline_test['short_sharpe']:.2f}\")\n",
    "    print(f\"\\nImprovement:\")\n",
    "    print(f\"  IC Sharpe:    {test_metrics['ic_sharpe'] - baseline_test['ic_sharpe']:+.2f}\")\n",
    "    print(f\"  Short Sharpe: {test_metrics['short_sharpe'] - baseline_test['short_sharpe']:+.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed IC analysis\n",
    "test_df_eval = test_df.copy()\n",
    "test_df_eval[\"score\"] = get_scores(model, test_df_eval, price_feat_cols, fund_feat_cols, emb_cols, device)\n",
    "\n",
    "ic_df = compute_daily_ic(test_df_eval)\n",
    "short_df = compute_short_returns(test_df_eval, k=5, clip_return=0.10)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Daily IC\n",
    "ax = axes[0]\n",
    "ax.bar(ic_df['date'], ic_df['ic'], alpha=0.7, width=1)\n",
    "ax.axhline(0, color='black', linewidth=0.5)\n",
    "ax.axhline(ic_df['ic'].mean(), color='red', linestyle='--', \n",
    "           label=f'Mean IC: {ic_df[\"ic\"].mean():.4f}')\n",
    "ax.set_ylabel('Daily IC')\n",
    "ax.set_title('Information Coefficient Over Time')\n",
    "ax.legend()\n",
    "\n",
    "# Cumulative returns\n",
    "ax = axes[1]\n",
    "cumret = (1 + short_df['return']).cumprod()\n",
    "ax.plot(short_df['date'], cumret, label=f'Single-pair model (Sharpe: {test_metrics[\"short_sharpe\"]:.2f})')\n",
    "ax.axhline(1, color='black', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel('Cumulative Return')\n",
    "ax.set_title('Short Strategy (K=5)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## 8. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"config\": config,\n",
    "    \"price_cols\": price_feat_cols,\n",
    "    \"fund_cols\": fund_feat_cols,\n",
    "    \"emb_cols\": emb_cols,\n",
    "    \"training_approach\": \"single_pair_per_symbol_v2\",\n",
    "    \"results\": {\n",
    "        \"test_mean_ic\": test_metrics[\"mean_ic\"],\n",
    "        \"test_ic_sharpe\": test_metrics[\"ic_sharpe\"],\n",
    "        \"test_short_sharpe\": test_metrics[\"short_sharpe\"],\n",
    "    },\n",
    "    \"baseline_comparison\": baseline_test,\n",
    "}, \"data/model_single_pair_v2.pt\")\n",
    "\n",
    "print(\"Saved model to data/model_single_pair_v2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
