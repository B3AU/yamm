{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.7 Hyperparameter Optimization\n",
    "\n",
    "Unified optimization of architecture, training, and dropout hyperparameters.\n",
    "\n",
    "**Selection Metric: IC Sharpe (not Short Sharpe)**\n",
    "- IC Sharpe is bounded and robust to outliers\n",
    "- Short Sharpe is highly sensitive to single extreme trades\n",
    "- We track both but SELECT on IC Sharpe\n",
    "\n",
    "**Search Space:**\n",
    "1. Architecture: latent dims, hidden sizes, news alpha\n",
    "2. Training: learning rate, weight decay, label smoothing\n",
    "3. Dropout: fundamental, price, news dropout rates\n",
    "\n",
    "**Approach:** Staged search to reduce combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Full model configuration.\"\"\"\n",
    "    # Feature dimensions (fixed by data)\n",
    "    n_fundamental_features: int = 19\n",
    "    n_price_features: int = 9\n",
    "    n_embedding_dim: int = 768\n",
    "    \n",
    "    # Hidden layer sizes\n",
    "    fund_hidden: int = 64\n",
    "    price_hidden: int = 32\n",
    "    news_hidden: int = 128\n",
    "    \n",
    "    # Encoder latent dimensions\n",
    "    fundamental_latent: int = 32\n",
    "    price_latent: int = 16\n",
    "    news_latent: int = 32\n",
    "    \n",
    "    # Dropout rates\n",
    "    fundamental_dropout: float = 0.5\n",
    "    price_dropout: float = 0.3\n",
    "    news_dropout: float = 0.2\n",
    "    \n",
    "    # News influence\n",
    "    news_alpha: float = 0.8\n",
    "    \n",
    "    # Training\n",
    "    batch_size: int = 512\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-3\n",
    "    label_smoothing: float = 0.1\n",
    "    n_epochs: int = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 2,092,929 rows\n",
      "Date range: 2021-01-13 to 2025-12-18\n",
      "Clipped 81 extreme values for training\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"data/ml_dataset.pqt\")\n",
    "df[\"feature_date\"] = pd.to_datetime(df[\"feature_date\"])\n",
    "\n",
    "# Convert log returns to simple returns\n",
    "df[\"simple_return\"] = np.exp(df[\"target_return\"]) - 1\n",
    "\n",
    "# Clip extreme values (data errors) - only for TRAINING\n",
    "CLIP_LIMIT = 1.0  # Â±100%\n",
    "n_clipped = ((df[\"simple_return\"] < -CLIP_LIMIT) | (df[\"simple_return\"] > CLIP_LIMIT)).sum()\n",
    "df[\"simple_return_clipped\"] = df[\"simple_return\"].clip(-CLIP_LIMIT, CLIP_LIMIT)\n",
    "\n",
    "print(f\"Dataset: {len(df):,} rows\")\n",
    "print(f\"Date range: {df['feature_date'].min().date()} to {df['feature_date'].max().date()}\")\n",
    "print(f\"Clipped {n_clipped:,} extreme values for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price features: 9\n",
      "Fundamental features: 19\n",
      "Embedding dims: 768\n"
     ]
    }
   ],
   "source": [
    "# Feature columns\n",
    "price_feat_cols = [\n",
    "    \"overnight_gap_z\", \"intraday_ret_z\",\n",
    "    \"ret_1d_z\", \"ret_2d_z\", \"ret_3d_z\", \"ret_5d_z\",\n",
    "    \"vol_5d_z\", \"dist_from_high_5d_z\", \"dist_from_low_5d_z\"\n",
    "]\n",
    "fund_feat_cols = [c for c in df.columns if c.endswith(\"_z\") and c not in price_feat_cols and c != \"news_count_z\"]\n",
    "emb_cols = [c for c in df.columns if c.startswith(\"emb_\")]\n",
    "\n",
    "print(f\"Price features: {len(price_feat_cols)}\")\n",
    "print(f\"Fundamental features: {len(fund_feat_cols)}\")\n",
    "print(f\"Embedding dims: {len(emb_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1,418,494 rows (2021-01-13 to 2024-05-01)\n",
      "Val:   210,247 rows (2024-05-02 to 2024-10-21)\n",
      "Test:  464,188 rows (2024-10-22 to 2025-12-18)\n",
      "\n",
      "Val (news-only):  58,882\n",
      "Test (news-only): 128,502\n"
     ]
    }
   ],
   "source": [
    "# Time-based split: 70% train, 10% val, 20% test\n",
    "dates = sorted(df[\"feature_date\"].unique())\n",
    "n_dates = len(dates)\n",
    "train_end_idx = int(n_dates * 0.7)\n",
    "val_end_idx = int(n_dates * 0.8)\n",
    "\n",
    "train_dates = set(dates[:train_end_idx])\n",
    "val_dates = set(dates[train_end_idx:val_end_idx])\n",
    "test_dates = set(dates[val_end_idx:])\n",
    "\n",
    "train_df = df[df[\"feature_date\"].isin(train_dates)].copy()\n",
    "val_df = df[df[\"feature_date\"].isin(val_dates)].copy()\n",
    "test_df = df[df[\"feature_date\"].isin(test_dates)].copy()\n",
    "\n",
    "print(f\"Train: {len(train_df):,} rows ({min(train_dates).date()} to {max(train_dates).date()})\")\n",
    "print(f\"Val:   {len(val_df):,} rows ({min(val_dates).date()} to {max(val_dates).date()})\")\n",
    "print(f\"Test:  {len(test_df):,} rows ({min(test_dates).date()} to {max(test_dates).date()})\")\n",
    "\n",
    "# News-only filtering for evaluation\n",
    "def filter_news_only(df_in, emb_cols):\n",
    "    has_news = (df_in[emb_cols].abs().sum(axis=1) > 0)\n",
    "    return df_in[has_news].copy()\n",
    "\n",
    "val_df_news = filter_news_only(val_df, emb_cols)\n",
    "test_df_news = filter_news_only(test_df, emb_cols)\n",
    "print(f\"\\nVal (news-only):  {len(val_df_news):,}\")\n",
    "print(f\"Test (news-only): {len(test_df_news):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinglePairDataset(Dataset):\n",
    "    \"\"\"Dataset where each symbol appears in exactly one pair per day.\"\"\"\n",
    "\n",
    "    def __init__(self, df, price_cols, fund_cols, emb_cols, use_clipped=True, verbose=True):\n",
    "        has_news = (df[emb_cols].abs().sum(axis=1) > 0)\n",
    "        df_news = df[has_news].copy().reset_index(drop=True)\n",
    "        if verbose:\n",
    "            print(f\"Filtered to news-only: {len(df_news):,} rows\")\n",
    "\n",
    "        self.df = df_news\n",
    "        self.price_cols = price_cols\n",
    "        self.fund_cols = fund_cols\n",
    "        self.emb_cols = emb_cols\n",
    "\n",
    "        self.date_groups = {}\n",
    "        for date, group in self.df.groupby(\"feature_date\"):\n",
    "            indices = group.index.tolist()\n",
    "            if len(indices) >= 2:\n",
    "                self.date_groups[date] = indices\n",
    "\n",
    "        self.dates = list(self.date_groups.keys())\n",
    "\n",
    "        self.price_arr = self.df[price_cols].values.astype(np.float32)\n",
    "        self.fund_arr = self.df[fund_cols].values.astype(np.float32)\n",
    "        self.emb_arr = self.df[emb_cols].values.astype(np.float32)\n",
    "        \n",
    "        # Use clipped returns for training (avoid extreme outlier influence)\n",
    "        target_col = \"simple_return_clipped\" if use_clipped else \"simple_return\"\n",
    "        self.target_arr = self.df[target_col].values.astype(np.float32)\n",
    "\n",
    "        self.pairs = []\n",
    "        self._generate_pairs(verbose=verbose)\n",
    "\n",
    "    def _generate_pairs(self, verbose=False):\n",
    "        pairs = []\n",
    "        for date in self.dates:\n",
    "            indices = list(self.date_groups[date])\n",
    "            np.random.shuffle(indices)\n",
    "            for i in range(0, len(indices) - 1, 2):\n",
    "                pairs.append((indices[i], indices[i + 1]))\n",
    "        self.pairs = pairs\n",
    "        if verbose:\n",
    "            print(f\"Generated {len(self.pairs):,} pairs\")\n",
    "\n",
    "    def resample_pairs(self):\n",
    "        self._generate_pairs()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i, j = self.pairs[idx]\n",
    "        price_i, price_j = self.price_arr[i], self.price_arr[j]\n",
    "        fund_i, fund_j = self.fund_arr[i], self.fund_arr[j]\n",
    "        emb_i, emb_j = self.emb_arr[i], self.emb_arr[j]\n",
    "        actual_label = 1.0 if self.target_arr[i] > self.target_arr[j] else 0.0\n",
    "\n",
    "        # Random swap for symmetry\n",
    "        if np.random.random() < 0.5:\n",
    "            price_i, price_j = price_j, price_i\n",
    "            fund_i, fund_j = fund_j, fund_i\n",
    "            emb_i, emb_j = emb_j, emb_i\n",
    "            actual_label = 1.0 - actual_label\n",
    "\n",
    "        return {\n",
    "            \"price_i\": torch.tensor(price_i), \"price_j\": torch.tensor(price_j),\n",
    "            \"fund_i\": torch.tensor(fund_i), \"fund_j\": torch.tensor(fund_j),\n",
    "            \"emb_i\": torch.tensor(emb_i), \"emb_j\": torch.tensor(emb_j),\n",
    "            \"label\": torch.tensor(actual_label),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBranchRanker(nn.Module):\n",
    "    \"\"\"Multi-branch ranking model with configurable architecture.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.fund_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_fundamental_features, config.fund_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.fundamental_dropout),\n",
    "            nn.Linear(config.fund_hidden, config.fundamental_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.price_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_price_features, config.price_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.price_dropout),\n",
    "            nn.Linear(config.price_hidden, config.price_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.news_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_embedding_dim, config.news_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.news_dropout),\n",
    "            nn.Linear(config.news_hidden, config.news_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        fused_dim = config.fundamental_latent + config.price_latent + config.news_latent\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, price, fund, emb):\n",
    "        h_f = self.fund_encoder(fund)\n",
    "        h_p = self.price_encoder(price)\n",
    "        h_n = self.news_encoder(emb)\n",
    "        h_n_scaled = self.config.news_alpha * h_n\n",
    "        h = torch.cat([h_f, h_p, h_n_scaled], dim=-1)\n",
    "        return self.output_head(h).squeeze(-1)\n",
    "    \n",
    "    def forward_pair(self, price_i, fund_i, emb_i, price_j, fund_j, emb_j):\n",
    "        score_i = self.forward(price_i, fund_i, emb_i)\n",
    "        score_j = self.forward(price_j, fund_j, emb_j)\n",
    "        return torch.sigmoid(score_i - score_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_ranking_loss(pred_prob, label, smoothing=0.1):\n",
    "    smoothed_label = label * (1 - smoothing) + 0.5 * smoothing\n",
    "    return F.binary_cross_entropy(pred_prob, smoothed_label)\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer, device, label_smoothing=0.1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        price_i = batch[\"price_i\"].to(device)\n",
    "        price_j = batch[\"price_j\"].to(device)\n",
    "        fund_i = batch[\"fund_i\"].to(device)\n",
    "        fund_j = batch[\"fund_j\"].to(device)\n",
    "        emb_i = batch[\"emb_i\"].to(device)\n",
    "        emb_j = batch[\"emb_j\"].to(device)\n",
    "        label = batch[\"label\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred_prob = model.forward_pair(price_i, fund_i, emb_i, price_j, fund_j, emb_j)\n",
    "        loss = pairwise_ranking_loss(pred_prob, label, smoothing=label_smoothing)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(label)\n",
    "        total_samples += len(label)\n",
    "    \n",
    "    return total_loss / total_samples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_scores(model, df, price_cols, fund_cols, emb_cols, device, batch_size=1024):\n",
    "    model.eval()\n",
    "    price_arr = torch.tensor(df[price_cols].values.astype(np.float32))\n",
    "    fund_arr = torch.tensor(df[fund_cols].values.astype(np.float32))\n",
    "    emb_arr = torch.tensor(df[emb_cols].values.astype(np.float32))\n",
    "    \n",
    "    scores = []\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        price = price_arr[i:i+batch_size].to(device)\n",
    "        fund = fund_arr[i:i+batch_size].to(device)\n",
    "        emb = emb_arr[i:i+batch_size].to(device)\n",
    "        score = model(price, fund, emb)\n",
    "        scores.append(score.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(scores)\n",
    "\n",
    "\n",
    "def evaluate_model(model, df, price_cols, fund_cols, emb_cols, device, k=5):\n",
    "    \"\"\"Compute IC Sharpe and short strategy Sharpe.\n",
    "    \n",
    "    IC Sharpe: Uses ranks, bounded, robust to outliers\n",
    "    Short Sharpe: Uses actual returns, sensitive to outliers (for reporting only)\n",
    "    \"\"\"\n",
    "    df_eval = df.copy()\n",
    "    df_eval[\"score\"] = get_scores(model, df_eval, price_cols, fund_cols, emb_cols, device)\n",
    "    \n",
    "    # IC Sharpe (rank-based, robust)\n",
    "    ics = []\n",
    "    for date, group in df_eval.groupby(\"feature_date\"):\n",
    "        if len(group) < 10:\n",
    "            continue\n",
    "        ic, _ = spearmanr(group[\"score\"], group[\"simple_return\"])\n",
    "        if not np.isnan(ic):\n",
    "            ics.append(ic)\n",
    "    \n",
    "    mean_ic = np.mean(ics) if ics else 0\n",
    "    ic_std = np.std(ics) if ics else 1\n",
    "    ic_sharpe = mean_ic / ic_std * np.sqrt(252) if ic_std > 0 else 0\n",
    "    \n",
    "    # Short strategy Sharpe (for reporting, sensitive to outliers)\n",
    "    returns = []\n",
    "    for date, group in df_eval.groupby(\"feature_date\"):\n",
    "        if len(group) < 10:\n",
    "            continue\n",
    "        bottom = group.nsmallest(k, \"score\")\n",
    "        short_ret = -bottom[\"simple_return\"].mean()\n",
    "        returns.append(short_ret)\n",
    "    \n",
    "    if len(returns) > 1:\n",
    "        short_sharpe = np.mean(returns) / np.std(returns) * np.sqrt(252)\n",
    "    else:\n",
    "        short_sharpe = 0\n",
    "    \n",
    "    return {\"mean_ic\": mean_ic, \"ic_sharpe\": ic_sharpe, \"short_sharpe\": short_sharpe}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(config, train_dataset, val_df_news, n_epochs=15, verbose=False):\n",
    "    \"\"\"Train a model and return validation metrics.\n",
    "    \n",
    "    Selects best checkpoint by IC Sharpe (robust to outliers).\n",
    "    \"\"\"\n",
    "    model = MultiBranchRanker(config).to(device)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=config.learning_rate, \n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    \n",
    "    best_ic_sharpe = -float('inf')\n",
    "    best_state = None\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        train_dataset.resample_pairs()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device, config.label_smoothing)\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Evaluate every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0 or epoch == n_epochs - 1:\n",
    "            metrics = evaluate_model(model, val_df_news, price_feat_cols, fund_feat_cols, emb_cols, device)\n",
    "            # Select by IC Sharpe (robust to outliers)\n",
    "            if metrics[\"ic_sharpe\"] > best_ic_sharpe:\n",
    "                best_ic_sharpe = metrics[\"ic_sharpe\"]\n",
    "                best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            if verbose:\n",
    "                print(f\"  Epoch {epoch+1}: IC={metrics['ic_sharpe']:.2f}, Short={metrics['short_sharpe']:.2f}\")\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    final_metrics = evaluate_model(model, val_df_news, price_feat_cols, fund_feat_cols, emb_cols, device)\n",
    "    \n",
    "    return final_metrics, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered to news-only: 339,872 rows\n",
      "Generated 169,737 pairs\n"
     ]
    }
   ],
   "source": [
    "# Create training dataset\n",
    "train_dataset = SinglePairDataset(\n",
    "    train_df, price_feat_cols, fund_feat_cols, emb_cols, \n",
    "    use_clipped=True,  # Use clipped returns for training\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Search Space Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 (Architecture): 27 configs\n",
      "Stage 2 (Training):     27 configs\n",
      "Stage 3 (Dropout):      36 configs\n",
      "Total (staged):         90 configs\n",
      "Full grid would be:     972 configs\n"
     ]
    }
   ],
   "source": [
    "# Full search space (staged to reduce combinations)\n",
    "SEARCH_SPACE = {\n",
    "    # Stage 1: Architecture\n",
    "    \"latent_scale\": [0.5, 1.0, 2.0],\n",
    "    \"hidden_scale\": [0.5, 1.0, 1.5],\n",
    "    \"news_alpha\": [0.6, 0.8, 1.0],\n",
    "    \n",
    "    # Stage 2: Training\n",
    "    \"learning_rate\": [5e-4, 1e-3, 2e-3],\n",
    "    \"weight_decay\": [1e-4, 1e-3, 1e-2],\n",
    "    \"label_smoothing\": [0.05, 0.1, 0.15],\n",
    "    \n",
    "    # Stage 3: Dropout\n",
    "    \"fund_dropout\": [0.4, 0.5, 0.6, 0.7],\n",
    "    \"price_dropout\": [0.2, 0.3, 0.4],\n",
    "    \"news_dropout\": [0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "n_arch = len(SEARCH_SPACE[\"latent_scale\"]) * len(SEARCH_SPACE[\"hidden_scale\"]) * len(SEARCH_SPACE[\"news_alpha\"])\n",
    "n_train = len(SEARCH_SPACE[\"learning_rate\"]) * len(SEARCH_SPACE[\"weight_decay\"]) * len(SEARCH_SPACE[\"label_smoothing\"])\n",
    "n_dropout = len(SEARCH_SPACE[\"fund_dropout\"]) * len(SEARCH_SPACE[\"price_dropout\"]) * len(SEARCH_SPACE[\"news_dropout\"])\n",
    "\n",
    "print(f\"Stage 1 (Architecture): {n_arch} configs\")\n",
    "print(f\"Stage 2 (Training):     {n_train} configs\")\n",
    "print(f\"Stage 3 (Dropout):      {n_dropout} configs\")\n",
    "print(f\"Total (staged):         {n_arch + n_train + n_dropout} configs\")\n",
    "print(f\"Full grid would be:     {n_arch * n_train * n_dropout // (3*3*3):,} configs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stage 1: Architecture Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 1: ARCHITECTURE SEARCH\n",
      "Selection metric: IC Sharpe (robust to outliers)\n",
      "======================================================================\n",
      "[ 1/27] lat=0.5 hid=0.5 alpha=0.6 | IC= 4.79 short=-0.48 | 133s\n",
      "[ 2/27] lat=0.5 hid=0.5 alpha=0.8 | IC= 4.69 short=-0.40 | 134s\n",
      "[ 3/27] lat=0.5 hid=0.5 alpha=1.0 | IC= 4.19 short=-0.25 | 134s\n",
      "[ 4/27] lat=0.5 hid=1.0 alpha=0.6 | IC= 5.10 short=-0.94 | 146s\n",
      "[ 5/27] lat=0.5 hid=1.0 alpha=0.8 | IC= 4.32 short=-0.97 | 146s\n",
      "[ 6/27] lat=0.5 hid=1.0 alpha=1.0 | IC= 4.36 short=-0.42 | 146s\n",
      "[ 7/27] lat=0.5 hid=1.5 alpha=0.6 | IC= 4.59 short=-0.04 | 155s\n",
      "[ 8/27] lat=0.5 hid=1.5 alpha=0.8 | IC= 4.59 short= 0.06 | 156s\n",
      "[ 9/27] lat=0.5 hid=1.5 alpha=1.0 | IC= 4.72 short=-0.36 | 156s\n",
      "[10/27] lat=1.0 hid=0.5 alpha=0.6 | IC= 4.45 short=-0.07 | 136s\n",
      "[11/27] lat=1.0 hid=0.5 alpha=0.8 | IC= 4.13 short=-0.79 | 136s\n",
      "[12/27] lat=1.0 hid=0.5 alpha=1.0 | IC= 4.14 short=-0.40 | 138s\n",
      "[13/27] lat=1.0 hid=1.0 alpha=0.6 | IC= 4.41 short=-0.50 | 149s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 34\u001b[0m\n\u001b[1;32m     16\u001b[0m config \u001b[38;5;241m=\u001b[39m ModelConfig(\n\u001b[1;32m     17\u001b[0m     n_fundamental_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(fund_feat_cols),\n\u001b[1;32m     18\u001b[0m     n_price_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(price_feat_cols),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     news_dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m start \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m---> 34\u001b[0m metrics, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_df_news\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m (datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start)\u001b[38;5;241m.\u001b[39mtotal_seconds()\n\u001b[1;32m     37\u001b[0m arch_results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatent_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m: latent_scale,\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m: hidden_scale,\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnews_alpha\u001b[39m\u001b[38;5;124m\"\u001b[39m: news_alpha,\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetrics,\n\u001b[1;32m     42\u001b[0m })\n",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(config, train_dataset, val_df_news, n_epochs, verbose)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m     20\u001b[0m     train_dataset\u001b[38;5;241m.\u001b[39mresample_pairs()\n\u001b[0;32m---> 21\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Evaluate every 5 epochs\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, device, label_smoothing)\u001b[0m\n\u001b[1;32m      8\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      9\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m     12\u001b[0m     price_i \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_i\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m     price_j \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_j\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    738\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:788\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    787\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 788\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    790\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[6], line 66\u001b[0m, in \u001b[0;36mSinglePairDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     62\u001b[0m     emb_i, emb_j \u001b[38;5;241m=\u001b[39m emb_j, emb_i\n\u001b[1;32m     63\u001b[0m     actual_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m actual_label\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_i\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprice_i\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_j\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(price_j),\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfund_i\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(fund_i), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfund_j\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(fund_j),\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memb_i\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(emb_i), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memb_j\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(emb_j),\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(actual_label),\n\u001b[1;32m     70\u001b[0m }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"STAGE 1: ARCHITECTURE SEARCH\")\n",
    "print(\"Selection metric: IC Sharpe (robust to outliers)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "arch_configs = list(product(\n",
    "    SEARCH_SPACE[\"latent_scale\"],\n",
    "    SEARCH_SPACE[\"hidden_scale\"],\n",
    "    SEARCH_SPACE[\"news_alpha\"],\n",
    "))\n",
    "\n",
    "arch_results = []\n",
    "best_ic = -float('inf')\n",
    "best_arch = None\n",
    "\n",
    "for i, (latent_scale, hidden_scale, news_alpha) in enumerate(arch_configs):\n",
    "    config = ModelConfig(\n",
    "        n_fundamental_features=len(fund_feat_cols),\n",
    "        n_price_features=len(price_feat_cols),\n",
    "        n_embedding_dim=len(emb_cols),\n",
    "        fund_hidden=int(64 * hidden_scale),\n",
    "        price_hidden=int(32 * hidden_scale),\n",
    "        news_hidden=int(128 * hidden_scale),\n",
    "        fundamental_latent=int(32 * latent_scale),\n",
    "        price_latent=int(16 * latent_scale),\n",
    "        news_latent=int(32 * latent_scale),\n",
    "        news_alpha=news_alpha,\n",
    "        # Default dropout and training\n",
    "        fundamental_dropout=0.8,\n",
    "        price_dropout=0.4,\n",
    "        news_dropout=0.2,\n",
    "    )\n",
    "    \n",
    "    start = datetime.now()\n",
    "    metrics, _ = train_and_evaluate(config, train_dataset, val_df_news, n_epochs=15)\n",
    "    elapsed = (datetime.now() - start).total_seconds()\n",
    "    \n",
    "    arch_results.append({\n",
    "        \"latent_scale\": latent_scale,\n",
    "        \"hidden_scale\": hidden_scale,\n",
    "        \"news_alpha\": news_alpha,\n",
    "        **metrics,\n",
    "    })\n",
    "    \n",
    "    if metrics[\"ic_sharpe\"] > best_ic:\n",
    "        best_ic = metrics[\"ic_sharpe\"]\n",
    "        best_arch = (latent_scale, hidden_scale, news_alpha)\n",
    "    \n",
    "    print(f\"[{i+1:2d}/{len(arch_configs)}] lat={latent_scale:.1f} hid={hidden_scale:.1f} alpha={news_alpha:.1f} | \"\n",
    "          f\"IC={metrics['ic_sharpe']:5.2f} short={metrics['short_sharpe']:5.2f} | {elapsed:.0f}s\")\n",
    "\n",
    "print(f\"\\nBest architecture: latent={best_arch[0]}, hidden={best_arch[1]}, alpha={best_arch[2]} -> IC={best_ic:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_df = pd.DataFrame(arch_results).sort_values(\"ic_sharpe\", ascending=False)\n",
    "print(\"\\nTop 10 by IC Sharpe:\")\n",
    "print(arch_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stage 2: Training Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTAGE 2: TRAINING HYPERPARAMETER SEARCH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_latent, best_hidden, best_alpha = best_arch\n",
    "\n",
    "train_configs = list(product(\n",
    "    SEARCH_SPACE[\"learning_rate\"],\n",
    "    SEARCH_SPACE[\"weight_decay\"],\n",
    "    SEARCH_SPACE[\"label_smoothing\"],\n",
    "))\n",
    "\n",
    "train_results = []\n",
    "best_train_ic = -float('inf')\n",
    "best_train = None\n",
    "\n",
    "for i, (lr, wd, smoothing) in enumerate(train_configs):\n",
    "    config = ModelConfig(\n",
    "        n_fundamental_features=len(fund_feat_cols),\n",
    "        n_price_features=len(price_feat_cols),\n",
    "        n_embedding_dim=len(emb_cols),\n",
    "        # Best architecture\n",
    "        fund_hidden=int(64 * best_hidden),\n",
    "        price_hidden=int(32 * best_hidden),\n",
    "        news_hidden=int(128 * best_hidden),\n",
    "        fundamental_latent=int(32 * best_latent),\n",
    "        price_latent=int(16 * best_latent),\n",
    "        news_latent=int(32 * best_latent),\n",
    "        news_alpha=best_alpha,\n",
    "        # Default dropout\n",
    "        fundamental_dropout=0.5,\n",
    "        price_dropout=0.3,\n",
    "        news_dropout=0.2,\n",
    "        # Tuned training\n",
    "        learning_rate=lr,\n",
    "        weight_decay=wd,\n",
    "        label_smoothing=smoothing,\n",
    "    )\n",
    "    \n",
    "    start = datetime.now()\n",
    "    metrics, _ = train_and_evaluate(config, train_dataset, val_df_news, n_epochs=15)\n",
    "    elapsed = (datetime.now() - start).total_seconds()\n",
    "    \n",
    "    train_results.append({\n",
    "        \"learning_rate\": lr,\n",
    "        \"weight_decay\": wd,\n",
    "        \"label_smoothing\": smoothing,\n",
    "        **metrics,\n",
    "    })\n",
    "    \n",
    "    if metrics[\"ic_sharpe\"] > best_train_ic:\n",
    "        best_train_ic = metrics[\"ic_sharpe\"]\n",
    "        best_train = (lr, wd, smoothing)\n",
    "    \n",
    "    print(f\"[{i+1:2d}/{len(train_configs)}] lr={lr:.0e} wd={wd:.0e} smooth={smoothing:.2f} | \"\n",
    "          f\"IC={metrics['ic_sharpe']:5.2f} short={metrics['short_sharpe']:5.2f} | {elapsed:.0f}s\")\n",
    "\n",
    "print(f\"\\nBest training: lr={best_train[0]:.0e}, wd={best_train[1]:.0e}, smooth={best_train[2]} -> IC={best_train_ic:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_results = pd.DataFrame(train_results).sort_values(\"ic_sharpe\", ascending=False)\n",
    "print(\"\\nTop 10 by IC Sharpe:\")\n",
    "print(train_df_results.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stage 3: Dropout Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSTAGE 3: DROPOUT SEARCH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_lr, best_wd, best_smooth = best_train\n",
    "\n",
    "dropout_configs = list(product(\n",
    "    SEARCH_SPACE[\"fund_dropout\"],\n",
    "    SEARCH_SPACE[\"price_dropout\"],\n",
    "    SEARCH_SPACE[\"news_dropout\"],\n",
    "))\n",
    "\n",
    "dropout_results = []\n",
    "best_dropout_ic = -float('inf')\n",
    "best_dropout = None\n",
    "best_model = None\n",
    "\n",
    "for i, (fund_do, price_do, news_do) in enumerate(dropout_configs):\n",
    "    config = ModelConfig(\n",
    "        n_fundamental_features=len(fund_feat_cols),\n",
    "        n_price_features=len(price_feat_cols),\n",
    "        n_embedding_dim=len(emb_cols),\n",
    "        # Best architecture\n",
    "        fund_hidden=int(64 * best_hidden),\n",
    "        price_hidden=int(32 * best_hidden),\n",
    "        news_hidden=int(128 * best_hidden),\n",
    "        fundamental_latent=int(32 * best_latent),\n",
    "        price_latent=int(16 * best_latent),\n",
    "        news_latent=int(32 * best_latent),\n",
    "        news_alpha=best_alpha,\n",
    "        # Tuned dropout\n",
    "        fundamental_dropout=fund_do,\n",
    "        price_dropout=price_do,\n",
    "        news_dropout=news_do,\n",
    "        # Best training\n",
    "        learning_rate=best_lr,\n",
    "        weight_decay=best_wd,\n",
    "        label_smoothing=best_smooth,\n",
    "    )\n",
    "    \n",
    "    start = datetime.now()\n",
    "    metrics, model = train_and_evaluate(config, train_dataset, val_df_news, n_epochs=15)\n",
    "    elapsed = (datetime.now() - start).total_seconds()\n",
    "    \n",
    "    dropout_results.append({\n",
    "        \"fund_dropout\": fund_do,\n",
    "        \"price_dropout\": price_do,\n",
    "        \"news_dropout\": news_do,\n",
    "        **metrics,\n",
    "    })\n",
    "    \n",
    "    if metrics[\"ic_sharpe\"] > best_dropout_ic:\n",
    "        best_dropout_ic = metrics[\"ic_sharpe\"]\n",
    "        best_dropout = (fund_do, price_do, news_do)\n",
    "        best_model = model\n",
    "    \n",
    "    print(f\"[{i+1:2d}/{len(dropout_configs)}] fund={fund_do:.1f} price={price_do:.1f} news={news_do:.1f} | \"\n",
    "          f\"IC={metrics['ic_sharpe']:5.2f} short={metrics['short_sharpe']:5.2f} | {elapsed:.0f}s\")\n",
    "\n",
    "print(f\"\\nBest dropout: fund={best_dropout[0]}, price={best_dropout[1]}, news={best_dropout[2]} -> IC={best_dropout_ic:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_df = pd.DataFrame(dropout_results).sort_values(\"ic_sharpe\", ascending=False)\n",
    "print(\"\\nTop 10 by IC Sharpe:\")\n",
    "print(dropout_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTRAINING FINAL MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "best_fund_do, best_price_do, best_news_do = best_dropout\n",
    "\n",
    "final_config = ModelConfig(\n",
    "    n_fundamental_features=len(fund_feat_cols),\n",
    "    n_price_features=len(price_feat_cols),\n",
    "    n_embedding_dim=len(emb_cols),\n",
    "    # Best architecture\n",
    "    fund_hidden=int(64 * best_hidden),\n",
    "    price_hidden=int(32 * best_hidden),\n",
    "    news_hidden=int(128 * best_hidden),\n",
    "    fundamental_latent=int(32 * best_latent),\n",
    "    price_latent=int(16 * best_latent),\n",
    "    news_latent=int(32 * best_latent),\n",
    "    news_alpha=best_alpha,\n",
    "    # Best dropout\n",
    "    fundamental_dropout=best_fund_do,\n",
    "    price_dropout=best_price_do,\n",
    "    news_dropout=best_news_do,\n",
    "    # Best training\n",
    "    learning_rate=best_lr,\n",
    "    weight_decay=best_wd,\n",
    "    label_smoothing=best_smooth,\n",
    "    n_epochs=25,\n",
    ")\n",
    "\n",
    "print(\"Final configuration:\")\n",
    "print(f\"  Architecture: hidden=({final_config.fund_hidden}, {final_config.price_hidden}, {final_config.news_hidden})\")\n",
    "print(f\"  Latent dims:  ({final_config.fundamental_latent}, {final_config.price_latent}, {final_config.news_latent})\")\n",
    "print(f\"  News alpha:   {final_config.news_alpha}\")\n",
    "print(f\"  Dropout:      ({final_config.fundamental_dropout}, {final_config.price_dropout}, {final_config.news_dropout})\")\n",
    "print(f\"  Training:     lr={final_config.learning_rate:.0e}, wd={final_config.weight_decay:.0e}, smooth={final_config.label_smoothing}\")\n",
    "print()\n",
    "\n",
    "final_metrics, final_model = train_and_evaluate(final_config, train_dataset, val_df_news, n_epochs=25, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_metrics = evaluate_model(final_model, test_df_news, price_feat_cols, fund_feat_cols, emb_cols, device)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL MODEL - TEST SET RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mean IC:       {test_metrics['mean_ic']:.4f}\")\n",
    "print(f\"IC Sharpe:     {test_metrics['ic_sharpe']:.2f}\")\n",
    "print(f\"Short Sharpe:  {test_metrics['short_sharpe']:.2f}  (for reference only, sensitive to outliers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Architecture search\n",
    "ax = axes[0]\n",
    "arch_df_sorted = arch_df.sort_values(\"ic_sharpe\", ascending=True).tail(10)\n",
    "labels = [f\"l={r['latent_scale']:.1f} h={r['hidden_scale']:.1f}\" for _, r in arch_df_sorted.iterrows()]\n",
    "ax.barh(labels, arch_df_sorted[\"ic_sharpe\"], color='steelblue')\n",
    "ax.set_xlabel('IC Sharpe')\n",
    "ax.set_title('Stage 1: Architecture')\n",
    "\n",
    "# Training search\n",
    "ax = axes[1]\n",
    "train_sorted = train_df_results.sort_values(\"ic_sharpe\", ascending=True).tail(10)\n",
    "labels = [f\"lr={r['learning_rate']:.0e}\" for _, r in train_sorted.iterrows()]\n",
    "ax.barh(labels, train_sorted[\"ic_sharpe\"], color='coral')\n",
    "ax.set_xlabel('IC Sharpe')\n",
    "ax.set_title('Stage 2: Training')\n",
    "\n",
    "# Dropout search\n",
    "ax = axes[2]\n",
    "drop_sorted = dropout_df.sort_values(\"ic_sharpe\", ascending=True).tail(10)\n",
    "labels = [f\"f={r['fund_dropout']:.1f} p={r['price_dropout']:.1f}\" for _, r in drop_sorted.iterrows()]\n",
    "ax.barh(labels, drop_sorted[\"ic_sharpe\"], color='green')\n",
    "ax.set_xlabel('IC Sharpe')\n",
    "ax.set_title('Stage 3: Dropout')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IC Sharpe vs Short Sharpe scatter\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "all_results = pd.concat([\n",
    "    arch_df.assign(stage='Architecture'),\n",
    "    train_df_results.assign(stage='Training'),\n",
    "    dropout_df.assign(stage='Dropout'),\n",
    "])\n",
    "\n",
    "for stage, color in [('Architecture', 'steelblue'), ('Training', 'coral'), ('Dropout', 'green')]:\n",
    "    data = all_results[all_results['stage'] == stage]\n",
    "    ax.scatter(data['ic_sharpe'], data['short_sharpe'], label=stage, alpha=0.6, s=50, c=color)\n",
    "\n",
    "ax.set_xlabel('IC Sharpe (selection metric)')\n",
    "ax.set_ylabel('Short Sharpe (reporting only)')\n",
    "ax.set_title('IC Sharpe vs Short Sharpe Across All Experiments')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add correlation\n",
    "corr = all_results[['ic_sharpe', 'short_sharpe']].corr().iloc[0, 1]\n",
    "ax.text(0.05, 0.95, f'Correlation: {corr:.2f}', transform=ax.transAxes, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save search results\n",
    "pd.DataFrame(arch_results).to_parquet(\"data/hyperparam_arch_results.pqt\")\n",
    "pd.DataFrame(train_results).to_parquet(\"data/hyperparam_train_results.pqt\")\n",
    "pd.DataFrame(dropout_results).to_parquet(\"data/hyperparam_dropout_results.pqt\")\n",
    "print(\"Saved search results to data/hyperparam_*.pqt\")\n",
    "\n",
    "# Save best model\n",
    "torch.save({\n",
    "    \"model_state_dict\": final_model.state_dict(),\n",
    "    \"config\": final_config,\n",
    "    \"price_cols\": price_feat_cols,\n",
    "    \"fund_cols\": fund_feat_cols,\n",
    "    \"emb_cols\": emb_cols,\n",
    "    \"search_results\": {\n",
    "        \"best_arch\": {\"latent_scale\": best_latent, \"hidden_scale\": best_hidden, \"news_alpha\": best_alpha},\n",
    "        \"best_train\": {\"lr\": best_lr, \"wd\": best_wd, \"smoothing\": best_smooth},\n",
    "        \"best_dropout\": {\"fund\": best_fund_do, \"price\": best_price_do, \"news\": best_news_do},\n",
    "    },\n",
    "    \"test_metrics\": test_metrics,\n",
    "}, \"data/model_optimized.pt\")\n",
    "\n",
    "print(\"Saved model to data/model_optimized.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OPTIMIZATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nSelection metric: IC Sharpe (robust to outliers)\")\n",
    "print(f\"\\nBest configuration:\")\n",
    "print(f\"  Architecture: latent_scale={best_latent}, hidden_scale={best_hidden}, alpha={best_alpha}\")\n",
    "print(f\"  Training:     lr={best_lr:.0e}, wd={best_wd:.0e}, smooth={best_smooth}\")\n",
    "print(f\"  Dropout:      fund={best_fund_do}, price={best_price_do}, news={best_news_do}\")\n",
    "print(f\"\\nTest set performance:\")\n",
    "print(f\"  IC Sharpe:    {test_metrics['ic_sharpe']:.2f}\")\n",
    "print(f\"  Short Sharpe: {test_metrics['short_sharpe']:.2f} (sensitive to outliers)\")\n",
    "print(f\"  Mean IC:      {test_metrics['mean_ic']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
