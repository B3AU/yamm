{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0000-0001-0001-000000000000",
   "metadata": {},
   "source": [
    "# Dataset Construction\n",
    "\n",
    "Merges price features, fundamentals, and news embeddings into final ML dataset.\n",
    "\n",
    "**Steps**:\n",
    "1. Load price features (base table)\n",
    "2. Apply liquidity filter ($5 price, $10M avg volume)\n",
    "3. Point-in-time join fundamentals (using actual SEC filing dates)\n",
    "4. Join news embeddings by trading_date\n",
    "5. Cross-sectional normalize all features\n",
    "6. Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1b2c3d4-0001-0001-0001-000000000001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1b2c3d4-0001-0001-0001-000000000002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "MIN_PRICE = 5.0\n",
    "MIN_DOLLAR_VOL = 10_000_000  # $10M\n",
    "FALLBACK_LAG_DAYS = 45  # Used only if filing_date unavailable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000003",
   "metadata": {},
   "source": [
    "## 1. Load price features (base table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1b2c3d4-0001-0001-0001-000000000004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price features: 5,530,540 rows, 5,547 symbols\n",
      "Date range: 2020-12-30 to 2025-12-18\n"
     ]
    }
   ],
   "source": [
    "prices = pd.read_parquet(\"data/price_features.pqt\")\n",
    "prices[\"feature_date\"] = pd.to_datetime(prices[\"feature_date\"])\n",
    "prices[\"target_date\"] = pd.to_datetime(prices[\"target_date\"])\n",
    "\n",
    "print(f\"Price features: {len(prices):,} rows, {prices['symbol'].nunique():,} symbols\")\n",
    "print(f\"Date range: {prices['feature_date'].min().date()} to {prices['feature_date'].max().date()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000005",
   "metadata": {},
   "source": [
    "## 2. Apply liquidity filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b2c3d4-0001-0001-0001-000000000006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dollar vol stats:\n",
      "count    5.480681e+06\n",
      "mean     4.792787e+15\n",
      "std      2.508936e+18\n",
      "min      0.000000e+00\n",
      "25%      5.080269e+05\n",
      "50%      4.226939e+06\n",
      "75%      3.206292e+07\n",
      "max      1.313387e+21\n",
      "Name: avg_dollar_vol_20d, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Compute dollar volume\n",
    "prices[\"dollar_vol\"] = prices[\"close\"] * prices[\"volume\"]\n",
    "\n",
    "# 20-day rolling average dollar volume (per symbol)\n",
    "prices = prices.sort_values([\"symbol\", \"feature_date\"])\n",
    "prices[\"avg_dollar_vol_20d\"] = prices.groupby(\"symbol\")[\"dollar_vol\"].transform(\n",
    "    lambda x: x.rolling(20, min_periods=10).mean()\n",
    ")\n",
    "\n",
    "print(f\"Dollar vol stats:\")\n",
    "print(prices[\"avg_dollar_vol_20d\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b2c3d4-0001-0001-0001-000000000007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liquidity filter: 5,530,540 -> 2,092,929 (37.8%)\n",
      "Symbols remaining: 3,506\n"
     ]
    }
   ],
   "source": [
    "# Apply filters\n",
    "n_before = len(prices)\n",
    "mask = (prices[\"close\"] >= MIN_PRICE) & (prices[\"avg_dollar_vol_20d\"] >= MIN_DOLLAR_VOL)\n",
    "prices = prices[mask].copy()\n",
    "n_after = len(prices)\n",
    "\n",
    "print(f\"Liquidity filter: {n_before:,} -> {n_after:,} ({n_after/n_before*100:.1f}%)\")\n",
    "print(f\"Symbols remaining: {prices['symbol'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1b2c3d4-0001-0001-0001-000000000008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop helper columns\n",
    "prices = prices.drop(columns=[\"dollar_vol\", \"avg_dollar_vol_20d\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000009",
   "metadata": {},
   "source": [
    "## 3. Load and prepare fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1b2c3d4-0001-0001-0001-000000000010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: 307,009 rows\n",
      "Ratios: 307,009 rows\n",
      "Growth: 307,009 rows\n",
      "Filing dates: 305,371 rows\n"
     ]
    }
   ],
   "source": [
    "# Load all fundamental data\n",
    "metrics = pd.read_parquet(\"data/key_metrics.pqt\")\n",
    "ratios = pd.read_parquet(\"data/ratios.pqt\")\n",
    "growth = pd.read_parquet(\"data/growth.pqt\")\n",
    "\n",
    "# Load actual SEC filing dates\n",
    "filing_dates = pd.read_parquet(\"data/filing_dates.pqt\")\n",
    "\n",
    "print(f\"Metrics: {len(metrics):,} rows\")\n",
    "print(f\"Ratios: {len(ratios):,} rows\")\n",
    "print(f\"Growth: {len(growth):,} rows\")\n",
    "print(f\"Filing dates: {len(filing_dates):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b2c3d4-0001-0001-0001-000000000011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features (~15-20 focused features)\n",
    "METRIC_COLS = [\n",
    "    \"evToEBITDA\",           # Value\n",
    "    \"freeCashFlowYield\",    # Value\n",
    "    \"earningsYield\",        # Value\n",
    "    \"returnOnEquity\",       # Quality\n",
    "    \"returnOnAssets\",       # Quality\n",
    "    \"returnOnInvestedCapital\",  # Quality\n",
    "    \"currentRatio\",         # Liquidity\n",
    "]\n",
    "\n",
    "RATIO_COLS = [\n",
    "    \"priceToEarningsRatio\",  # Value (P/E)\n",
    "    \"priceToBookRatio\",      # Value\n",
    "    \"priceToSalesRatio\",     # Value\n",
    "    \"grossProfitMargin\",     # Quality\n",
    "    \"operatingProfitMargin\", # Quality\n",
    "    \"netProfitMargin\",       # Quality\n",
    "    \"debtToEquityRatio\",     # Leverage\n",
    "    \"debtToAssetsRatio\",     # Leverage\n",
    "]\n",
    "\n",
    "GROWTH_COLS = [\n",
    "    \"revenueGrowth\",         # Growth\n",
    "    \"netIncomeGrowth\",       # Growth\n",
    "    \"epsgrowth\",             # Growth\n",
    "    \"operatingIncomeGrowth\", # Growth\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1b2c3d4-0001-0001-0001-000000000012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined fundamentals: 307,481 rows\n",
      "Symbols: 5,564\n"
     ]
    }
   ],
   "source": [
    "# Merge fundamentals into single table\n",
    "metrics_sub = metrics[[\"symbol\", \"date\"] + METRIC_COLS].copy()\n",
    "ratios_sub = ratios[[\"symbol\", \"date\"] + RATIO_COLS].copy()\n",
    "growth_sub = growth[[\"symbol\", \"date\"] + GROWTH_COLS].copy()\n",
    "\n",
    "# Merge on symbol + date\n",
    "fundamentals = metrics_sub.merge(ratios_sub, on=[\"symbol\", \"date\"], how=\"outer\")\n",
    "fundamentals = fundamentals.merge(growth_sub, on=[\"symbol\", \"date\"], how=\"outer\")\n",
    "\n",
    "print(f\"Combined fundamentals: {len(fundamentals):,} rows\")\n",
    "print(f\"Symbols: {fundamentals['symbol'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1b2c3d4-0001-0001-0001-000000000013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filing date coverage: 304,183/307,532 (98.9%)\n",
      "\n",
      "Example available_date (with actual filing dates):\n",
      "  symbol period_end filing_date available_date\n",
      "0      A 2001-01-31  2001-03-19     2001-03-19\n",
      "1      A 2001-04-30  2001-06-14     2001-06-14\n",
      "2      A 2001-07-31  2001-09-14     2001-09-14\n",
      "3      A 2001-10-31  2002-01-22     2002-01-22\n",
      "4      A 2002-01-31  2002-03-06     2002-03-06\n",
      "5      A 2002-04-30  2002-06-05     2002-06-05\n",
      "6      A 2002-07-31  2002-09-13     2002-09-13\n",
      "7      A 2002-10-31  2002-12-20     2002-12-20\n",
      "8      A 2003-01-31  2003-03-12     2003-03-12\n",
      "9      A 2003-04-30  2003-06-04     2003-06-04\n"
     ]
    }
   ],
   "source": [
    "# Join filing dates to fundamentals for point-in-time alignment\n",
    "fundamentals[\"period_end\"] = pd.to_datetime(fundamentals[\"date\"])\n",
    "\n",
    "# Prepare filing dates for join\n",
    "filing_dates_clean = filing_dates[[\"symbol\", \"period_end\", \"filing_date\"]].copy()\n",
    "filing_dates_clean[\"period_end\"] = pd.to_datetime(filing_dates_clean[\"period_end\"])\n",
    "filing_dates_clean[\"filing_date\"] = pd.to_datetime(filing_dates_clean[\"filing_date\"])\n",
    "\n",
    "# Join filing dates to fundamentals\n",
    "fundamentals = fundamentals.merge(\n",
    "    filing_dates_clean,\n",
    "    on=[\"symbol\", \"period_end\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Use actual filing_date where available, fallback to period_end + 45 days\n",
    "fundamentals[\"available_date\"] = fundamentals[\"filing_date\"].fillna(\n",
    "    fundamentals[\"period_end\"] + timedelta(days=FALLBACK_LAG_DAYS)\n",
    ")\n",
    "\n",
    "# Stats on filing date coverage\n",
    "n_with_filing = fundamentals[\"filing_date\"].notna().sum()\n",
    "n_total = len(fundamentals)\n",
    "print(f\"Filing date coverage: {n_with_filing:,}/{n_total:,} ({n_with_filing/n_total*100:.1f}%)\")\n",
    "\n",
    "# Sort for asof merge\n",
    "fundamentals = fundamentals.sort_values([\"symbol\", \"available_date\"])\n",
    "\n",
    "print(f\"\\nExample available_date (with actual filing dates):\")\n",
    "print(fundamentals[[\"symbol\", \"period_end\", \"filing_date\", \"available_date\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1b2c3d4-0001-0001-0001-000000000014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point-in-time join: for each (symbol, feature_date), get most recent fundamental\n",
    "# where available_date <= feature_date\n",
    "\n",
    "fund_cols = METRIC_COLS + RATIO_COLS + GROWTH_COLS\n",
    "\n",
    "def pit_join_fundamentals(prices_df: pd.DataFrame, fund_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Point-in-time join fundamentals to prices using vectorized merge_asof.\"\"\"\n",
    "    # merge_asof requires sorting by the merge keys (left_on, right_on)\n",
    "    prices_sorted = prices_df.sort_values(\"feature_date\")\n",
    "    fund_sorted = fund_df.sort_values(\"available_date\")\n",
    "    \n",
    "    # Single vectorized merge_asof with by='symbol'\n",
    "    merged = pd.merge_asof(\n",
    "        prices_sorted,\n",
    "        fund_sorted[[\"symbol\", \"available_date\"] + fund_cols],\n",
    "        left_on=\"feature_date\",\n",
    "        right_on=\"available_date\",\n",
    "        by=\"symbol\",\n",
    "        direction=\"backward\"\n",
    "    )\n",
    "    \n",
    "    # Flag for whether we have fundamentals\n",
    "    merged[\"has_fundamentals\"] = merged[fund_cols[0]].notna().astype(int)\n",
    "    merged = merged.drop(columns=[\"available_date\"])\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1b2c3d4-0001-0001-0001-000000000015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining fundamentals (point-in-time)...\n",
      "After fundamental join: 2,092,929 rows\n",
      "Has fundamentals: 2,088,521 (99.8%)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# This can be slow, show progress\n",
    "print(\"Joining fundamentals (point-in-time)...\")\n",
    "df = pit_join_fundamentals(prices, fundamentals)\n",
    "\n",
    "print(f\"After fundamental join: {len(df):,} rows\")\n",
    "print(f\"Has fundamentals: {df['has_fundamentals'].sum():,} ({df['has_fundamentals'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000016",
   "metadata": {},
   "source": [
    "## 4. Load and join news embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1b2c3d4-0001-0001-0001-000000000017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: 1,748,149 rows\n",
      "News: 1,747,711 rows\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings and news (for trading_date)\n",
    "embeddings = pd.read_parquet(\"data/news_embeddings.pqt\")\n",
    "news = pd.read_parquet(\"data/all_the_news_anon.pqt\")\n",
    "\n",
    "print(f\"Embeddings: {len(embeddings):,} rows\")\n",
    "print(f\"News: {len(news):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1b2c3d4-0001-0001-0001-000000000018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings with trading_date: 1,747,711\n"
     ]
    }
   ],
   "source": [
    "# Get trading_date from news\n",
    "news_meta = news[[\"url\", \"symbol\", \"trading_date\"]].copy()\n",
    "news_meta[\"trading_date\"] = pd.to_datetime(news_meta[\"trading_date\"])\n",
    "\n",
    "# Join embeddings with trading_date\n",
    "emb_with_date = embeddings.merge(news_meta, on=[\"url\", \"symbol\"], how=\"inner\")\n",
    "print(f\"Embeddings with trading_date: {len(emb_with_date):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1b2c3d4-0001-0001-0001-000000000019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# Identify embedding columns\n",
    "emb_cols = [c for c in embeddings.columns if c.startswith(\"emb_\")]\n",
    "print(f\"Embedding dimension: {len(emb_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1b2c3d4-0001-0001-0001-000000000020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated embeddings: 858,503 (symbol, trading_date) pairs\n"
     ]
    }
   ],
   "source": [
    "# Aggregate: mean embedding + count per (symbol, trading_date)\n",
    "emb_agg = emb_with_date.groupby([\"symbol\", \"trading_date\"]).agg(\n",
    "    **{col: (col, \"mean\") for col in emb_cols},\n",
    "    news_count=(\"url\", \"count\")\n",
    ").reset_index()\n",
    "\n",
    "print(f\"Aggregated embeddings: {len(emb_agg):,} (symbol, trading_date) pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1b2c3d4-0001-0001-0001-000000000021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After news join: 2,092,929 rows\n",
      "Rows with news: 527,256 (25.2%)\n"
     ]
    }
   ],
   "source": [
    "# Join embeddings to dataset\n",
    "df = df.merge(\n",
    "    emb_agg,\n",
    "    left_on=[\"symbol\", \"feature_date\"],\n",
    "    right_on=[\"symbol\", \"trading_date\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "df = df.drop(columns=[\"trading_date\"], errors=\"ignore\")\n",
    "\n",
    "# Fill missing news\n",
    "df[\"news_count\"] = df[\"news_count\"].fillna(0).astype(int)\n",
    "df[emb_cols] = df[emb_cols].fillna(0)\n",
    "\n",
    "print(f\"After news join: {len(df):,} rows\")\n",
    "print(f\"Rows with news: {(df['news_count'] > 0).sum():,} ({(df['news_count'] > 0).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000022",
   "metadata": {},
   "source": [
    "## 5. Cross-sectional normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1b2c3d4-0001-0001-0001-000000000023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_sectional_zscore(df: pd.DataFrame, col: str, clip: float = 3.0) -> pd.Series:\n",
    "    \"\"\"Z-score within each date, with winsorization.\"\"\"\n",
    "    grouped = df.groupby(\"feature_date\")[col]\n",
    "    mean = grouped.transform(\"mean\")\n",
    "    std = grouped.transform(\"std\")\n",
    "    z = (df[col] - mean) / std\n",
    "    return z.clip(-clip, clip)\n",
    "\n",
    "def fill_with_median(df: pd.DataFrame, col: str) -> pd.Series:\n",
    "    \"\"\"Fill NaN with cross-sectional median.\"\"\"\n",
    "    median = df.groupby(\"feature_date\")[col].transform(\"median\")\n",
    "    return df[col].fillna(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1b2c3d4-0001-0001-0001-000000000024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1729466/1600563521.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_z\"] = cross_sectional_zscore(df, col)\n",
      "/tmp/ipykernel_1729466/1600563521.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_z\"] = cross_sectional_zscore(df, col)\n",
      "/tmp/ipykernel_1729466/1600563521.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_z\"] = cross_sectional_zscore(df, col)\n",
      "/tmp/ipykernel_1729466/1600563521.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_z\"] = cross_sectional_zscore(df, col)\n",
      "/tmp/ipykernel_1729466/1600563521.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_z\"] = cross_sectional_zscore(df, col)\n",
      "/tmp/ipykernel_1729466/1600563521.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_z\"] = cross_sectional_zscore(df, col)\n",
      "/tmp/ipykernel_1729466/1600563521.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_z\"] = cross_sectional_zscore(df, col)\n",
      "/tmp/ipykernel_1729466/1600563521.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_z\"] = cross_sectional_zscore(df, col)\n",
      "/tmp/ipykernel_1729466/1600563521.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_z\"] = cross_sectional_zscore(df, col)\n",
      "/tmp/ipykernel_1729466/1600563521.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_z\"] = cross_sectional_zscore(df, col)\n",
      "/tmp/ipykernel_1729466/1600563521.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_z\"] = cross_sectional_zscore(df, col)\n",
      "/tmp/ipykernel_1729466/1600563521.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_z\"] = cross_sectional_zscore(df, col)\n",
      "/tmp/ipykernel_1729466/1600563521.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_z\"] = cross_sectional_zscore(df, col)\n",
      "/tmp/ipykernel_1729466/1600563521.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_z\"] = cross_sectional_zscore(df, col)\n",
      "/tmp/ipykernel_1729466/1600563521.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_z\"] = cross_sectional_zscore(df, col)\n",
      "/tmp/ipykernel_1729466/1600563521.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_z\"] = cross_sectional_zscore(df, col)\n",
      "/tmp/ipykernel_1729466/1600563521.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_z\"] = cross_sectional_zscore(df, col)\n",
      "/tmp/ipykernel_1729466/1600563521.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_z\"] = cross_sectional_zscore(df, col)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized 19 fundamental features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1729466/1600563521.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f\"{col}_z\"] = cross_sectional_zscore(df, col)\n"
     ]
    }
   ],
   "source": [
    "# Normalize fundamental features\n",
    "for col in fund_cols:\n",
    "    # Fill missing with median first\n",
    "    df[col] = fill_with_median(df, col)\n",
    "    # Then z-score\n",
    "    df[f\"{col}_z\"] = cross_sectional_zscore(df, col)\n",
    "\n",
    "fund_cols_z = [f\"{col}_z\" for col in fund_cols]\n",
    "print(f\"Normalized {len(fund_cols)} fundamental features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1b2c3d4-0001-0001-0001-000000000025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News count stats (z-scored):\n",
      "count    2.092929e+06\n",
      "mean    -4.704306e-02\n",
      "std      6.030708e-01\n",
      "min     -4.562799e-01\n",
      "25%     -3.225289e-01\n",
      "50%     -2.880563e-01\n",
      "75%     -1.153238e-02\n",
      "max      3.000000e+00\n",
      "Name: news_count_z, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1729466/1465116152.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[\"news_count_z\"] = cross_sectional_zscore(df, \"news_count\")\n"
     ]
    }
   ],
   "source": [
    "# Normalize news_count\n",
    "df[\"news_count_z\"] = cross_sectional_zscore(df, \"news_count\")\n",
    "\n",
    "print(\"News count stats (z-scored):\")\n",
    "print(df[\"news_count_z\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-0001-0001-000000000026",
   "metadata": {},
   "source": [
    "## 6. Final dataset assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1b2c3d4-0001-0001-0001-000000000027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns: 804\n"
     ]
    }
   ],
   "source": [
    "# Price feature columns (already normalized)\n",
    "price_feat_cols = [\n",
    "    \"overnight_gap_z\", \"intraday_ret_z\",\n",
    "    \"ret_1d_z\", \"ret_2d_z\", \"ret_3d_z\", \"ret_5d_z\",\n",
    "    \"vol_5d_z\", \"dist_from_high_5d_z\", \"dist_from_low_5d_z\"\n",
    "]\n",
    "\n",
    "# Assemble final columns\n",
    "id_cols = [\"symbol\", \"feature_date\", \"target_date\"]\n",
    "target_cols = [\"target_return\", \"target_demean\", \"target_rank\"]\n",
    "flag_cols = [\"has_fundamentals\"]\n",
    "\n",
    "final_cols = (\n",
    "    id_cols + \n",
    "    target_cols + \n",
    "    flag_cols +\n",
    "    price_feat_cols + \n",
    "    fund_cols_z + \n",
    "    [\"news_count_z\"] + \n",
    "    emb_cols\n",
    ")\n",
    "\n",
    "dataset = df[final_cols].copy()\n",
    "print(f\"Final columns: {len(final_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1b2c3d4-0001-0001-0001-000000000028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 0 rows with NaN\n",
      "Final dataset: 2,092,929 rows\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with any remaining NaN in features\n",
    "n_before = len(dataset)\n",
    "dataset = dataset.dropna()\n",
    "n_after = len(dataset)\n",
    "print(f\"Dropped {n_before - n_after:,} rows with NaN\")\n",
    "print(f\"Final dataset: {n_after:,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1b2c3d4-0001-0001-0001-000000000029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range: 2021-01-13 to 2025-12-18\n",
      "Symbols: 3,506\n",
      "Days: 1,186\n",
      "Avg rows per day: 1765\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(f\"Date range: {dataset['feature_date'].min().date()} to {dataset['feature_date'].max().date()}\")\n",
    "print(f\"Symbols: {dataset['symbol'].nunique():,}\")\n",
    "print(f\"Days: {dataset['feature_date'].nunique():,}\")\n",
    "print(f\"Avg rows per day: {len(dataset) / dataset['feature_date'].nunique():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1b2c3d4-0001-0001-0001-000000000030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature coverage:\n",
      "  Has fundamentals: 99.8%\n",
      "  Has news: 24.9%\n"
     ]
    }
   ],
   "source": [
    "# Feature coverage\n",
    "print(f\"\\nFeature coverage:\")\n",
    "print(f\"  Has fundamentals: {dataset['has_fundamentals'].mean()*100:.1f}%\")\n",
    "print(f\"  Has news: {(dataset['news_count_z'] > 0).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1b2c3d4-0001-0001-0001-000000000031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to data/ml_dataset.pqt\n",
      "File size: 3.82 GB\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "OUTPUT_PATH = Path(\"data/ml_dataset.pqt\")\n",
    "dataset.to_parquet(OUTPUT_PATH, index=False)\n",
    "print(f\"Saved to {OUTPUT_PATH}\")\n",
    "print(f\"File size: {OUTPUT_PATH.stat().st_size / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1b2c3d4-0001-0001-0001-000000000032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>feature_date</th>\n",
       "      <th>target_date</th>\n",
       "      <th>target_return</th>\n",
       "      <th>target_demean</th>\n",
       "      <th>target_rank</th>\n",
       "      <th>has_fundamentals</th>\n",
       "      <th>overnight_gap_z</th>\n",
       "      <th>intraday_ret_z</th>\n",
       "      <th>ret_1d_z</th>\n",
       "      <th>...</th>\n",
       "      <th>emb_758</th>\n",
       "      <th>emb_759</th>\n",
       "      <th>emb_760</th>\n",
       "      <th>emb_761</th>\n",
       "      <th>emb_762</th>\n",
       "      <th>emb_763</th>\n",
       "      <th>emb_764</th>\n",
       "      <th>emb_765</th>\n",
       "      <th>emb_766</th>\n",
       "      <th>emb_767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2021-01-13</td>\n",
       "      <td>2021-01-14</td>\n",
       "      <td>-0.007510</td>\n",
       "      <td>-0.024006</td>\n",
       "      <td>0.183731</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>-0.027192</td>\n",
       "      <td>-0.503303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OXY</td>\n",
       "      <td>2021-01-13</td>\n",
       "      <td>2021-01-14</td>\n",
       "      <td>0.082073</td>\n",
       "      <td>0.065576</td>\n",
       "      <td>0.956633</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.004014</td>\n",
       "      <td>-0.343336</td>\n",
       "      <td>2.246372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028902</td>\n",
       "      <td>-0.016045</td>\n",
       "      <td>-0.047937</td>\n",
       "      <td>-0.014188</td>\n",
       "      <td>0.052219</td>\n",
       "      <td>-0.065113</td>\n",
       "      <td>0.026437</td>\n",
       "      <td>0.007484</td>\n",
       "      <td>0.017268</td>\n",
       "      <td>-0.028285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OVV</td>\n",
       "      <td>2021-01-13</td>\n",
       "      <td>2021-01-14</td>\n",
       "      <td>0.025548</td>\n",
       "      <td>0.009051</td>\n",
       "      <td>0.756479</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.117113</td>\n",
       "      <td>-0.701539</td>\n",
       "      <td>0.653796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OUT</td>\n",
       "      <td>2021-01-13</td>\n",
       "      <td>2021-01-14</td>\n",
       "      <td>0.017049</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.644598</td>\n",
       "      <td>1</td>\n",
       "      <td>0.110191</td>\n",
       "      <td>-0.731670</td>\n",
       "      <td>-0.390871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADM</td>\n",
       "      <td>2021-01-13</td>\n",
       "      <td>2021-01-14</td>\n",
       "      <td>0.021025</td>\n",
       "      <td>0.004529</td>\n",
       "      <td>0.698229</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.155114</td>\n",
       "      <td>0.326001</td>\n",
       "      <td>-0.565920</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050260</td>\n",
       "      <td>0.005310</td>\n",
       "      <td>0.028035</td>\n",
       "      <td>-0.061282</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>-0.016311</td>\n",
       "      <td>0.016215</td>\n",
       "      <td>0.018197</td>\n",
       "      <td>0.031753</td>\n",
       "      <td>-0.022559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 804 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol feature_date target_date  target_return  target_demean  target_rank  \\\n",
       "0      A   2021-01-13  2021-01-14      -0.007510      -0.024006     0.183731   \n",
       "1    OXY   2021-01-13  2021-01-14       0.082073       0.065576     0.956633   \n",
       "2    OVV   2021-01-13  2021-01-14       0.025548       0.009051     0.756479   \n",
       "3    OUT   2021-01-13  2021-01-14       0.017049       0.000552     0.644598   \n",
       "4    ADM   2021-01-13  2021-01-14       0.021025       0.004529     0.698229   \n",
       "\n",
       "   has_fundamentals  overnight_gap_z  intraday_ret_z  ret_1d_z  ...   emb_758  \\\n",
       "0                 1         0.016528       -0.027192 -0.503303  ...  0.000000   \n",
       "1                 1        -0.004014       -0.343336  2.246372  ...  0.028902   \n",
       "2                 1        -0.117113       -0.701539  0.653796  ...  0.000000   \n",
       "3                 1         0.110191       -0.731670 -0.390871  ...  0.000000   \n",
       "4                 1        -0.155114        0.326001 -0.565920  ... -0.050260   \n",
       "\n",
       "    emb_759   emb_760   emb_761   emb_762   emb_763   emb_764   emb_765  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1 -0.016045 -0.047937 -0.014188  0.052219 -0.065113  0.026437  0.007484   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.005310  0.028035 -0.061282  0.006071 -0.016311  0.016215  0.018197   \n",
       "\n",
       "    emb_766   emb_767  \n",
       "0  0.000000  0.000000  \n",
       "1  0.017268 -0.028285  \n",
       "2  0.000000  0.000000  \n",
       "3  0.000000  0.000000  \n",
       "4  0.031753 -0.022559  \n",
       "\n",
       "[5 rows x 804 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a933ea26-d2c0-4e9a-9900-c5bb9de91135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2092929"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7df76f-0e4e-4825-abde-07d149187d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
