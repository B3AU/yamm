{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 2.4 Fundamental Regularization\n",
    "\n",
    "Address the imbalance between fundamental and news feature exposure.\n",
    "\n",
    "**Problem**: Fundamentals update quarterly but news is daily, so the model sees the same fundamental features ~60x more often than each news embedding. This may cause:\n",
    "- Overfitting to specific fundamental values\n",
    "- Underweighting news signal (each embedding is unique/\"noisy\")\n",
    "- Gradient imbalance favoring fundamentals\n",
    "\n",
    "**Solutions tested**:\n",
    "1. **Noise augmentation**: Add small noise to fundamentals during training\n",
    "2. **Higher dropout**: Increase fundamental encoder dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    # Feature dimensions\n",
    "    n_fundamental_features: int = 19\n",
    "    n_price_features: int = 9\n",
    "    n_embedding_dim: int = 768\n",
    "    \n",
    "    # Encoder latent dimensions\n",
    "    fundamental_latent: int = 32\n",
    "    price_latent: int = 16\n",
    "    news_latent: int = 32\n",
    "    \n",
    "    # Dropout - fundamental dropout is now a tunable parameter\n",
    "    fundamental_dropout: float = 0.2\n",
    "    price_dropout: float = 0.2\n",
    "    news_dropout: float = 0.3\n",
    "    \n",
    "    # News influence cap\n",
    "    news_alpha: float = 0.8\n",
    "    \n",
    "    # Training\n",
    "    batch_size: int = 512\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-3\n",
    "    n_epochs: int = 5\n",
    "    \n",
    "    # NEW: Fundamental noise augmentation\n",
    "    fundamental_noise_std: float = 0.0  # 0 = no noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 2,092,929 rows\n",
      "Date range: 2021-01-13 to 2025-12-18\n",
      "Symbols: 3,506\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"data/ml_dataset.pqt\")\n",
    "df[\"feature_date\"] = pd.to_datetime(df[\"feature_date\"])\n",
    "\n",
    "print(f\"Dataset: {len(df):,} rows\")\n",
    "print(f\"Date range: {df['feature_date'].min().date()} to {df['feature_date'].max().date()}\")\n",
    "print(f\"Symbols: {df['symbol'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price features: 9\n",
      "Fundamental features: 19\n",
      "Embedding dims: 768\n"
     ]
    }
   ],
   "source": [
    "# Feature columns\n",
    "price_feat_cols = [\n",
    "    \"overnight_gap_z\", \"intraday_ret_z\",\n",
    "    \"ret_1d_z\", \"ret_2d_z\", \"ret_3d_z\", \"ret_5d_z\",\n",
    "    \"vol_5d_z\", \"dist_from_high_5d_z\", \"dist_from_low_5d_z\"\n",
    "]\n",
    "fund_feat_cols = [c for c in df.columns if c.endswith(\"_z\") and c not in price_feat_cols and c != \"news_count_z\"]\n",
    "emb_cols = [c for c in df.columns if c.startswith(\"emb_\")]\n",
    "\n",
    "print(f\"Price features: {len(price_feat_cols)}\")\n",
    "print(f\"Fundamental features: {len(fund_feat_cols)}\")\n",
    "print(f\"Embedding dims: {len(emb_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1,418,494 rows, 830 days\n",
      "Val: 210,247 rows, 118 days\n",
      "Test: 464,188 rows, 238 days\n"
     ]
    }
   ],
   "source": [
    "# Time-based split\n",
    "dates = sorted(df[\"feature_date\"].unique())\n",
    "n_dates = len(dates)\n",
    "train_end_idx = int(n_dates * 0.7)\n",
    "val_end_idx = int(n_dates * 0.8)\n",
    "\n",
    "train_dates = set(dates[:train_end_idx])\n",
    "val_dates = set(dates[train_end_idx:val_end_idx])\n",
    "test_dates = set(dates[val_end_idx:])\n",
    "\n",
    "train_df = df[df[\"feature_date\"].isin(train_dates)].copy()\n",
    "val_df = df[df[\"feature_date\"].isin(val_dates)].copy()\n",
    "test_df = df[df[\"feature_date\"].isin(test_dates)].copy()\n",
    "\n",
    "print(f\"Train: {len(train_df):,} rows, {len(train_dates)} days\")\n",
    "print(f\"Val: {len(val_df):,} rows, {len(val_dates)} days\")\n",
    "print(f\"Test: {len(test_df):,} rows, {len(test_dates)} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 2. Analyze Fundamental Repetition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rows with news: 339,872 (24.0%)\n",
      "\n",
      "Fundamental repetition stats:\n",
      "count    2477.0\n",
      "mean        1.0\n",
      "std         0.0\n",
      "min         1.0\n",
      "25%         1.0\n",
      "50%         1.0\n",
      "75%         1.0\n",
      "max         1.0\n",
      "Name: repetition_factor, dtype: float64\n",
      "\n",
      "Top 10 symbols by repetition factor:\n",
      "        unique_fundamentals  news_days  repetition_factor\n",
      "symbol                                                   \n",
      "A                       303        303                1.0\n",
      "AA                      394        394                1.0\n",
      "AACG                      2          2                1.0\n",
      "AAL                     635        635                1.0\n",
      "AAME                      2          2                1.0\n",
      "AAOI                     37         37                1.0\n",
      "AAON                     87         87                1.0\n",
      "AAPL                    830        830                1.0\n",
      "AAT                      16         16                1.0\n",
      "AB                      182        182                1.0\n"
     ]
    }
   ],
   "source": [
    "# Filter to news-only rows (what we train on)\n",
    "has_news = (train_df[emb_cols].abs().sum(axis=1) > 0)\n",
    "train_news = train_df[has_news].copy()\n",
    "\n",
    "print(f\"Training rows with news: {len(train_news):,} ({len(train_news)/len(train_df)*100:.1f}%)\")\n",
    "\n",
    "# For each symbol, count unique fundamental \"snapshots\"\n",
    "# (fundamentals change quarterly, so should have ~4 per year)\n",
    "fund_cols_check = fund_feat_cols[:5]  # Just check a few columns\n",
    "train_news['fund_hash'] = train_news[fund_feat_cols].apply(lambda x: hash(tuple(x.round(4))), axis=1)\n",
    "\n",
    "fund_stats = train_news.groupby('symbol').agg({\n",
    "    'fund_hash': 'nunique',\n",
    "    'feature_date': 'count'\n",
    "}).rename(columns={'fund_hash': 'unique_fundamentals', 'feature_date': 'news_days'})\n",
    "\n",
    "fund_stats['repetition_factor'] = fund_stats['news_days'] / fund_stats['unique_fundamentals']\n",
    "\n",
    "print(f\"\\nFundamental repetition stats:\")\n",
    "print(fund_stats['repetition_factor'].describe())\n",
    "\n",
    "print(f\"\\nTop 10 symbols by repetition factor:\")\n",
    "print(fund_stats.nlargest(10, 'repetition_factor'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. Dataset with Noise Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedPairwiseDataset(Dataset):\n",
    "    \"\"\"Pairwise dataset with fundamental noise augmentation.\n",
    "    \n",
    "    Adds noise to fundamental features during training to prevent\n",
    "    overfitting to repeated quarterly values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        price_cols: list[str],\n",
    "        fund_cols: list[str],\n",
    "        emb_cols: list[str],\n",
    "        fundamental_noise_std: float = 0.0,\n",
    "        training: bool = True,\n",
    "    ):\n",
    "        # Filter to rows with news only\n",
    "        has_news = (df[emb_cols].abs().sum(axis=1) > 0)\n",
    "        df_news = df[has_news].copy()\n",
    "        print(f\"Filtered to news-only: {len(df_news):,} / {len(df):,} rows ({len(df_news)/len(df)*100:.1f}%)\")\n",
    "\n",
    "        self.df = df_news.reset_index(drop=True)\n",
    "        self.price_cols = price_cols\n",
    "        self.fund_cols = fund_cols\n",
    "        self.emb_cols = emb_cols\n",
    "        self.fundamental_noise_std = fundamental_noise_std\n",
    "        self.training = training\n",
    "\n",
    "        # Group by date\n",
    "        self.date_groups = {}\n",
    "        for date, group in df_news.groupby(\"feature_date\"):\n",
    "            indices = group.index.tolist()\n",
    "            if len(indices) < 2:\n",
    "                continue\n",
    "            self.date_groups[date] = np.array(indices)\n",
    "\n",
    "        self.dates = list(self.date_groups.keys())\n",
    "        print(f\"Days with sufficient news coverage: {len(self.dates)}\")\n",
    "\n",
    "        # Precompute arrays\n",
    "        self.price_arr = df_news[price_cols].values.astype(np.float32)\n",
    "        self.fund_arr = df_news[fund_cols].values.astype(np.float32)\n",
    "        self.emb_arr = df_news[emb_cols].values.astype(np.float32)\n",
    "        self.target_arr = df_news[\"target_return\"].values.astype(np.float32)\n",
    "\n",
    "        # Map original index to position in filtered df\n",
    "        self.idx_map = {old_idx: new_idx for new_idx, old_idx in enumerate(df_news.index)}\n",
    "\n",
    "        # Generate pairs\n",
    "        self.pairs = []\n",
    "        self._generate_pairs()\n",
    "        \n",
    "        if self.fundamental_noise_std > 0:\n",
    "            print(f\"Fundamental noise augmentation: std={self.fundamental_noise_std}\")\n",
    "\n",
    "    def _generate_pairs(self):\n",
    "        \"\"\"Generate all pairs.\"\"\"\n",
    "        pairs = []\n",
    "        for date in self.dates:\n",
    "            indices = self.date_groups[date]\n",
    "            n = len(indices)\n",
    "            for i in range(n):\n",
    "                for j in range(i + 1, n):\n",
    "                    idx_i = self.idx_map[indices[i]]\n",
    "                    idx_j = self.idx_map[indices[j]]\n",
    "                    pairs.append((idx_i, idx_j))\n",
    "\n",
    "        self.pairs = pairs\n",
    "        print(f\"Generated {len(self.pairs):,} pairs\")\n",
    "\n",
    "    def resample_pairs(self):\n",
    "        \"\"\"Reshuffle pairs.\"\"\"\n",
    "        np.random.shuffle(self.pairs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i, j = self.pairs[idx]\n",
    "\n",
    "        price_i = self.price_arr[i].copy()\n",
    "        price_j = self.price_arr[j].copy()\n",
    "        fund_i = self.fund_arr[i].copy()\n",
    "        fund_j = self.fund_arr[j].copy()\n",
    "        emb_i = self.emb_arr[i]\n",
    "        emb_j = self.emb_arr[j]\n",
    "\n",
    "        # Add noise to fundamentals during training\n",
    "        if self.training and self.fundamental_noise_std > 0:\n",
    "            fund_i = fund_i + np.random.normal(0, self.fundamental_noise_std, size=fund_i.shape).astype(np.float32)\n",
    "            fund_j = fund_j + np.random.normal(0, self.fundamental_noise_std, size=fund_j.shape).astype(np.float32)\n",
    "\n",
    "        actual_label = 1.0 if self.target_arr[i] > self.target_arr[j] else 0.0\n",
    "\n",
    "        # Random swap for label balance\n",
    "        if np.random.random() < 0.5:\n",
    "            price_i, price_j = price_j, price_i\n",
    "            fund_i, fund_j = fund_j, fund_i\n",
    "            emb_i, emb_j = emb_j, emb_i\n",
    "            label = 1.0 - actual_label\n",
    "        else:\n",
    "            label = actual_label\n",
    "\n",
    "        return {\n",
    "            \"price_i\": torch.tensor(price_i),\n",
    "            \"price_j\": torch.tensor(price_j),\n",
    "            \"fund_i\": torch.tensor(fund_i),\n",
    "            \"fund_j\": torch.tensor(fund_j),\n",
    "            \"emb_i\": torch.tensor(emb_i),\n",
    "            \"emb_j\": torch.tensor(emb_j),\n",
    "            \"label\": torch.tensor(label),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBranchRanker(nn.Module):\n",
    "    \"\"\"Multi-branch model with configurable fundamental dropout.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.fund_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_fundamental_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.fundamental_dropout),  # Tunable!\n",
    "            nn.Linear(64, config.fundamental_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.price_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_price_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.price_dropout),\n",
    "            nn.Linear(32, config.price_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.news_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.news_dropout),\n",
    "            nn.Linear(128, config.news_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        fused_dim = config.fundamental_latent + config.price_latent + config.news_latent\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, price, fund, emb):\n",
    "        h_f = self.fund_encoder(fund)\n",
    "        h_p = self.price_encoder(price)\n",
    "        h_n = self.news_encoder(emb)\n",
    "        h_n_scaled = self.config.news_alpha * h_n\n",
    "        h = torch.cat([h_f, h_p, h_n_scaled], dim=-1)\n",
    "        return self.output_head(h).squeeze(-1)\n",
    "    \n",
    "    def forward_pair(self, price_i, fund_i, emb_i, price_j, fund_j, emb_j):\n",
    "        score_i = self.forward(price_i, fund_i, emb_i)\n",
    "        score_j = self.forward(price_j, fund_j, emb_j)\n",
    "        return torch.sigmoid(score_i - score_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_ranking_loss(pred_prob, label, smoothing=0.1):\n",
    "    smoothed_label = label * (1 - smoothing) + 0.5 * smoothing\n",
    "    return F.binary_cross_entropy(pred_prob, smoothed_label)\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        price_i = batch[\"price_i\"].to(device)\n",
    "        price_j = batch[\"price_j\"].to(device)\n",
    "        fund_i = batch[\"fund_i\"].to(device)\n",
    "        fund_j = batch[\"fund_j\"].to(device)\n",
    "        emb_i = batch[\"emb_i\"].to(device)\n",
    "        emb_j = batch[\"emb_j\"].to(device)\n",
    "        label = batch[\"label\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred_prob = model.forward_pair(price_i, fund_i, emb_i, price_j, fund_j, emb_j)\n",
    "        loss = pairwise_ranking_loss(pred_prob, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(label)\n",
    "        total_correct += ((pred_prob > 0.5) == (label > 0.5)).sum().item()\n",
    "        total_samples += len(label)\n",
    "    \n",
    "    return total_loss / total_samples, total_correct / total_samples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        price_i = batch[\"price_i\"].to(device)\n",
    "        price_j = batch[\"price_j\"].to(device)\n",
    "        fund_i = batch[\"fund_i\"].to(device)\n",
    "        fund_j = batch[\"fund_j\"].to(device)\n",
    "        emb_i = batch[\"emb_i\"].to(device)\n",
    "        emb_j = batch[\"emb_j\"].to(device)\n",
    "        label = batch[\"label\"].to(device)\n",
    "        \n",
    "        pred_prob = model.forward_pair(price_i, fund_i, emb_i, price_j, fund_j, emb_j)\n",
    "        loss = pairwise_ranking_loss(pred_prob, label)\n",
    "        \n",
    "        total_loss += loss.item() * len(label)\n",
    "        total_correct += ((pred_prob > 0.5) == (label > 0.5)).sum().item()\n",
    "        total_samples += len(label)\n",
    "    \n",
    "    return total_loss / total_samples, total_correct / total_samples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_scores(model, df, price_cols, fund_cols, emb_cols, device, batch_size=1024):\n",
    "    \"\"\"Score all rows in dataframe.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    price_arr = torch.tensor(df[price_cols].values.astype(np.float32))\n",
    "    fund_arr = torch.tensor(df[fund_cols].values.astype(np.float32))\n",
    "    emb_arr = torch.tensor(df[emb_cols].values.astype(np.float32))\n",
    "    \n",
    "    scores = []\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        price = price_arr[i:i+batch_size].to(device)\n",
    "        fund = fund_arr[i:i+batch_size].to(device)\n",
    "        emb = emb_arr[i:i+batch_size].to(device)\n",
    "        score = model(price, fund, emb)\n",
    "        scores.append(score.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(scores)\n",
    "\n",
    "\n",
    "def compute_daily_ic(df):\n",
    "    \"\"\"Compute Spearman IC per day.\"\"\"\n",
    "    ics = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < 10:\n",
    "            continue\n",
    "        ic, _ = spearmanr(group[\"score\"], group[\"target_return\"])\n",
    "        if not np.isnan(ic):\n",
    "            ics.append({\"date\": date, \"ic\": ic})\n",
    "    return pd.DataFrame(ics)\n",
    "\n",
    "\n",
    "def compute_short_returns(df, k=5, clip_return=0.10):\n",
    "    \"\"\"Compute daily short returns for bottom-K with return clipping.\"\"\"\n",
    "    returns = []\n",
    "    for date, group in df.groupby(\"feature_date\"):\n",
    "        if len(group) < k * 2:\n",
    "            continue\n",
    "        bottom = group.nsmallest(k, \"score\")\n",
    "        clipped_returns = bottom[\"target_return\"].clip(-clip_return, clip_return)\n",
    "        short_ret = -clipped_returns.mean()\n",
    "        returns.append({\"date\": date, \"return\": short_ret})\n",
    "    return pd.DataFrame(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6. Train and Compare Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_df, val_df, config, fundamental_noise_std=0.0, n_epochs=10):\n",
    "    \"\"\"Train a model with given config and return results.\"\"\"\n",
    "    \n",
    "    noise_str = f\"noise={fundamental_noise_std}\" if fundamental_noise_std > 0 else \"no_noise\"\n",
    "    dropout_str = f\"dropout={config.fundamental_dropout}\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {noise_str}, {dropout_str}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = AugmentedPairwiseDataset(\n",
    "        train_df, price_feat_cols, fund_feat_cols, emb_cols,\n",
    "        fundamental_noise_std=fundamental_noise_std,\n",
    "        training=True,\n",
    "    )\n",
    "    val_dataset = AugmentedPairwiseDataset(\n",
    "        val_df, price_feat_cols, fund_feat_cols, emb_cols,\n",
    "        fundamental_noise_std=0.0,  # No noise during validation\n",
    "        training=False,\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "    \n",
    "    # Create model\n",
    "    model = MultiBranchRanker(config).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "    \n",
    "    # Train\n",
    "    best_val_acc = 0\n",
    "    history = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        train_dataset.resample_pairs()\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device)\n",
    "        val_loss, val_acc = eval_epoch(model, val_loader, device)\n",
    "        scheduler.step()\n",
    "        \n",
    "        history.append({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc,\n",
    "        })\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}: \"\n",
    "              f\"train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = model.state_dict().copy()\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_state)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.5, 0.1), (0.5, 0.05), (0.5, 0.0), (0.2, 0.1), (0.2, 0.05), (0.2, 0.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configurations to test\n",
    "# Format: (fundamental_dropout, fundamental_noise_std)\n",
    "configs_to_test = [\n",
    "    (0.2, 0.0),   # Baseline: original dropout, no noise\n",
    "    (0.2, 0.05),  # Low noise\n",
    "    (0.2, 0.10),  # Medium noise\n",
    "    (0.5, 0.0),   # High dropout only\n",
    "    (0.5, 0.05),  # High dropout + low noise\n",
    "    (0.5, 0.10),  # High dropout + medium noise\n",
    "]\n",
    "configs_to_test = list(reversed(configs_to_test))\n",
    "configs_to_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training: noise=0.1, dropout=0.5\n",
      "============================================================\n",
      "Filtered to news-only: 339,872 / 1,418,494 rows (24.0%)\n",
      "Days with sufficient news coverage: 830\n",
      "Generated 71,008,149 pairs\n",
      "Fundamental noise augmentation: std=0.1\n",
      "Filtered to news-only: 58,882 / 210,247 rows (28.0%)\n",
      "Days with sufficient news coverage: 118\n",
      "Generated 14,886,015 pairs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/138688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/29075 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: train_acc=0.5544, val_acc=0.5034\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/138688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/29075 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: train_acc=0.5759, val_acc=0.5028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/138688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/29075 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: train_acc=0.5843, val_acc=0.5033\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/138688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/29075 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: train_acc=0.5911, val_acc=0.5028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/138688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/29075 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: train_acc=0.5967, val_acc=0.5028\n",
      "\n",
      "d=0.5_n=0.1 Test Results:\n",
      "  IC: -0.0121 (Sharpe: -2.36)\n",
      "  Short K=5 Sharpe: 3.12\n",
      "\n",
      "============================================================\n",
      "Training: noise=0.05, dropout=0.5\n",
      "============================================================\n",
      "Filtered to news-only: 339,872 / 1,418,494 rows (24.0%)\n",
      "Days with sufficient news coverage: 830\n",
      "Generated 71,008,149 pairs\n",
      "Fundamental noise augmentation: std=0.05\n",
      "Filtered to news-only: 58,882 / 210,247 rows (28.0%)\n",
      "Days with sufficient news coverage: 118\n",
      "Generated 14,886,015 pairs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/138688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/29075 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: train_acc=0.5524, val_acc=0.5053\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/138688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/29075 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: train_acc=0.5721, val_acc=0.5033\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/138688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/29075 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: train_acc=0.5796, val_acc=0.5025\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/138688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/29075 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: train_acc=0.5855, val_acc=0.5029\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/138688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/29075 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: train_acc=0.5905, val_acc=0.5029\n",
      "\n",
      "d=0.5_n=0.05 Test Results:\n",
      "  IC: 0.0170 (Sharpe: 2.82)\n",
      "  Short K=5 Sharpe: 2.89\n",
      "\n",
      "============================================================\n",
      "Training: no_noise, dropout=0.5\n",
      "============================================================\n",
      "Filtered to news-only: 339,872 / 1,418,494 rows (24.0%)\n",
      "Days with sufficient news coverage: 830\n",
      "Generated 71,008,149 pairs\n",
      "Filtered to news-only: 58,882 / 210,247 rows (28.0%)\n",
      "Days with sufficient news coverage: 118\n",
      "Generated 14,886,015 pairs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3f707b7b7f42fd8b2afd06700c9d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/138688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 13\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fund_dropout, noise_std \u001b[38;5;129;01min\u001b[39;00m configs_to_test:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Create config with specific dropout\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     config \u001b[38;5;241m=\u001b[39m ModelConfig(\n\u001b[1;32m      6\u001b[0m         n_fundamental_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(fund_feat_cols),\n\u001b[1;32m      7\u001b[0m         n_price_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(price_feat_cols),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m         fundamental_noise_std\u001b[38;5;241m=\u001b[39mnoise_std,\n\u001b[1;32m     11\u001b[0m     )\n\u001b[0;32m---> 13\u001b[0m     model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfundamental_noise_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     test_df_eval \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mcopy()\n",
      "Cell \u001b[0;32mIn[10], line 37\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_df, val_df, config, fundamental_noise_std, n_epochs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m     35\u001b[0m     train_dataset\u001b[38;5;241m.\u001b[39mresample_pairs()\n\u001b[0;32m---> 37\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m eval_epoch(model, val_loader, device)\n\u001b[1;32m     39\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m total_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     10\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     13\u001b[0m     price_i \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_i\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m     price_j \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice_j\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    738\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:788\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    787\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 788\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    790\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[7], line 96\u001b[0m, in \u001b[0;36mAugmentedPairwiseDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     93\u001b[0m actual_label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_arr[i] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_arr[j] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Random swap for label balance\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.5\u001b[39m:\n\u001b[1;32m     97\u001b[0m     price_i, price_j \u001b[38;5;241m=\u001b[39m price_j, price_i\n\u001b[1;32m     98\u001b[0m     fund_i, fund_j \u001b[38;5;241m=\u001b[39m fund_j, fund_i\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for fund_dropout, noise_std in configs_to_test:\n",
    "    # Create config with specific dropout\n",
    "    config = ModelConfig(\n",
    "        n_fundamental_features=len(fund_feat_cols),\n",
    "        n_price_features=len(price_feat_cols),\n",
    "        n_embedding_dim=len(emb_cols),\n",
    "        fundamental_dropout=fund_dropout,\n",
    "        fundamental_noise_std=noise_std,\n",
    "    )\n",
    "    \n",
    "    model, history = train_model(\n",
    "        train_df, val_df, config, \n",
    "        fundamental_noise_std=noise_std,\n",
    "        n_epochs=5\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_df_eval = test_df.copy()\n",
    "    test_df_eval[\"score\"] = get_scores(model, test_df_eval, price_feat_cols, fund_feat_cols, emb_cols, device)\n",
    "    \n",
    "    # Compute metrics\n",
    "    ic_df = compute_daily_ic(test_df_eval)\n",
    "    short_df = compute_short_returns(test_df_eval, k=5, clip_return=0.10)\n",
    "    \n",
    "    mean_ic = ic_df['ic'].mean()\n",
    "    ic_sharpe = mean_ic / ic_df['ic'].std() * np.sqrt(252)\n",
    "    \n",
    "    short_sharpe = short_df['return'].mean() / short_df['return'].std() * np.sqrt(252)\n",
    "    short_cumret = (1 + short_df['return']).cumprod().iloc[-1] - 1\n",
    "    \n",
    "    config_name = f\"d={fund_dropout}_n={noise_std}\"\n",
    "    results[config_name] = {\n",
    "        'model': model,\n",
    "        'config': config,\n",
    "        'history': history,\n",
    "        'mean_ic': mean_ic,\n",
    "        'ic_sharpe': ic_sharpe,\n",
    "        'short_sharpe': short_sharpe,\n",
    "        'short_cumret': short_cumret,\n",
    "        'fund_dropout': fund_dropout,\n",
    "        'noise_std': noise_std,\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{config_name} Test Results:\")\n",
    "    print(f\"  IC: {mean_ic:.4f} (Sharpe: {ic_sharpe:.2f})\")\n",
    "    print(f\"  Short K=5 Sharpe: {short_sharpe:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 7. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "summary = []\n",
    "for config_name, res in results.items():\n",
    "    summary.append({\n",
    "        'config': config_name,\n",
    "        'fund_dropout': res['fund_dropout'],\n",
    "        'noise_std': res['noise_std'],\n",
    "        'mean_ic': res['mean_ic'],\n",
    "        'ic_sharpe': res['ic_sharpe'],\n",
    "        'short_sharpe': res['short_sharpe'],\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON SUMMARY (Test Set)\")\n",
    "print(\"=\" * 80)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "configs = list(results.keys())\n",
    "x = np.arange(len(configs))\n",
    "\n",
    "# IC Sharpe\n",
    "ax = axes[0]\n",
    "ax.bar(x, [results[c]['ic_sharpe'] for c in configs])\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(configs, rotation=45, ha='right')\n",
    "ax.set_ylabel('IC Sharpe')\n",
    "ax.set_title('Information Coefficient Sharpe')\n",
    "ax.axhline(results['d=0.2_n=0.0']['ic_sharpe'], color='red', linestyle='--', alpha=0.5, label='Baseline')\n",
    "ax.legend()\n",
    "\n",
    "# Short Sharpe\n",
    "ax = axes[1]\n",
    "ax.bar(x, [results[c]['short_sharpe'] for c in configs])\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(configs, rotation=45, ha='right')\n",
    "ax.set_ylabel('Short Sharpe')\n",
    "ax.set_title('Short Strategy Sharpe (K=5)')\n",
    "ax.axhline(results['d=0.2_n=0.0']['short_sharpe'], color='red', linestyle='--', alpha=0.5, label='Baseline')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap view: dropout vs noise\n",
    "pivot_ic = summary_df.pivot(index='fund_dropout', columns='noise_std', values='ic_sharpe')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "im = ax.imshow(pivot_ic.values, cmap='RdYlGn', aspect='auto')\n",
    "\n",
    "ax.set_xticks(np.arange(len(pivot_ic.columns)))\n",
    "ax.set_yticks(np.arange(len(pivot_ic.index)))\n",
    "ax.set_xticklabels([f'{x:.2f}' for x in pivot_ic.columns])\n",
    "ax.set_yticklabels([f'{x:.1f}' for x in pivot_ic.index])\n",
    "ax.set_xlabel('Noise Std')\n",
    "ax.set_ylabel('Fundamental Dropout')\n",
    "ax.set_title('IC Sharpe by Dropout and Noise')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(pivot_ic.index)):\n",
    "    for j in range(len(pivot_ic.columns)):\n",
    "        text = ax.text(j, i, f'{pivot_ic.values[i, j]:.2f}',\n",
    "                       ha='center', va='center', color='black', fontsize=12)\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='IC Sharpe')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for config_name, res in results.items():\n",
    "    hist = pd.DataFrame(res['history'])\n",
    "    axes[0].plot(hist['epoch'], hist['train_acc'], label=f\"{config_name}\")\n",
    "    axes[1].plot(hist['epoch'], hist['val_acc'], label=f\"{config_name}\")\n",
    "\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Training Accuracy')\n",
    "axes[0].legend(fontsize=8)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Validation Accuracy')\n",
    "axes[1].legend(fontsize=8)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 8. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best config by IC Sharpe\n",
    "best_config_name = max(results.keys(), key=lambda c: results[c]['ic_sharpe'])\n",
    "baseline_name = 'd=0.2_n=0.0'\n",
    "\n",
    "print(f\"Best config: {best_config_name}\")\n",
    "print(f\"  IC Sharpe: {results[best_config_name]['ic_sharpe']:.2f}\")\n",
    "print(f\"  Mean IC: {results[best_config_name]['mean_ic']:.4f}\")\n",
    "\n",
    "print(f\"\\nBaseline ({baseline_name}):\")\n",
    "print(f\"  IC Sharpe: {results[baseline_name]['ic_sharpe']:.2f}\")\n",
    "print(f\"  Mean IC: {results[baseline_name]['mean_ic']:.4f}\")\n",
    "\n",
    "improvement = results[best_config_name]['ic_sharpe'] - results[baseline_name]['ic_sharpe']\n",
    "print(f\"\\nImprovement: {improvement:+.2f}\")\n",
    "\n",
    "if improvement > 0:\n",
    "    print(\"\\n>>> Regularization IMPROVES performance!\")\n",
    "else:\n",
    "    print(\"\\n>>> Baseline is best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model if it beats baseline\n",
    "if results[best_config_name]['ic_sharpe'] > results[baseline_name]['ic_sharpe']:\n",
    "    best_model = results[best_config_name]['model']\n",
    "    best_config = results[best_config_name]['config']\n",
    "    \n",
    "    torch.save({\n",
    "        \"model_state_dict\": best_model.state_dict(),\n",
    "        \"config\": best_config,\n",
    "        \"price_cols\": price_feat_cols,\n",
    "        \"fund_cols\": fund_feat_cols,\n",
    "        \"emb_cols\": emb_cols,\n",
    "        \"fundamental_dropout\": best_config.fundamental_dropout,\n",
    "        \"fundamental_noise_std\": best_config.fundamental_noise_std,\n",
    "    }, \"data/model_fund_reg.pt\")\n",
    "    print(f\"Saved model ({best_config_name}) to data/model_fund_reg.pt\")\n",
    "else:\n",
    "    print(\"NOT saving - baseline performs better\")\n",
    "    print(\"Use model_final.pt instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
