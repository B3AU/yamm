{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Liquid Universe Training\n",
    "\n",
    "Retrain the model on liquid stocks only (>$50M daily volume).\n",
    "\n",
    "**Hypothesis**: The original model learned patterns in small/illiquid stocks that:\n",
    "1. Don't transfer to liquid stocks\n",
    "2. Actually invert in liquid stocks\n",
    "\n",
    "A model trained specifically on liquid stocks might find different (weaker but tradeable) patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from itertools import product\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    n_fundamental_features: int = 19\n",
    "    n_price_features: int = 9\n",
    "    n_embedding_dim: int = 768\n",
    "    fund_hidden: int = 64\n",
    "    price_hidden: int = 32\n",
    "    news_hidden: int = 128\n",
    "    fundamental_latent: int = 32\n",
    "    price_latent: int = 16\n",
    "    news_latent: int = 32\n",
    "    fundamental_dropout: float = 0.6\n",
    "    price_dropout: float = 0.3\n",
    "    news_dropout: float = 0.1\n",
    "    news_alpha: float = 0.8\n",
    "    batch_size: int = 512\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-3\n",
    "    label_smoothing: float = 0.1\n",
    "    n_epochs: int = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Filter to Liquid Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset: 2,092,929 rows\n",
      "Date range: 2021-01-13 to 2025-12-18\n"
     ]
    }
   ],
   "source": [
    "# Load ML dataset\n",
    "df = pd.read_parquet(\"data/ml_dataset.pqt\")\n",
    "df[\"feature_date\"] = pd.to_datetime(df[\"feature_date\"])\n",
    "df[\"simple_return\"] = np.exp(df[\"target_return\"]) - 1\n",
    "df[\"simple_return_clipped\"] = df[\"simple_return\"].clip(-1.0, 1.0)\n",
    "\n",
    "print(f\"Full dataset: {len(df):,} rows\")\n",
    "print(f\"Date range: {df['feature_date'].min().date()} to {df['feature_date'].max().date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liquidity data for 5,644 symbols\n"
     ]
    }
   ],
   "source": [
    "# Load price data to compute liquidity\n",
    "prices_df = pd.read_parquet('data/prices.pqt')\n",
    "prices_df['date'] = pd.to_datetime(prices_df['date'])\n",
    "prices_df['dollar_volume'] = prices_df['volume'] * prices_df['close']\n",
    "\n",
    "# Compute average dollar volume per symbol (rolling would be better, but this is simpler)\n",
    "symbol_liquidity = prices_df.groupby('symbol')['dollar_volume'].mean().reset_index()\n",
    "symbol_liquidity.columns = ['symbol', 'avg_dollar_volume']\n",
    "\n",
    "print(f\"Liquidity data for {len(symbol_liquidity):,} symbols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged. Missing liquidity: 0\n",
      "After dropping missing: 2,092,929 rows\n"
     ]
    }
   ],
   "source": [
    "# Merge liquidity into main dataset\n",
    "df = df.merge(symbol_liquidity, on='symbol', how='left')\n",
    "print(f\"Merged. Missing liquidity: {df['avg_dollar_volume'].isna().sum():,}\")\n",
    "\n",
    "# Drop rows without liquidity data\n",
    "df = df[df['avg_dollar_volume'].notna()].copy()\n",
    "print(f\"After dropping missing: {len(df):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LIQUID UNIVERSE (>= $50M daily volume)\n",
      "==================================================\n",
      "Rows: 1,100,207 (52.6% of total)\n",
      "Unique symbols: 1,022\n",
      "Date range: 2021-01-13 to 2025-12-18\n"
     ]
    }
   ],
   "source": [
    "# Filter to liquid stocks only\n",
    "LIQUID_THRESHOLD = 50e6  # $50M daily volume\n",
    "\n",
    "df_liquid = df[df['avg_dollar_volume'] >= LIQUID_THRESHOLD].copy()\n",
    "\n",
    "print(f\"\\nLIQUID UNIVERSE (>= ${LIQUID_THRESHOLD/1e6:.0f}M daily volume)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Rows: {len(df_liquid):,} ({len(df_liquid)/len(df)*100:.1f}% of total)\")\n",
    "print(f\"Unique symbols: {df_liquid['symbol'].nunique():,}\")\n",
    "print(f\"Date range: {df_liquid['feature_date'].min().date()} to {df_liquid['feature_date'].max().date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price features: 9\n",
      "Fundamental features: 19\n",
      "Embedding dims: 768\n"
     ]
    }
   ],
   "source": [
    "# Feature columns\n",
    "price_feat_cols = [\n",
    "    \"overnight_gap_z\", \"intraday_ret_z\",\n",
    "    \"ret_1d_z\", \"ret_2d_z\", \"ret_3d_z\", \"ret_5d_z\",\n",
    "    \"vol_5d_z\", \"dist_from_high_5d_z\", \"dist_from_low_5d_z\"\n",
    "]\n",
    "fund_feat_cols = [c for c in df_liquid.columns if c.endswith(\"_z\") and c not in price_feat_cols and c != \"news_count_z\"]\n",
    "emb_cols = [c for c in df_liquid.columns if c.startswith(\"emb_\")]\n",
    "\n",
    "print(f\"Price features: {len(price_feat_cols)}\")\n",
    "print(f\"Fundamental features: {len(fund_feat_cols)}\")\n",
    "print(f\"Embedding dims: {len(emb_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DATA SPLITS (Liquid + News-only):\n",
      "Train: 241,382 rows (2021-01-13 to 2024-05-01)\n",
      "Val:   40,236 rows (2024-05-02 to 2024-10-21)\n",
      "Test:  83,936 rows (2024-10-22 to 2025-12-18)\n"
     ]
    }
   ],
   "source": [
    "# Time-based split\n",
    "dates = sorted(df_liquid[\"feature_date\"].unique())\n",
    "n_dates = len(dates)\n",
    "train_end = int(n_dates * 0.7)\n",
    "val_end = int(n_dates * 0.8)\n",
    "\n",
    "train_dates = set(dates[:train_end])\n",
    "val_dates = set(dates[train_end:val_end])\n",
    "test_dates = set(dates[val_end:])\n",
    "\n",
    "train_df = df_liquid[df_liquid[\"feature_date\"].isin(train_dates)].copy()\n",
    "val_df = df_liquid[df_liquid[\"feature_date\"].isin(val_dates)].copy()\n",
    "test_df = df_liquid[df_liquid[\"feature_date\"].isin(test_dates)].copy()\n",
    "\n",
    "def filter_news_only(df_in):\n",
    "    has_news = (df_in[emb_cols].abs().sum(axis=1) > 0)\n",
    "    return df_in[has_news].copy()\n",
    "\n",
    "train_df_news = filter_news_only(train_df)\n",
    "val_df_news = filter_news_only(val_df)\n",
    "test_df_news = filter_news_only(test_df)\n",
    "\n",
    "print(f\"\\nDATA SPLITS (Liquid + News-only):\")\n",
    "print(f\"Train: {len(train_df_news):,} rows ({min(train_dates).date()} to {max(train_dates).date()})\")\n",
    "print(f\"Val:   {len(val_df_news):,} rows ({min(val_dates).date()} to {max(val_dates).date()})\")\n",
    "print(f\"Test:  {len(test_df_news):,} rows ({min(test_dates).date()} to {max(test_dates).date()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset and Model (Same as 2.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinglePairDataset(Dataset):\n",
    "    def __init__(self, df, price_cols, fund_cols, emb_cols, verbose=True):\n",
    "        # Already filtered to news-only before passing in\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.price_cols = price_cols\n",
    "        self.fund_cols = fund_cols\n",
    "        self.emb_cols = emb_cols\n",
    "\n",
    "        self.date_groups = {}\n",
    "        for date, group in self.df.groupby(\"feature_date\"):\n",
    "            indices = group.index.tolist()\n",
    "            if len(indices) >= 2:\n",
    "                self.date_groups[date] = indices\n",
    "        self.dates = list(self.date_groups.keys())\n",
    "\n",
    "        self.price_arr = self.df[price_cols].values.astype(np.float32)\n",
    "        self.fund_arr = self.df[fund_cols].values.astype(np.float32)\n",
    "        self.emb_arr = self.df[emb_cols].values.astype(np.float32)\n",
    "        self.target_arr = self.df[\"simple_return_clipped\"].values.astype(np.float32)\n",
    "\n",
    "        self.pairs = []\n",
    "        self._generate_pairs(verbose=verbose)\n",
    "\n",
    "    def _generate_pairs(self, verbose=False):\n",
    "        pairs = []\n",
    "        for date in self.dates:\n",
    "            indices = list(self.date_groups[date])\n",
    "            np.random.shuffle(indices)\n",
    "            for i in range(0, len(indices) - 1, 2):\n",
    "                pairs.append((indices[i], indices[i + 1]))\n",
    "        self.pairs = pairs\n",
    "        if verbose:\n",
    "            print(f\"Generated {len(self.pairs):,} pairs from {len(self.df):,} rows\")\n",
    "\n",
    "    def resample_pairs(self):\n",
    "        self._generate_pairs()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i, j = self.pairs[idx]\n",
    "        price_i, price_j = self.price_arr[i], self.price_arr[j]\n",
    "        fund_i, fund_j = self.fund_arr[i], self.fund_arr[j]\n",
    "        emb_i, emb_j = self.emb_arr[i], self.emb_arr[j]\n",
    "        label = 1.0 if self.target_arr[i] > self.target_arr[j] else 0.0\n",
    "\n",
    "        if np.random.random() < 0.5:\n",
    "            price_i, price_j = price_j, price_i\n",
    "            fund_i, fund_j = fund_j, fund_i\n",
    "            emb_i, emb_j = emb_j, emb_i\n",
    "            label = 1.0 - label\n",
    "\n",
    "        return {\n",
    "            \"price_i\": torch.tensor(price_i), \"price_j\": torch.tensor(price_j),\n",
    "            \"fund_i\": torch.tensor(fund_i), \"fund_j\": torch.tensor(fund_j),\n",
    "            \"emb_i\": torch.tensor(emb_i), \"emb_j\": torch.tensor(emb_j),\n",
    "            \"label\": torch.tensor(label),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiBranchRanker(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.fund_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_fundamental_features, config.fund_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.fundamental_dropout),\n",
    "            nn.Linear(config.fund_hidden, config.fundamental_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.price_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_price_features, config.price_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.price_dropout),\n",
    "            nn.Linear(config.price_hidden, config.price_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.news_encoder = nn.Sequential(\n",
    "            nn.Linear(config.n_embedding_dim, config.news_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.news_dropout),\n",
    "            nn.Linear(config.news_hidden, config.news_latent),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        fused_dim = config.fundamental_latent + config.price_latent + config.news_latent\n",
    "        self.output_head = nn.Sequential(\n",
    "            nn.Linear(fused_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, price, fund, emb):\n",
    "        h_f = self.fund_encoder(fund)\n",
    "        h_p = self.price_encoder(price)\n",
    "        h_n = self.news_encoder(emb)\n",
    "        h_n_scaled = self.config.news_alpha * h_n\n",
    "        h = torch.cat([h_f, h_p, h_n_scaled], dim=-1)\n",
    "        return self.output_head(h).squeeze(-1)\n",
    "    \n",
    "    def forward_pair(self, price_i, fund_i, emb_i, price_j, fund_j, emb_j):\n",
    "        score_i = self.forward(price_i, fund_i, emb_i)\n",
    "        score_j = self.forward(price_j, fund_j, emb_j)\n",
    "        return torch.sigmoid(score_i - score_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_scores(model, df, price_cols, fund_cols, emb_cols, device, batch_size=1024):\n",
    "    model.eval()\n",
    "    price_arr = torch.tensor(df[price_cols].values.astype(np.float32))\n",
    "    fund_arr = torch.tensor(df[fund_cols].values.astype(np.float32))\n",
    "    emb_arr = torch.tensor(df[emb_cols].values.astype(np.float32))\n",
    "    \n",
    "    scores = []\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        price = price_arr[i:i+batch_size].to(device)\n",
    "        fund = fund_arr[i:i+batch_size].to(device)\n",
    "        emb = emb_arr[i:i+batch_size].to(device)\n",
    "        score = model(price, fund, emb)\n",
    "        scores.append(score.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(scores)\n",
    "\n",
    "\n",
    "def evaluate_model(model, df, price_cols, fund_cols, emb_cols, device, k=5):\n",
    "    \"\"\"Evaluate model on liquid stocks - long side, short side, and L/S.\"\"\"\n",
    "    df_eval = df.copy()\n",
    "    df_eval[\"score\"] = get_scores(model, df_eval, price_cols, fund_cols, emb_cols, device)\n",
    "    \n",
    "    # IC (rank correlation)\n",
    "    ics = []\n",
    "    for date, group in df_eval.groupby(\"feature_date\"):\n",
    "        if len(group) < 10:\n",
    "            continue\n",
    "        ic, _ = spearmanr(group[\"score\"], group[\"simple_return\"])\n",
    "        if not np.isnan(ic):\n",
    "            ics.append(ic)\n",
    "    \n",
    "    mean_ic = np.mean(ics) if ics else 0\n",
    "    ic_std = np.std(ics) if ics else 1\n",
    "    ic_sharpe = mean_ic / ic_std * np.sqrt(252) if ic_std > 0 else 0\n",
    "    \n",
    "    # Strategy returns\n",
    "    long_returns = []\n",
    "    short_returns = []\n",
    "    \n",
    "    for date, group in df_eval.groupby(\"feature_date\"):\n",
    "        if len(group) < 10:\n",
    "            continue\n",
    "        top_k = group.nlargest(k, \"score\")\n",
    "        bottom_k = group.nsmallest(k, \"score\")\n",
    "        \n",
    "        long_returns.append(top_k[\"simple_return\"].mean())\n",
    "        short_returns.append(-bottom_k[\"simple_return\"].mean())\n",
    "    \n",
    "    long_arr = np.array(long_returns)\n",
    "    short_arr = np.array(short_returns)\n",
    "    ls_arr = (long_arr + short_arr) / 2\n",
    "    \n",
    "    def compute_sharpe(returns):\n",
    "        if len(returns) < 2 or np.std(returns) == 0:\n",
    "            return 0\n",
    "        return np.mean(returns) / np.std(returns) * np.sqrt(252)\n",
    "    \n",
    "    return {\n",
    "        'ic_sharpe': ic_sharpe,\n",
    "        'mean_ic': mean_ic,\n",
    "        'long_return': np.mean(long_arr) * 252,  # Annualized\n",
    "        'long_sharpe': compute_sharpe(long_arr),\n",
    "        'short_return': np.mean(short_arr) * 252,\n",
    "        'short_sharpe': compute_sharpe(short_arr),\n",
    "        'ls_return': np.mean(ls_arr) * 252,\n",
    "        'ls_sharpe': compute_sharpe(ls_arr),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_ranking_loss(pred_prob, label, smoothing=0.1):\n",
    "    smoothed_label = label * (1 - smoothing) + 0.5 * smoothing\n",
    "    return F.binary_cross_entropy(pred_prob, smoothed_label)\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, optimizer, device, label_smoothing=0.1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch in loader:\n",
    "        price_i = batch[\"price_i\"].to(device)\n",
    "        price_j = batch[\"price_j\"].to(device)\n",
    "        fund_i = batch[\"fund_i\"].to(device)\n",
    "        fund_j = batch[\"fund_j\"].to(device)\n",
    "        emb_i = batch[\"emb_i\"].to(device)\n",
    "        emb_j = batch[\"emb_j\"].to(device)\n",
    "        label = batch[\"label\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred_prob = model.forward_pair(price_i, fund_i, emb_i, price_j, fund_j, emb_j)\n",
    "        loss = pairwise_ranking_loss(pred_prob, label, smoothing=label_smoothing)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * len(label)\n",
    "        total_samples += len(label)\n",
    "    \n",
    "    return total_loss / total_samples\n",
    "\n",
    "\n",
    "def train_and_evaluate(config, train_dataset, val_df, price_cols, fund_cols, emb_cols, \n",
    "                       selection_metric='ic_sharpe', n_epochs=15, verbose=False):\n",
    "    \"\"\"Train model, select best checkpoint by specified metric.\"\"\"\n",
    "    model = MultiBranchRanker(config).to(device)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=config.learning_rate, \n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    \n",
    "    best_metric = -float('inf')\n",
    "    best_state = None\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        train_dataset.resample_pairs()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device, config.label_smoothing)\n",
    "        scheduler.step()\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0 or epoch == n_epochs - 1:\n",
    "            metrics = evaluate_model(model, val_df, price_cols, fund_cols, emb_cols, device)\n",
    "            \n",
    "            current = metrics[selection_metric]\n",
    "            if current > best_metric:\n",
    "                best_metric = current\n",
    "                best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Epoch {epoch+1}: IC={metrics['ic_sharpe']:.2f}, Long={metrics['long_sharpe']:.2f}, Short={metrics['short_sharpe']:.2f}\")\n",
    "    \n",
    "    model.load_state_dict(best_state)\n",
    "    final_metrics = evaluate_model(model, val_df, price_cols, fund_cols, emb_cols, device)\n",
    "    \n",
    "    return final_metrics, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 120,493 pairs from 241,382 rows\n"
     ]
    }
   ],
   "source": [
    "# Create training dataset from liquid stocks\n",
    "train_dataset = SinglePairDataset(train_df_news, price_feat_cols, fund_feat_cols, emb_cols, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline: Original Model on Liquid Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL ON LIQUID TEST SET\n",
      "==================================================\n",
      "IC Sharpe: 1.66\n",
      "Long:  -5.8% ann, Sharpe -0.25\n",
      "Short: -73.4% ann, Sharpe -0.72\n"
     ]
    }
   ],
   "source": [
    "# Load original model and evaluate on liquid test set\n",
    "from trading.model import ModelInference\n",
    "\n",
    "original_model_path = Path('data/model_robust_optimized.pt')\n",
    "if original_model_path.exists():\n",
    "    original_model = ModelInference(original_model_path)\n",
    "    \n",
    "    # Score liquid test set\n",
    "    test_df_news['orig_score'] = original_model.score(test_df_news)\n",
    "    \n",
    "    # Evaluate\n",
    "    orig_long = []\n",
    "    orig_short = []\n",
    "    orig_ics = []\n",
    "    \n",
    "    for date, group in test_df_news.groupby('feature_date'):\n",
    "        if len(group) < 10:\n",
    "            continue\n",
    "        \n",
    "        ic, _ = spearmanr(group['orig_score'], group['simple_return'])\n",
    "        if not np.isnan(ic):\n",
    "            orig_ics.append(ic)\n",
    "        \n",
    "        top_5 = group.nlargest(5, 'orig_score')\n",
    "        bottom_5 = group.nsmallest(5, 'orig_score')\n",
    "        \n",
    "        orig_long.append(top_5['simple_return'].mean())\n",
    "        orig_short.append(-bottom_5['simple_return'].mean())\n",
    "    \n",
    "    print(\"ORIGINAL MODEL ON LIQUID TEST SET\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"IC Sharpe: {np.mean(orig_ics)/np.std(orig_ics)*np.sqrt(252):.2f}\")\n",
    "    print(f\"Long:  {np.mean(orig_long)*252*100:.1f}% ann, Sharpe {np.mean(orig_long)/np.std(orig_long)*np.sqrt(252):.2f}\")\n",
    "    print(f\"Short: {np.mean(orig_short)*252*100:.1f}% ann, Sharpe {np.mean(orig_short)/np.std(orig_short)*np.sqrt(252):.2f}\")\n",
    "else:\n",
    "    print(\"Original model not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Search Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture configs: 27\n"
     ]
    }
   ],
   "source": [
    "SEARCH_SPACE = {\n",
    "    # Architecture\n",
    "    \"latent_scale\": [0.5, 1.0, 2.0],\n",
    "    \"hidden_scale\": [0.5, 1.0, 1.5],\n",
    "    \"news_alpha\": [0.6, 0.8, 1.0],\n",
    "    # Training\n",
    "    \"learning_rate\": [5e-4, 1e-3, 2e-3],\n",
    "    \"weight_decay\": [1e-4, 1e-3, 1e-2],\n",
    "    \"label_smoothing\": [0.05, 0.1, 0.15],\n",
    "    # Dropout\n",
    "    \"fund_dropout\": [0.3, 0.5, 0.7],\n",
    "    \"price_dropout\": [0.2, 0.3, 0.4],\n",
    "    \"news_dropout\": [0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "# Selection metric - use IC Sharpe since we want to find patterns that generalize\n",
    "SELECTION_METRIC = 'ic_sharpe'\n",
    "\n",
    "n_arch = len(SEARCH_SPACE[\"latent_scale\"]) * len(SEARCH_SPACE[\"hidden_scale\"]) * len(SEARCH_SPACE[\"news_alpha\"])\n",
    "print(f\"Architecture configs: {n_arch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Architecture Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 1: ARCHITECTURE SEARCH\n",
      "======================================================================\n",
      "[ 1/27] l=0.5 h=0.5 a=0.6 | IC=3.27 Long=3.31 Short=-1.15 | 83s\n",
      "[ 2/27] l=0.5 h=0.5 a=0.8 | IC=3.69 Long=2.62 Short=-2.12 | 83s\n",
      "[ 3/27] l=0.5 h=0.5 a=1.0 | IC=3.19 Long=4.37 Short=-2.68 | 83s\n",
      "[ 4/27] l=0.5 h=1.0 a=0.6 | IC=3.62 Long=3.11 Short=-2.65 | 90s\n",
      "[ 5/27] l=0.5 h=1.0 a=0.8 | IC=3.53 Long=2.46 Short=-1.67 | 90s\n",
      "[ 6/27] l=0.5 h=1.0 a=1.0 | IC=3.24 Long=2.22 Short=-0.93 | 90s\n",
      "[ 7/27] l=0.5 h=1.5 a=0.6 | IC=2.92 Long=2.76 Short=-1.29 | 96s\n",
      "[ 8/27] l=0.5 h=1.5 a=0.8 | IC=2.88 Long=1.97 Short=-2.82 | 97s\n",
      "[ 9/27] l=0.5 h=1.5 a=1.0 | IC=2.82 Long=2.81 Short=-1.68 | 97s\n",
      "[10/27] l=1.0 h=0.5 a=0.6 | IC=4.15 Long=1.71 Short=-2.71 | 84s\n",
      "[11/27] l=1.0 h=0.5 a=0.8 | IC=4.54 Long=1.76 Short=-2.61 | 84s\n",
      "[12/27] l=1.0 h=0.5 a=1.0 | IC=3.59 Long=3.57 Short=-2.55 | 84s\n",
      "[13/27] l=1.0 h=1.0 a=0.6 | IC=3.14 Long=2.45 Short=-2.69 | 91s\n",
      "[14/27] l=1.0 h=1.0 a=0.8 | IC=3.83 Long=2.45 Short=-1.56 | 91s\n",
      "[15/27] l=1.0 h=1.0 a=1.0 | IC=3.50 Long=2.14 Short=-2.37 | 91s\n",
      "[16/27] l=1.0 h=1.5 a=0.6 | IC=3.49 Long=1.15 Short=-1.16 | 98s\n",
      "[17/27] l=1.0 h=1.5 a=0.8 | IC=2.29 Long=2.17 Short=-1.95 | 98s\n",
      "[18/27] l=1.0 h=1.5 a=1.0 | IC=3.59 Long=2.44 Short=-2.48 | 98s\n",
      "[19/27] l=2.0 h=0.5 a=0.6 | IC=3.42 Long=3.04 Short=-1.22 | 85s\n",
      "[20/27] l=2.0 h=0.5 a=0.8 | IC=2.17 Long=2.39 Short=-1.68 | 85s\n",
      "[21/27] l=2.0 h=0.5 a=1.0 | IC=2.88 Long=1.28 Short=-2.19 | 85s\n",
      "[22/27] l=2.0 h=1.0 a=0.6 | IC=3.67 Long=2.16 Short=-2.27 | 93s\n",
      "[23/27] l=2.0 h=1.0 a=0.8 | IC=2.69 Long=1.54 Short=-2.35 | 93s\n",
      "[24/27] l=2.0 h=1.0 a=1.0 | IC=3.30 Long=1.63 Short=-2.29 | 92s\n",
      "[25/27] l=2.0 h=1.5 a=0.6 | IC=3.04 Long=2.75 Short=-2.33 | 100s\n",
      "[26/27] l=2.0 h=1.5 a=0.8 | IC=3.35 Long=2.64 Short=-1.88 | 100s\n",
      "[27/27] l=2.0 h=1.5 a=1.0 | IC=3.52 Long=2.11 Short=-1.28 | 100s\n",
      "\n",
      "Best: latent=1.0, hidden=0.5, alpha=0.8\n"
     ]
    }
   ],
   "source": [
    "print(\"STAGE 1: ARCHITECTURE SEARCH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "arch_configs = list(product(\n",
    "    SEARCH_SPACE[\"latent_scale\"],\n",
    "    SEARCH_SPACE[\"hidden_scale\"],\n",
    "    SEARCH_SPACE[\"news_alpha\"],\n",
    "))\n",
    "\n",
    "arch_results = []\n",
    "best_arch_metric = -float('inf')\n",
    "best_arch = None\n",
    "\n",
    "for i, (latent_scale, hidden_scale, news_alpha) in enumerate(arch_configs):\n",
    "    config = ModelConfig(\n",
    "        n_fundamental_features=len(fund_feat_cols),\n",
    "        n_price_features=len(price_feat_cols),\n",
    "        n_embedding_dim=len(emb_cols),\n",
    "        fund_hidden=int(64 * hidden_scale),\n",
    "        price_hidden=int(32 * hidden_scale),\n",
    "        news_hidden=int(128 * hidden_scale),\n",
    "        fundamental_latent=int(32 * latent_scale),\n",
    "        price_latent=int(16 * latent_scale),\n",
    "        news_latent=int(32 * latent_scale),\n",
    "        news_alpha=news_alpha,\n",
    "    )\n",
    "    \n",
    "    start = datetime.now()\n",
    "    metrics, _ = train_and_evaluate(\n",
    "        config, train_dataset, val_df_news, \n",
    "        price_feat_cols, fund_feat_cols, emb_cols,\n",
    "        selection_metric=SELECTION_METRIC, n_epochs=15\n",
    "    )\n",
    "    elapsed = (datetime.now() - start).total_seconds()\n",
    "    \n",
    "    arch_results.append({\n",
    "        \"latent_scale\": latent_scale,\n",
    "        \"hidden_scale\": hidden_scale,\n",
    "        \"news_alpha\": news_alpha,\n",
    "        **metrics,\n",
    "    })\n",
    "    \n",
    "    if metrics[SELECTION_METRIC] > best_arch_metric:\n",
    "        best_arch_metric = metrics[SELECTION_METRIC]\n",
    "        best_arch = (latent_scale, hidden_scale, news_alpha)\n",
    "    \n",
    "    print(f\"[{i+1:2d}/{len(arch_configs)}] l={latent_scale:.1f} h={hidden_scale:.1f} a={news_alpha:.1f} | \"\n",
    "          f\"IC={metrics['ic_sharpe']:.2f} Long={metrics['long_sharpe']:.2f} Short={metrics['short_sharpe']:.2f} | {elapsed:.0f}s\")\n",
    "\n",
    "best_latent, best_hidden, best_alpha = best_arch\n",
    "print(f\"\\nBest: latent={best_latent}, hidden={best_hidden}, alpha={best_alpha}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 by IC Sharpe:\n",
      " latent_scale  hidden_scale  news_alpha  ic_sharpe  long_sharpe  short_sharpe  ls_sharpe\n",
      "          1.0           0.5         0.8       4.54         1.76         -2.61      -2.29\n",
      "          1.0           0.5         0.6       4.15         1.71         -2.71      -2.31\n",
      "          1.0           1.0         0.8       3.83         2.45         -1.56      -0.94\n",
      "          0.5           0.5         0.8       3.69         2.62         -2.12      -1.52\n",
      "          2.0           1.0         0.6       3.67         2.16         -2.27      -1.78\n",
      "          0.5           1.0         0.6       3.62         3.11         -2.65      -2.04\n",
      "          1.0           1.5         1.0       3.59         2.44         -2.48      -1.60\n",
      "          1.0           0.5         1.0       3.59         3.57         -2.55      -1.80\n",
      "          0.5           1.0         0.8       3.53         2.46         -1.67      -1.20\n",
      "          2.0           1.5         1.0       3.52         2.11         -1.28      -0.75\n"
     ]
    }
   ],
   "source": [
    "# Show top results\n",
    "arch_df = pd.DataFrame(arch_results).sort_values(SELECTION_METRIC, ascending=False)\n",
    "print(\"\\nTop 10 by IC Sharpe:\")\n",
    "print(arch_df.head(10)[['latent_scale', 'hidden_scale', 'news_alpha', 'ic_sharpe', 'long_sharpe', 'short_sharpe', 'ls_sharpe']].round(2).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STAGE 2: TRAINING HYPERPARAMETERS\n",
      "--------------------------------------------------\n",
      "[ 1/27] lr=5e-04 wd=1e-04 s=0.05 | IC=3.80 Long=2.51\n",
      "[ 2/27] lr=5e-04 wd=1e-04 s=0.10 | IC=3.46 Long=2.44\n",
      "[ 3/27] lr=5e-04 wd=1e-04 s=0.15 | IC=3.34 Long=3.06\n",
      "[ 4/27] lr=5e-04 wd=1e-03 s=0.05 | IC=3.51 Long=3.33\n",
      "[ 5/27] lr=5e-04 wd=1e-03 s=0.10 | IC=3.39 Long=3.71\n",
      "[ 6/27] lr=5e-04 wd=1e-03 s=0.15 | IC=3.15 Long=2.41\n",
      "[ 7/27] lr=5e-04 wd=1e-02 s=0.05 | IC=3.01 Long=2.48\n",
      "[ 8/27] lr=5e-04 wd=1e-02 s=0.10 | IC=3.77 Long=1.67\n",
      "[ 9/27] lr=5e-04 wd=1e-02 s=0.15 | IC=3.78 Long=3.67\n",
      "[10/27] lr=1e-03 wd=1e-04 s=0.05 | IC=3.99 Long=2.58\n",
      "[11/27] lr=1e-03 wd=1e-04 s=0.10 | IC=2.33 Long=0.28\n",
      "[12/27] lr=1e-03 wd=1e-04 s=0.15 | IC=3.72 Long=2.02\n",
      "[13/27] lr=1e-03 wd=1e-03 s=0.05 | IC=3.22 Long=1.54\n",
      "[14/27] lr=1e-03 wd=1e-03 s=0.10 | IC=3.05 Long=3.36\n",
      "[15/27] lr=1e-03 wd=1e-03 s=0.15 | IC=3.13 Long=2.22\n",
      "[16/27] lr=1e-03 wd=1e-02 s=0.05 | IC=3.10 Long=2.32\n",
      "[17/27] lr=1e-03 wd=1e-02 s=0.10 | IC=2.96 Long=3.47\n",
      "[18/27] lr=1e-03 wd=1e-02 s=0.15 | IC=3.38 Long=3.47\n",
      "[19/27] lr=2e-03 wd=1e-04 s=0.05 | IC=2.72 Long=1.65\n",
      "[20/27] lr=2e-03 wd=1e-04 s=0.10 | IC=3.49 Long=1.72\n",
      "[21/27] lr=2e-03 wd=1e-04 s=0.15 | IC=3.58 Long=1.41\n",
      "[22/27] lr=2e-03 wd=1e-03 s=0.05 | IC=4.21 Long=2.76\n",
      "[23/27] lr=2e-03 wd=1e-03 s=0.10 | IC=3.05 Long=2.11\n",
      "[24/27] lr=2e-03 wd=1e-03 s=0.15 | IC=3.01 Long=2.67\n",
      "[25/27] lr=2e-03 wd=1e-02 s=0.05 | IC=3.31 Long=2.44\n",
      "[26/27] lr=2e-03 wd=1e-02 s=0.10 | IC=3.06 Long=2.26\n",
      "[27/27] lr=2e-03 wd=1e-02 s=0.15 | IC=2.68 Long=2.61\n",
      "\n",
      "Best: lr=2e-03, wd=1e-03, smooth=0.05\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTAGE 2: TRAINING HYPERPARAMETERS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "train_configs = list(product(\n",
    "    SEARCH_SPACE[\"learning_rate\"],\n",
    "    SEARCH_SPACE[\"weight_decay\"],\n",
    "    SEARCH_SPACE[\"label_smoothing\"],\n",
    "))\n",
    "\n",
    "train_results = []\n",
    "best_train_metric = -float('inf')\n",
    "best_train = None\n",
    "\n",
    "for i, (lr, wd, smoothing) in enumerate(train_configs):\n",
    "    config = ModelConfig(\n",
    "        n_fundamental_features=len(fund_feat_cols),\n",
    "        n_price_features=len(price_feat_cols),\n",
    "        n_embedding_dim=len(emb_cols),\n",
    "        fund_hidden=int(64 * best_hidden),\n",
    "        price_hidden=int(32 * best_hidden),\n",
    "        news_hidden=int(128 * best_hidden),\n",
    "        fundamental_latent=int(32 * best_latent),\n",
    "        price_latent=int(16 * best_latent),\n",
    "        news_latent=int(32 * best_latent),\n",
    "        news_alpha=best_alpha,\n",
    "        learning_rate=lr,\n",
    "        weight_decay=wd,\n",
    "        label_smoothing=smoothing,\n",
    "    )\n",
    "    \n",
    "    metrics, _ = train_and_evaluate(\n",
    "        config, train_dataset, val_df_news,\n",
    "        price_feat_cols, fund_feat_cols, emb_cols,\n",
    "        selection_metric=SELECTION_METRIC, n_epochs=15\n",
    "    )\n",
    "    \n",
    "    train_results.append({\"lr\": lr, \"wd\": wd, \"smooth\": smoothing, **metrics})\n",
    "    \n",
    "    if metrics[SELECTION_METRIC] > best_train_metric:\n",
    "        best_train_metric = metrics[SELECTION_METRIC]\n",
    "        best_train = (lr, wd, smoothing)\n",
    "    \n",
    "    print(f\"[{i+1:2d}/{len(train_configs)}] lr={lr:.0e} wd={wd:.0e} s={smoothing:.2f} | \"\n",
    "          f\"IC={metrics['ic_sharpe']:.2f} Long={metrics['long_sharpe']:.2f}\")\n",
    "\n",
    "best_lr, best_wd, best_smooth = best_train\n",
    "print(f\"\\nBest: lr={best_lr:.0e}, wd={best_wd:.0e}, smooth={best_smooth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Dropout Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STAGE 3: DROPOUT\n",
      "--------------------------------------------------\n",
      "[ 1/27] f=0.3 p=0.2 n=0.1 | IC=2.67 Long=1.21\n",
      "[ 2/27] f=0.3 p=0.2 n=0.2 | IC=2.88 Long=1.69\n",
      "[ 3/27] f=0.3 p=0.2 n=0.3 | IC=2.64 Long=1.46\n",
      "[ 4/27] f=0.3 p=0.3 n=0.1 | IC=2.79 Long=2.25\n",
      "[ 5/27] f=0.3 p=0.3 n=0.2 | IC=3.71 Long=2.17\n",
      "[ 6/27] f=0.3 p=0.3 n=0.3 | IC=3.23 Long=0.42\n",
      "[ 7/27] f=0.3 p=0.4 n=0.1 | IC=2.91 Long=2.04\n",
      "[ 8/27] f=0.3 p=0.4 n=0.2 | IC=2.92 Long=1.97\n",
      "[ 9/27] f=0.3 p=0.4 n=0.3 | IC=3.48 Long=2.19\n",
      "[10/27] f=0.5 p=0.2 n=0.1 | IC=3.24 Long=2.34\n",
      "[11/27] f=0.5 p=0.2 n=0.2 | IC=2.85 Long=3.24\n",
      "[12/27] f=0.5 p=0.2 n=0.3 | IC=3.05 Long=1.11\n",
      "[13/27] f=0.5 p=0.3 n=0.1 | IC=3.46 Long=2.28\n",
      "[14/27] f=0.5 p=0.3 n=0.2 | IC=3.04 Long=3.16\n",
      "[15/27] f=0.5 p=0.3 n=0.3 | IC=2.50 Long=2.00\n",
      "[16/27] f=0.5 p=0.4 n=0.1 | IC=3.10 Long=2.87\n",
      "[17/27] f=0.5 p=0.4 n=0.2 | IC=2.75 Long=1.17\n",
      "[18/27] f=0.5 p=0.4 n=0.3 | IC=2.39 Long=1.27\n",
      "[19/27] f=0.7 p=0.2 n=0.1 | IC=3.20 Long=2.99\n",
      "[20/27] f=0.7 p=0.2 n=0.2 | IC=3.83 Long=1.72\n",
      "[21/27] f=0.7 p=0.2 n=0.3 | IC=3.31 Long=2.09\n",
      "[22/27] f=0.7 p=0.3 n=0.1 | IC=3.17 Long=0.50\n",
      "[23/27] f=0.7 p=0.3 n=0.2 | IC=2.93 Long=2.00\n",
      "[24/27] f=0.7 p=0.3 n=0.3 | IC=3.40 Long=3.08\n",
      "[25/27] f=0.7 p=0.4 n=0.1 | IC=3.65 Long=-0.33\n",
      "[26/27] f=0.7 p=0.4 n=0.2 | IC=3.40 Long=2.81\n",
      "[27/27] f=0.7 p=0.4 n=0.3 | IC=3.40 Long=2.24\n",
      "\n",
      "Best: fund=0.7, price=0.2, news=0.2\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSTAGE 3: DROPOUT\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "dropout_configs = list(product(\n",
    "    SEARCH_SPACE[\"fund_dropout\"],\n",
    "    SEARCH_SPACE[\"price_dropout\"],\n",
    "    SEARCH_SPACE[\"news_dropout\"],\n",
    "))\n",
    "\n",
    "dropout_results = []\n",
    "best_dropout_metric = -float('inf')\n",
    "best_dropout = None\n",
    "best_model = None\n",
    "\n",
    "for i, (fund_do, price_do, news_do) in enumerate(dropout_configs):\n",
    "    config = ModelConfig(\n",
    "        n_fundamental_features=len(fund_feat_cols),\n",
    "        n_price_features=len(price_feat_cols),\n",
    "        n_embedding_dim=len(emb_cols),\n",
    "        fund_hidden=int(64 * best_hidden),\n",
    "        price_hidden=int(32 * best_hidden),\n",
    "        news_hidden=int(128 * best_hidden),\n",
    "        fundamental_latent=int(32 * best_latent),\n",
    "        price_latent=int(16 * best_latent),\n",
    "        news_latent=int(32 * best_latent),\n",
    "        news_alpha=best_alpha,\n",
    "        fundamental_dropout=fund_do,\n",
    "        price_dropout=price_do,\n",
    "        news_dropout=news_do,\n",
    "        learning_rate=best_lr,\n",
    "        weight_decay=best_wd,\n",
    "        label_smoothing=best_smooth,\n",
    "    )\n",
    "    \n",
    "    metrics, model = train_and_evaluate(\n",
    "        config, train_dataset, val_df_news,\n",
    "        price_feat_cols, fund_feat_cols, emb_cols,\n",
    "        selection_metric=SELECTION_METRIC, n_epochs=15\n",
    "    )\n",
    "    \n",
    "    dropout_results.append({\"fund\": fund_do, \"price\": price_do, \"news\": news_do, **metrics})\n",
    "    \n",
    "    if metrics[SELECTION_METRIC] > best_dropout_metric:\n",
    "        best_dropout_metric = metrics[SELECTION_METRIC]\n",
    "        best_dropout = (fund_do, price_do, news_do)\n",
    "        best_model = model\n",
    "    \n",
    "    print(f\"[{i+1:2d}/{len(dropout_configs)}] f={fund_do:.1f} p={price_do:.1f} n={news_do:.1f} | \"\n",
    "          f\"IC={metrics['ic_sharpe']:.2f} Long={metrics['long_sharpe']:.2f}\")\n",
    "\n",
    "best_fund_do, best_price_do, best_news_do = best_dropout\n",
    "print(f\"\\nBest: fund={best_fund_do}, price={best_price_do}, news={best_news_do}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING FINAL MODEL (25 epochs)\n",
      "======================================================================\n",
      "Config:\n",
      "  hidden=(32,16,64)\n",
      "  latent=(32,16,32)\n",
      "  dropout=(0.7,0.2,0.2)\n",
      "  lr=2e-03, wd=1e-03\n",
      "  Epoch 5: IC=1.96, Long=2.65, Short=-1.97\n",
      "  Epoch 10: IC=4.33, Long=1.03, Short=-1.93\n",
      "  Epoch 15: IC=3.13, Long=3.28, Short=-1.36\n",
      "  Epoch 20: IC=2.70, Long=3.49, Short=-1.27\n",
      "  Epoch 25: IC=2.52, Long=3.58, Short=-1.29\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTRAINING FINAL MODEL (25 epochs)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "final_config = ModelConfig(\n",
    "    n_fundamental_features=len(fund_feat_cols),\n",
    "    n_price_features=len(price_feat_cols),\n",
    "    n_embedding_dim=len(emb_cols),\n",
    "    fund_hidden=int(64 * best_hidden),\n",
    "    price_hidden=int(32 * best_hidden),\n",
    "    news_hidden=int(128 * best_hidden),\n",
    "    fundamental_latent=int(32 * best_latent),\n",
    "    price_latent=int(16 * best_latent),\n",
    "    news_latent=int(32 * best_latent),\n",
    "    news_alpha=best_alpha,\n",
    "    fundamental_dropout=best_fund_do,\n",
    "    price_dropout=best_price_do,\n",
    "    news_dropout=best_news_do,\n",
    "    learning_rate=best_lr,\n",
    "    weight_decay=best_wd,\n",
    "    label_smoothing=best_smooth,\n",
    "    n_epochs=25,\n",
    ")\n",
    "\n",
    "print(f\"Config:\")\n",
    "print(f\"  hidden=({final_config.fund_hidden},{final_config.price_hidden},{final_config.news_hidden})\")\n",
    "print(f\"  latent=({final_config.fundamental_latent},{final_config.price_latent},{final_config.news_latent})\")\n",
    "print(f\"  dropout=({final_config.fundamental_dropout},{final_config.price_dropout},{final_config.news_dropout})\")\n",
    "print(f\"  lr={final_config.learning_rate:.0e}, wd={final_config.weight_decay:.0e}\")\n",
    "\n",
    "final_metrics, final_model = train_and_evaluate(\n",
    "    final_config, train_dataset, val_df_news,\n",
    "    price_feat_cols, fund_feat_cols, emb_cols,\n",
    "    selection_metric=SELECTION_METRIC, n_epochs=25, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST SET RESULTS (LIQUID-TRAINED MODEL)\n",
      "============================================================\n",
      "IC Sharpe:    1.64\n",
      "Mean IC:      0.0106\n",
      "\n",
      "Long:  -32.6% ann, Sharpe -1.37\n",
      "Short: -147.2% ann, Sharpe -1.55\n",
      "L/S:   -89.9% ann, Sharpe -2.00\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_metrics = evaluate_model(final_model, test_df_news, price_feat_cols, fund_feat_cols, emb_cols, device)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEST SET RESULTS (LIQUID-TRAINED MODEL)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"IC Sharpe:    {test_metrics['ic_sharpe']:.2f}\")\n",
    "print(f\"Mean IC:      {test_metrics['mean_ic']:.4f}\")\n",
    "print(f\"\")\n",
    "print(f\"Long:  {test_metrics['long_return']*100:.1f}% ann, Sharpe {test_metrics['long_sharpe']:.2f}\")\n",
    "print(f\"Short: {test_metrics['short_return']*100:.1f}% ann, Sharpe {test_metrics['short_sharpe']:.2f}\")\n",
    "print(f\"L/S:   {test_metrics['ls_return']*100:.1f}% ann, Sharpe {test_metrics['ls_sharpe']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMPARISON: ORIGINAL vs LIQUID-TRAINED\n",
      "============================================================\n",
      "\n",
      "On liquid test set ($50M+ daily volume):\n",
      "\n",
      "Metric              Original  Liquid-Trained\n",
      "---------------------------------------------\n",
      "IC Sharpe               1.66            1.64\n",
      "Long Sharpe            -0.25           -1.37\n",
      "Short Sharpe           -0.72           -1.55\n",
      "Long Return            -5.8%          -32.6%\n",
      "Short Return          -73.4%         -147.2%\n"
     ]
    }
   ],
   "source": [
    "# Compare to original model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON: ORIGINAL vs LIQUID-TRAINED\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nOn liquid test set ($50M+ daily volume):\")\n",
    "print(\"\")\n",
    "print(f\"{'Metric':<15} {'Original':>12} {'Liquid-Trained':>15}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "if 'orig_score' in test_df_news.columns:\n",
    "    orig_ic_sharpe = np.mean(orig_ics)/np.std(orig_ics)*np.sqrt(252)\n",
    "    orig_long_sharpe = np.mean(orig_long)/np.std(orig_long)*np.sqrt(252)\n",
    "    orig_short_sharpe = np.mean(orig_short)/np.std(orig_short)*np.sqrt(252)\n",
    "    \n",
    "    print(f\"{'IC Sharpe':<15} {orig_ic_sharpe:>12.2f} {test_metrics['ic_sharpe']:>15.2f}\")\n",
    "    print(f\"{'Long Sharpe':<15} {orig_long_sharpe:>12.2f} {test_metrics['long_sharpe']:>15.2f}\")\n",
    "    print(f\"{'Short Sharpe':<15} {orig_short_sharpe:>12.2f} {test_metrics['short_sharpe']:>15.2f}\")\n",
    "    print(f\"{'Long Return':<15} {np.mean(orig_long)*252*100:>11.1f}% {test_metrics['long_return']*100:>14.1f}%\")\n",
    "    print(f\"{'Short Return':<15} {np.mean(orig_short)*252*100:>11.1f}% {test_metrics['short_return']*100:>14.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Cost-Adjusted Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COST-ADJUSTED RETURNS (1-day holding)\n",
      "============================================================\n",
      "Costs: Spread=0.10%, Impact=0.10% RT, Borrow=3.0% ann\n",
      "\n",
      "Long:  Gross -32.6% → Net -83.0%\n",
      "Short: Gross -147.2% → Net -200.6%\n",
      "L/S:   Net -141.8%\n",
      "\n",
      "Long Net Sharpe:  -3.49\n",
      "Short Net Sharpe: -2.11\n",
      "L/S Net Sharpe:   -3.15\n"
     ]
    }
   ],
   "source": [
    "# Simulate returns with costs for liquid-trained model\n",
    "SPREAD_COST = 0.001  # 10 bps round-trip\n",
    "IMPACT_COST = 0.0005  # 5 bps each way\n",
    "BORROW_COST_DAILY = 0.03 / 252  # 3% annual\n",
    "\n",
    "test_df_eval = test_df_news.copy()\n",
    "test_df_eval['score'] = get_scores(final_model, test_df_eval, price_feat_cols, fund_feat_cols, emb_cols, device)\n",
    "\n",
    "long_returns_gross = []\n",
    "short_returns_gross = []\n",
    "\n",
    "for date, group in test_df_eval.groupby('feature_date'):\n",
    "    if len(group) < 10:\n",
    "        continue\n",
    "    top_5 = group.nlargest(5, 'score')\n",
    "    bottom_5 = group.nsmallest(5, 'score')\n",
    "    \n",
    "    long_returns_gross.append(top_5['simple_return'].mean())\n",
    "    short_returns_gross.append(-bottom_5['simple_return'].mean())\n",
    "\n",
    "long_arr = np.array(long_returns_gross)\n",
    "short_arr = np.array(short_returns_gross)\n",
    "\n",
    "# Apply costs\n",
    "long_cost = SPREAD_COST + IMPACT_COST * 2  # Entry + exit\n",
    "short_cost = SPREAD_COST + IMPACT_COST * 2 + BORROW_COST_DAILY\n",
    "\n",
    "long_net = long_arr - long_cost\n",
    "short_net = short_arr - short_cost\n",
    "ls_net = (long_net + short_net) / 2\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COST-ADJUSTED RETURNS (1-day holding)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Costs: Spread={SPREAD_COST*100:.2f}%, Impact={IMPACT_COST*200:.2f}% RT, Borrow={BORROW_COST_DAILY*252*100:.1f}% ann\")\n",
    "print(\"\")\n",
    "print(f\"Long:  Gross {np.mean(long_arr)*252*100:.1f}% → Net {np.mean(long_net)*252*100:.1f}%\")\n",
    "print(f\"Short: Gross {np.mean(short_arr)*252*100:.1f}% → Net {np.mean(short_net)*252*100:.1f}%\")\n",
    "print(f\"L/S:   Net {np.mean(ls_net)*252*100:.1f}%\")\n",
    "print(\"\")\n",
    "print(f\"Long Net Sharpe:  {np.mean(long_net)/np.std(long_net)*np.sqrt(252):.2f}\")\n",
    "print(f\"Short Net Sharpe: {np.mean(short_net)/np.std(short_net)*np.sqrt(252):.2f}\")\n",
    "print(f\"L/S Net Sharpe:   {np.mean(ls_net)/np.std(ls_net)*np.sqrt(252):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to data/model_liquid_trained.pt\n"
     ]
    }
   ],
   "source": [
    "# Save liquid-trained model\n",
    "torch.save({\n",
    "    \"model_state_dict\": final_model.state_dict(),\n",
    "    \"config\": final_config,\n",
    "    \"price_cols\": price_feat_cols,\n",
    "    \"fund_cols\": fund_feat_cols,\n",
    "    \"emb_cols\": emb_cols,\n",
    "    \"liquid_threshold\": LIQUID_THRESHOLD,\n",
    "    \"test_metrics\": test_metrics,\n",
    "}, \"data/model_liquid_trained.pt\")\n",
    "\n",
    "print(\"Saved to data/model_liquid_trained.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARY\n",
      "============================================================\n",
      "\n",
      "Training universe: Liquid stocks (>= $50M daily volume)\n",
      "Training samples: 241,382\n",
      "\n",
      "Best config:\n",
      "  Architecture: latent=1.0, hidden=0.5, alpha=0.8\n",
      "  Training: lr=2e-03, wd=1e-03, smooth=0.05\n",
      "  Dropout: fund=0.7, price=0.2, news=0.2\n",
      "\n",
      "Test performance (liquid stocks):\n",
      "  IC Sharpe: 1.64\n",
      "  Long: -32.6% ann (Sharpe -1.37)\n",
      "  Short: -147.2% ann (Sharpe -1.55)\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTraining universe: Liquid stocks (>= ${LIQUID_THRESHOLD/1e6:.0f}M daily volume)\")\n",
    "print(f\"Training samples: {len(train_df_news):,}\")\n",
    "print(f\"\")\n",
    "print(f\"Best config:\")\n",
    "print(f\"  Architecture: latent={best_latent}, hidden={best_hidden}, alpha={best_alpha}\")\n",
    "print(f\"  Training: lr={best_lr:.0e}, wd={best_wd:.0e}, smooth={best_smooth}\")\n",
    "print(f\"  Dropout: fund={best_fund_do}, price={best_price_do}, news={best_news_do}\")\n",
    "print(f\"\")\n",
    "print(f\"Test performance (liquid stocks):\")\n",
    "print(f\"  IC Sharpe: {test_metrics['ic_sharpe']:.2f}\")\n",
    "print(f\"  Long: {test_metrics['long_return']*100:.1f}% ann (Sharpe {test_metrics['long_sharpe']:.2f})\")\n",
    "print(f\"  Short: {test_metrics['short_return']*100:.1f}% ann (Sharpe {test_metrics['short_sharpe']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
