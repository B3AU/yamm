{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2 Historical Earnings Move Analysis\n",
    "\n",
    "**Objective:** Analyze historical post-earnings price moves to understand:\n",
    "1. Distribution of |moves| by market cap bucket\n",
    "2. Historical volatility around earnings\n",
    "3. Baseline statistics for ML model comparison\n",
    "4. Which stocks have predictable vs unpredictable earnings reactions\n",
    "\n",
    "This provides the foundation for the ML model that will predict |move| quantiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DATA_DIR = Path('../data/earnings')\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Headers for Nasdaq API\n",
    "NASDAQ_HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Accept': 'application/json, text/plain, */*',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Origin': 'https://www.nasdaq.com',\n",
    "    'Referer': 'https://www.nasdaq.com/',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fetch Historical Earnings Calendar\n",
    "\n",
    "Get past earnings announcements from Nasdaq API (includes BMO/AMC timing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching earnings from 2021-01-08 to 2026-01-07...\n",
      "Total days to fetch: 1825\n",
      "This may take 20-30 minutes...\n",
      "\n",
      "  Progress: 30/1825 days (2%) - 976 records...\n",
      "  Progress: 60/1825 days (3%) - 2693 records...\n",
      "  Progress: 90/1825 days (5%) - 3439 records...\n",
      "  Progress: 120/1825 days (7%) - 5788 records...\n",
      "  Progress: 150/1825 days (8%) - 6965 records...\n",
      "  Progress: 180/1825 days (10%) - 7149 records...\n",
      "  Progress: 210/1825 days (12%) - 9460 records...\n",
      "  Progress: 240/1825 days (13%) - 10836 records...\n",
      "  Progress: 270/1825 days (15%) - 11041 records...\n",
      "  Progress: 300/1825 days (16%) - 12797 records...\n",
      "  Progress: 330/1825 days (18%) - 14844 records...\n",
      "  Progress: 360/1825 days (20%) - 15050 records...\n",
      "  Progress: 390/1825 days (21%) - 15682 records...\n",
      "  Progress: 420/1825 days (23%) - 17908 records...\n",
      "  Progress: 450/1825 days (25%) - 18892 records...\n",
      "  Progress: 480/1825 days (26%) - 20151 records...\n",
      "  Progress: 510/1825 days (28%) - 22832 records...\n",
      "  Progress: 540/1825 days (30%) - 23065 records...\n",
      "  Progress: 570/1825 days (31%) - 24188 records...\n",
      "  Progress: 600/1825 days (33%) - 26949 records...\n",
      "  Progress: 630/1825 days (35%) - 27218 records...\n",
      "  Progress: 660/1825 days (36%) - 28257 records...\n",
      "  Progress: 690/1825 days (38%) - 31106 records...\n",
      "  Progress: 720/1825 days (39%) - 31407 records...\n",
      "  Progress: 750/1825 days (41%) - 31917 records...\n",
      "  Progress: 780/1825 days (43%) - 33783 records...\n",
      "  Progress: 810/1825 days (44%) - 35240 records...\n",
      "  Progress: 840/1825 days (46%) - 36445 records...\n",
      "  Progress: 870/1825 days (48%) - 39458 records...\n",
      "  Progress: 900/1825 days (49%) - 39722 records...\n",
      "  Progress: 930/1825 days (51%) - 40443 records...\n",
      "  Progress: 960/1825 days (53%) - 43746 records...\n",
      "  Progress: 990/1825 days (54%) - 44045 records...\n",
      "  Progress: 1020/1825 days (56%) - 44508 records...\n",
      "  Progress: 1050/1825 days (58%) - 48140 records...\n",
      "  Progress: 1080/1825 days (59%) - 48473 records...\n",
      "  Progress: 1110/1825 days (61%) - 48626 records...\n",
      "  Progress: 1140/1825 days (62%) - 50542 records...\n",
      "  Progress: 1170/1825 days (64%) - 52289 records...\n",
      "  Progress: 1200/1825 days (66%) - 52935 records...\n",
      "  Progress: 1230/1825 days (67%) - 56736 records...\n",
      "  Progress: 1260/1825 days (69%) - 57146 records...\n",
      "  Progress: 1290/1825 days (71%) - 57417 records...\n",
      "  Progress: 1320/1825 days (72%) - 61224 records...\n",
      "  Progress: 1350/1825 days (74%) - 61641 records...\n",
      "  Progress: 1380/1825 days (76%) - 61926 records...\n",
      "  Progress: 1410/1825 days (77%) - 65745 records...\n",
      "  Progress: 1440/1825 days (79%) - 66231 records...\n",
      "  Progress: 1470/1825 days (81%) - 66389 records...\n",
      "  Progress: 1500/1825 days (82%) - 68038 records...\n",
      "  Progress: 1530/1825 days (84%) - 70003 records...\n",
      "  Progress: 1560/1825 days (85%) - 70813 records...\n",
      "  Progress: 1590/1825 days (87%) - 74772 records...\n",
      "  Progress: 1620/1825 days (89%) - 75244 records...\n",
      "  Progress: 1650/1825 days (90%) - 75422 records...\n",
      "  Progress: 1680/1825 days (92%) - 79472 records...\n",
      "  Progress: 1710/1825 days (94%) - 79941 records...\n",
      "  Progress: 1740/1825 days (95%) - 80110 records...\n",
      "  Progress: 1770/1825 days (97%) - 83680 records...\n",
      "  Progress: 1800/1825 days (99%) - 84713 records...\n",
      "\n",
      "Total earnings records: 84829\n"
     ]
    }
   ],
   "source": [
    "def fetch_earnings_calendar_nasdaq(from_date: datetime, to_date: datetime) -> pd.DataFrame:\n",
    "    \"\"\"Fetch earnings calendar from Nasdaq API (includes BMO/AMC timing).\n",
    "    \n",
    "    Includes retry logic and longer delays to handle rate limiting.\n",
    "    \"\"\"\n",
    "    all_rows = []\n",
    "    current_date = from_date\n",
    "    consecutive_errors = 0\n",
    "    max_consecutive_errors = 10\n",
    "    \n",
    "    total_days = (to_date - from_date).days\n",
    "    \n",
    "    while current_date <= to_date:\n",
    "        date_str = current_date.strftime('%Y-%m-%d')\n",
    "        url = f\"https://api.nasdaq.com/api/calendar/earnings?date={date_str}\"\n",
    "        \n",
    "        success = False\n",
    "        for attempt in range(3):  # Up to 3 retries per date\n",
    "            try:\n",
    "                r = requests.get(url, headers=NASDAQ_HEADERS, timeout=15)\n",
    "                if r.status_code == 200:\n",
    "                    data = r.json()\n",
    "                    rows = data.get('data', {}).get('rows', [])\n",
    "                    if rows:\n",
    "                        for row in rows:\n",
    "                            row['date'] = date_str\n",
    "                        all_rows.extend(rows)\n",
    "                    success = True\n",
    "                    consecutive_errors = 0\n",
    "                    break\n",
    "                elif r.status_code == 429:  # Rate limited\n",
    "                    print(f\"  Rate limited, waiting 5s...\")\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    print(f\"  {date_str}: HTTP {r.status_code}\")\n",
    "                    break\n",
    "            except requests.exceptions.Timeout:\n",
    "                print(f\"  {date_str}: Timeout (attempt {attempt+1}/3)\")\n",
    "                time.sleep(2)\n",
    "            except requests.exceptions.ConnectionError as e:\n",
    "                print(f\"  {date_str}: Connection error (attempt {attempt+1}/3)\")\n",
    "                time.sleep(3)\n",
    "            except Exception as e:\n",
    "                print(f\"  {date_str}: {type(e).__name__}: {e}\")\n",
    "                break\n",
    "        \n",
    "        if not success:\n",
    "            consecutive_errors += 1\n",
    "            if consecutive_errors >= max_consecutive_errors:\n",
    "                print(f\"\\n  WARNING: {max_consecutive_errors} consecutive errors, stopping early\")\n",
    "                break\n",
    "        \n",
    "        current_date += timedelta(days=1)\n",
    "        time.sleep(0.15)  # Slightly longer delay between requests\n",
    "        \n",
    "        # Progress indicator every 30 days\n",
    "        days_done = (current_date - from_date).days\n",
    "        if days_done % 30 == 0:\n",
    "            pct = days_done / total_days * 100\n",
    "            print(f\"  Progress: {days_done}/{total_days} days ({pct:.0f}%) - {len(all_rows)} records...\")\n",
    "    \n",
    "    return pd.DataFrame(all_rows)\n",
    "\n",
    "# Fetch 5 years of earnings (20 quarters) to align with prices.pqt data range\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=1825)  # ~5 years\n",
    "\n",
    "print(f\"Fetching earnings from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}...\")\n",
    "print(f\"Total days to fetch: {(end_date - start_date).days}\")\n",
    "print(\"This may take 20-30 minutes...\\n\")\n",
    "\n",
    "earnings_df = fetch_earnings_calendar_nasdaq(start_date, end_date)\n",
    "print(f\"\\nTotal earnings records: {len(earnings_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US earnings (historical): 84381\n",
      "Unique symbols: 4864\n",
      "\n",
      "Date range: 2021-01-08 00:00:00 to 2026-01-07 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Filter to US stocks only (no dots/dashes in symbol)\n",
    "us_earnings = earnings_df[\n",
    "    ~earnings_df['symbol'].str.contains(r'[.-]', regex=True, na=False)\n",
    "].copy()\n",
    "\n",
    "# Parse dates\n",
    "us_earnings['date'] = pd.to_datetime(us_earnings['date'])\n",
    "\n",
    "# Keep only past earnings\n",
    "us_earnings = us_earnings[us_earnings['date'] < datetime.now()]\n",
    "\n",
    "# Parse timing from Nasdaq format\n",
    "def parse_timing(time_str):\n",
    "    if pd.isna(time_str):\n",
    "        return 'unknown'\n",
    "    time_str = str(time_str).lower()\n",
    "    if 'pre-market' in time_str or 'before' in time_str:\n",
    "        return 'BMO'\n",
    "    elif 'after-hours' in time_str or 'after' in time_str:\n",
    "        return 'AMC'\n",
    "    return 'unknown'\n",
    "\n",
    "us_earnings['timing'] = us_earnings['time'].apply(parse_timing)\n",
    "\n",
    "print(f\"US earnings (historical): {len(us_earnings)}\")\n",
    "print(f\"Unique symbols: {us_earnings['symbol'].nunique()}\")\n",
    "print(f\"\\nDate range: {us_earnings['date'].min()} to {us_earnings['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing distribution:\n",
      "timing\n",
      "unknown    84365\n",
      "AMC           11\n",
      "BMO            5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "BMO/AMC coverage: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Check BMO/AMC timing distribution (Nasdaq provides this!)\n",
    "print(\"Timing distribution:\")\n",
    "print(us_earnings['timing'].value_counts())\n",
    "print(f\"\\nBMO/AMC coverage: {(us_earnings['timing'] != 'unknown').mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fetch Historical Prices for Earnings Stocks\n",
    "\n",
    "For each stock with earnings, get prices around the earnings date to compute realized moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use existing prices.pqt - no need to fetch from API\n",
    "# The prices were previously fetched and saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stocks with 4+ earnings in period: 4651\n",
      "Sample: ['A', 'AA', 'AACG', 'AAL', 'AAME', 'AAMI', 'AAOI', 'AAON', 'AAP', 'AAPL', 'AAT', 'AB', 'ABAT', 'ABBV', 'ABCB', 'ABCL', 'ABEO', 'ABEV', 'ABG', 'ABM']\n"
     ]
    }
   ],
   "source": [
    "# Sample stocks with multiple earnings in our period\n",
    "# Focus on stocks with >= 4 earnings (1+ year of data)\n",
    "earnings_counts = us_earnings.groupby('symbol').size()\n",
    "frequent_earners = earnings_counts[earnings_counts >= 4].index.tolist()\n",
    "\n",
    "print(f\"Stocks with 4+ earnings in period: {len(frequent_earners)}\")\n",
    "print(f\"Sample: {frequent_earners[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# UPDATED: Use existing prices.pqt instead of fetching from API\n# This gives us much more coverage (5,644 symbols vs 200 sample)\n\nprices_file = Path('../data/prices.pqt')\n\nif prices_file.exists():\n    print(\"Loading existing prices.pqt...\")\n    all_prices = pd.read_parquet(prices_file)\n    all_prices['date'] = pd.to_datetime(all_prices['date'])\n    \n    # Get symbols that are in both prices and earnings\n    price_symbols = set(all_prices['symbol'].unique())\n    earning_symbols = set(us_earnings['symbol'].unique())\n    common_symbols = price_symbols & earning_symbols\n    \n    print(f\"Prices: {len(price_symbols):,} symbols\")\n    print(f\"Earnings: {len(earning_symbols):,} symbols\")\n    print(f\"Common: {len(common_symbols):,} symbols\")\n    \n    # Build price cache using groupby (MUCH faster than loop filtering)\n    print(\"Building price cache...\")\n    all_prices_sorted = all_prices.sort_values(['symbol', 'date'])\n    \n    # Filter to common symbols first, then groupby\n    common_prices = all_prices_sorted[all_prices_sorted['symbol'].isin(common_symbols)]\n    price_cache = {symbol: group for symbol, group in common_prices.groupby('symbol')}\n    \n    print(f\"Built price cache for {len(price_cache)} symbols\")\nelse:\n    print(\"prices.pqt not found, falling back to API fetch...\")\n    # Original API fetch code (limited to 200 symbols)\n    sample_size = 200\n    sample_symbols = frequent_earners[:sample_size]\n    \n    print(f\"Fetching prices for {len(sample_symbols)} symbols...\")\n    \n    price_cache = {}\n    for i, symbol in enumerate(sample_symbols):\n        if i > 0 and i % 20 == 0:\n            print(f\"  Progress: {i}/{len(sample_symbols)}\")\n        \n        df = fetch_historical_prices(\n            symbol,\n            start_date.strftime('%Y-%m-%d'),\n            end_date.strftime('%Y-%m-%d')\n        )\n        if not df.empty:\n            price_cache[symbol] = df\n        time.sleep(0.15)\n    \n    print(f\"\\nGot prices for {len(price_cache)} symbols\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute Earnings Moves\n",
    "\n",
    "For each earnings event, compute:\n",
    "- **Gap move:** |Close_T-1 → Open_T| (pure earnings reaction)\n",
    "- **Full move:** |Close_T-1 → Close_T| (includes intraday)\n",
    "- **Overnight hold move:** |Close_T-1 → Close_T+1| (matches our exit strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_earnings_moves(symbol: str, earnings_dates: list, prices_df: pd.DataFrame) -> list:\n",
    "    \"\"\"Compute moves around each earnings date.\"\"\"\n",
    "    moves = []\n",
    "    \n",
    "    prices_df = prices_df.set_index('date').sort_index()\n",
    "    \n",
    "    for earn_date in earnings_dates:\n",
    "        earn_date = pd.to_datetime(earn_date)\n",
    "        \n",
    "        try:\n",
    "            # Find T-1 (day before earnings)\n",
    "            t_minus_1_candidates = prices_df[prices_df.index < earn_date].tail(1)\n",
    "            if t_minus_1_candidates.empty:\n",
    "                continue\n",
    "            t_minus_1 = t_minus_1_candidates.index[0]\n",
    "            \n",
    "            # Find T (earnings day or next trading day)\n",
    "            t_candidates = prices_df[prices_df.index >= earn_date].head(1)\n",
    "            if t_candidates.empty:\n",
    "                continue\n",
    "            t = t_candidates.index[0]\n",
    "            \n",
    "            # Find T+1 (day after earnings reaction)\n",
    "            t_plus_1_candidates = prices_df[prices_df.index > t].head(1)\n",
    "            if t_plus_1_candidates.empty:\n",
    "                continue\n",
    "            t_plus_1 = t_plus_1_candidates.index[0]\n",
    "            \n",
    "            # Get prices\n",
    "            close_t_minus_1 = prices_df.loc[t_minus_1, 'close']\n",
    "            open_t = prices_df.loc[t, 'open']\n",
    "            close_t = prices_df.loc[t, 'close']\n",
    "            close_t_plus_1 = prices_df.loc[t_plus_1, 'close']\n",
    "            \n",
    "            # Compute moves\n",
    "            gap_move = (open_t - close_t_minus_1) / close_t_minus_1\n",
    "            full_move = (close_t - close_t_minus_1) / close_t_minus_1\n",
    "            overnight_move = (close_t_plus_1 - close_t_minus_1) / close_t_minus_1\n",
    "            \n",
    "            moves.append({\n",
    "                'symbol': symbol,\n",
    "                'earnings_date': earn_date,\n",
    "                'close_t_minus_1': close_t_minus_1,\n",
    "                'open_t': open_t,\n",
    "                'close_t': close_t,\n",
    "                'close_t_plus_1': close_t_plus_1,\n",
    "                'gap_move': gap_move,\n",
    "                'gap_move_abs': abs(gap_move),\n",
    "                'full_move': full_move,\n",
    "                'full_move_abs': abs(full_move),\n",
    "                'overnight_move': overnight_move,\n",
    "                'overnight_move_abs': abs(overnight_move),\n",
    "            })\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    return moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute moves for all symbols with prices\n",
    "all_moves = []\n",
    "\n",
    "for symbol in price_cache:\n",
    "    # Get earnings dates for this symbol\n",
    "    symbol_earnings = us_earnings[us_earnings['symbol'] == symbol]['date'].tolist()\n",
    "    \n",
    "    moves = compute_earnings_moves(symbol, symbol_earnings, price_cache[symbol])\n",
    "    all_moves.extend(moves)\n",
    "\n",
    "moves_df = pd.DataFrame(all_moves)\n",
    "print(f\"Computed {len(moves_df)} earnings moves\")\n",
    "print(f\"Unique symbols: {moves_df['symbol'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later use\n",
    "moves_df.to_parquet(DATA_DIR / 'historical_earnings_moves.parquet', index=False)\n",
    "print(f\"Saved to {DATA_DIR / 'historical_earnings_moves.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Move Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall distribution statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"EARNINGS MOVE DISTRIBUTION (Absolute Values)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for move_type in ['gap_move_abs', 'full_move_abs', 'overnight_move_abs']:\n",
    "    print(f\"\\n{move_type.replace('_abs', '').replace('_', ' ').title()}:\")\n",
    "    data = moves_df[move_type] * 100  # Convert to percentage\n",
    "    print(f\"  Mean:   {data.mean():.2f}%\")\n",
    "    print(f\"  Median: {data.median():.2f}%\")\n",
    "    print(f\"  Std:    {data.std():.2f}%\")\n",
    "    print(f\"  Q75:    {data.quantile(0.75):.2f}%\")\n",
    "    print(f\"  Q90:    {data.quantile(0.90):.2f}%\")\n",
    "    print(f\"  Q95:    {data.quantile(0.95):.2f}%\")\n",
    "    print(f\"  Max:    {data.max():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution by price bucket (proxy for market cap)\n",
    "moves_df['price_bucket'] = pd.cut(\n",
    "    moves_df['close_t_minus_1'],\n",
    "    bins=[0, 20, 50, 100, 200, 500, float('inf')],\n",
    "    labels=['<$20', '$20-50', '$50-100', '$100-200', '$200-500', '>$500']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OVERNIGHT MOVE BY PRICE BUCKET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bucket_stats = moves_df.groupby('price_bucket')['overnight_move_abs'].agg([\n",
    "    'count',\n",
    "    'mean',\n",
    "    'median',\n",
    "    ('q75', lambda x: x.quantile(0.75)),\n",
    "    ('q90', lambda x: x.quantile(0.90)),\n",
    "]) * 100  # Convert to percentage (except count)\n",
    "\n",
    "bucket_stats['count'] = bucket_stats['count'] / 100  # Fix count\n",
    "bucket_stats.columns = ['Count', 'Mean %', 'Median %', 'Q75 %', 'Q90 %']\n",
    "print(bucket_stats.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    for i, move_type in enumerate(['gap_move_abs', 'full_move_abs', 'overnight_move_abs']):\n",
    "        ax = axes[i]\n",
    "        data = moves_df[move_type] * 100\n",
    "        data = data[data < 30]  # Clip outliers for visualization\n",
    "        ax.hist(data, bins=50, edgecolor='black', alpha=0.7)\n",
    "        ax.axvline(data.median(), color='red', linestyle='--', label=f'Median: {data.median():.1f}%')\n",
    "        ax.axvline(data.quantile(0.75), color='orange', linestyle='--', label=f'Q75: {data.quantile(0.75):.1f}%')\n",
    "        ax.set_xlabel('|Move| %')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_title(move_type.replace('_abs', '').replace('_', ' ').title())\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(DATA_DIR / 'earnings_move_distributions.png', dpi=100)\n",
    "    plt.show()\n",
    "    print(f\"Saved plot to {DATA_DIR / 'earnings_move_distributions.png'}\")\n",
    "except ImportError:\n",
    "    print(\"matplotlib not available - skipping plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stock-Level Analysis\n",
    "\n",
    "Which stocks have predictable vs volatile earnings reactions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-stock statistics\n",
    "stock_stats = moves_df.groupby('symbol').agg({\n",
    "    'overnight_move_abs': ['count', 'mean', 'std', 'median'],\n",
    "    'close_t_minus_1': 'last',  # Most recent price\n",
    "}).round(4)\n",
    "\n",
    "stock_stats.columns = ['earnings_count', 'mean_move', 'std_move', 'median_move', 'last_price']\n",
    "stock_stats = stock_stats[stock_stats['earnings_count'] >= 4]  # At least 4 earnings\n",
    "\n",
    "# Compute coefficient of variation (std/mean) - lower = more predictable\n",
    "stock_stats['cv'] = stock_stats['std_move'] / stock_stats['mean_move']\n",
    "\n",
    "print(f\"Stocks with 4+ earnings: {len(stock_stats)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most volatile (largest average moves)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MOST VOLATILE EARNINGS (Largest Average |Move|)\")\n",
    "print(\"=\" * 60)\n",
    "volatile = stock_stats.nlargest(15, 'mean_move').copy()\n",
    "volatile['mean_move_pct'] = volatile['mean_move'] * 100\n",
    "volatile['median_move_pct'] = volatile['median_move'] * 100\n",
    "print(volatile[['earnings_count', 'mean_move_pct', 'median_move_pct', 'last_price']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most predictable (lowest coefficient of variation among decent movers)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MOST PREDICTABLE EARNINGS (Low CV, Decent Moves)\")\n",
    "print(\"=\" * 60)\n",
    "# Filter to stocks with at least 3% average move (interesting for trading)\n",
    "decent_movers = stock_stats[stock_stats['mean_move'] >= 0.03]\n",
    "predictable = decent_movers.nsmallest(15, 'cv').copy()\n",
    "predictable['mean_move_pct'] = predictable['mean_move'] * 100\n",
    "print(predictable[['earnings_count', 'mean_move_pct', 'cv', 'last_price']].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save stock stats\n",
    "stock_stats.to_parquet(DATA_DIR / 'stock_earnings_stats.parquet')\n",
    "print(f\"Saved stock stats to {DATA_DIR / 'stock_earnings_stats.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implied Move Baseline\n",
    "\n",
    "In the absence of historical options data, estimate what implied moves might look like.\n",
    "\n",
    "Typical ATM straddle pricing = ~1.2-1.5x expected |move|, so:\n",
    "- If historical mean |move| = 5%, implied move ≈ 6-7.5%\n",
    "- If historical mean |move| = 10%, implied move ≈ 12-15%\n",
    "\n",
    "The edge comes from correctly predicting the tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate implied moves and potential edge\n",
    "# Assumption: Market prices in ~1.3x historical mean (rough heuristic)\n",
    "IMPLIED_MULTIPLIER = 1.3\n",
    "\n",
    "stock_stats['estimated_implied'] = stock_stats['mean_move'] * IMPLIED_MULTIPLIER\n",
    "\n",
    "# Potential edge = q75 - estimated_implied\n",
    "# (If we can predict q75 will happen, and market only prices in mean*1.3)\n",
    "\n",
    "# First need to compute q75 per stock\n",
    "q75_by_stock = moves_df.groupby('symbol')['overnight_move_abs'].quantile(0.75)\n",
    "stock_stats['q75_move'] = q75_by_stock\n",
    "stock_stats['potential_edge'] = stock_stats['q75_move'] - stock_stats['estimated_implied']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"POTENTIAL EDGE (Q75 vs Estimated Implied)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Stocks where q75 exceeds estimated implied (potential long vol edge)\n",
    "edge_stocks = stock_stats[stock_stats['potential_edge'] > 0.01].copy()  # >1% edge\n",
    "edge_stocks['edge_pct'] = edge_stocks['potential_edge'] * 100\n",
    "edge_stocks['q75_pct'] = edge_stocks['q75_move'] * 100\n",
    "edge_stocks['implied_pct'] = edge_stocks['estimated_implied'] * 100\n",
    "\n",
    "print(f\"\\nStocks with >1% potential edge: {len(edge_stocks)}\")\n",
    "print(\"\\nTop 15 by potential edge:\")\n",
    "print(edge_stocks.nlargest(15, 'potential_edge')[['earnings_count', 'q75_pct', 'implied_pct', 'edge_pct', 'last_price']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"HISTORICAL EARNINGS MOVE ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\"\"\n",
    "Data Collected:\n",
    "  - Earnings events analyzed: {len(moves_df)}\n",
    "  - Unique stocks: {moves_df['symbol'].nunique()}\n",
    "  - Date range: {moves_df['earnings_date'].min()} to {moves_df['earnings_date'].max()}\n",
    "\n",
    "Overnight Move Distribution (|Close_T-1 → Close_T+1|):\n",
    "  - Mean: {moves_df['overnight_move_abs'].mean()*100:.2f}%\n",
    "  - Median: {moves_df['overnight_move_abs'].median()*100:.2f}%\n",
    "  - Q75: {moves_df['overnight_move_abs'].quantile(0.75)*100:.2f}%\n",
    "  - Q90: {moves_df['overnight_move_abs'].quantile(0.90)*100:.2f}%\n",
    "  - Q95: {moves_df['overnight_move_abs'].quantile(0.95)*100:.2f}%\n",
    "\n",
    "Key Findings:\n",
    "  1. Average earnings move is ~{moves_df['overnight_move_abs'].mean()*100:.1f}% (overnight hold)\n",
    "  2. ~25% of earnings moves exceed {moves_df['overnight_move_abs'].quantile(0.75)*100:.1f}% (q75)\n",
    "  3. ~10% of earnings moves exceed {moves_df['overnight_move_abs'].quantile(0.90)*100:.1f}% (q90)\n",
    "  4. Lower-priced stocks tend to have larger moves\n",
    "  5. Some stocks have predictable move magnitude (low CV)\n",
    "\n",
    "Files Saved:\n",
    "  - {DATA_DIR / 'historical_earnings_moves.parquet'}\n",
    "  - {DATA_DIR / 'stock_earnings_stats.parquet'}\n",
    "\n",
    "Next Steps:\n",
    "  1. Add more features (volatility regime, sector, etc.) for ML model\n",
    "  2. Compare to actual implied moves when we have options data\n",
    "  3. Build quantile regression model to predict q50/q75/q90\n",
    "  4. Validate calibration on held-out data\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}