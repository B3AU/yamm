{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.2c Infer BMO/AMC Timing from Price Data\n",
    "\n",
    "**Problem:** Historical earnings data from Nasdaq has ~0% BMO/AMC timing coverage because Nasdaq only provides timing for *upcoming* earnings, not historical.\n",
    "\n",
    "**Solution:** Infer timing by comparing overnight gap magnitudes:\n",
    "- **BMO (Before Market Open):** Large gap on earnings day T\n",
    "- **AMC (After Market Close):** Large gap on day T+1\n",
    "\n",
    "**Why this matters:** Without timing, ~50% of training data has misaligned price moves - we're computing moves on the wrong days for AMC earnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from datetime import datetime, timedelta, date\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_DIR = Path('../data/earnings')\n",
    "\n",
    "# Nasdaq headers\n",
    "NASDAQ_HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "    'Accept': 'application/json',\n",
    "    'Origin': 'https://www.nasdaq.com',\n",
    "    'Referer': 'https://www.nasdaq.com/',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters (can be overridden by papermill)\n",
    "USE_CACHE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Historical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load historical earnings moves (has prices but no timing)\nmoves_df = pd.read_parquet(DATA_DIR / 'historical_earnings_moves.parquet')\n\n# Phase 3.1: Deduplicate earnings dates (21 duplicate pairs found in review)\n# Keep first occurrence for each (symbol, earnings_date) pair\noriginal_len = len(moves_df)\nmoves_df = moves_df.drop_duplicates(subset=['symbol', 'earnings_date'], keep='first')\nif len(moves_df) < original_len:\n    print(f\"Removed {original_len - len(moves_df)} duplicate (symbol, earnings_date) pairs\")\n\n# Phase 3.2: Remove outliers\n# - ADTX has extreme prices (>$394M) due to reverse split data issues\n# - Records with >100% moves are likely data errors\n# - Weekend earnings dates are data errors\nmoves_df['earnings_date'] = pd.to_datetime(moves_df['earnings_date'])\n\n# Remove ADTX\nadtx_count = (moves_df['symbol'] == 'ADTX').sum()\nif adtx_count > 0:\n    moves_df = moves_df[moves_df['symbol'] != 'ADTX']\n    print(f\"Removed {adtx_count} ADTX records (bad price data from reverse split)\")\n\n# Remove >100% overnight moves (data errors)\nif 'overnight_move_abs' in moves_df.columns:\n    extreme_moves = moves_df['overnight_move_abs'] > 1.0\n    if extreme_moves.sum() > 0:\n        print(f\"Removed {extreme_moves.sum()} records with >100% moves\")\n        moves_df = moves_df[~extreme_moves]\n\n# Remove weekend earnings dates (data errors)\nweekend_mask = moves_df['earnings_date'].dt.dayofweek >= 5\nif weekend_mask.sum() > 0:\n    print(f\"Removed {weekend_mask.sum()} records with weekend earnings dates\")\n    moves_df = moves_df[~weekend_mask]\n\nprint(f\"\\nHistorical earnings records: {len(moves_df):,}\")\nprint(f\"Columns: {moves_df.columns.tolist()}\")\nprint(f\"\\nDate range: {moves_df['earnings_date'].min()} to {moves_df['earnings_date'].max()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prices for computing T+1 gaps (needed for AMC detection)\n",
    "prices_df = pd.read_parquet('../data/prices.pqt')\n",
    "prices_df['date'] = pd.to_datetime(prices_df['date'])\n",
    "print(f\"Prices: {len(prices_df):,} rows, {prices_df['symbol'].nunique():,} symbols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what price columns we have\n",
    "print(\"Price columns:\", prices_df.columns.tolist())\n",
    "prices_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute Gap Ratios\n",
    "\n",
    "For each earnings event, compute:\n",
    "- `gap_T`: Overnight gap on earnings day (Close_T-1 -> Open_T)\n",
    "- `gap_T1`: Overnight gap on day after (Close_T -> Open_T+1)\n",
    "- `gap_ratio`: gap_T / gap_T1 (>1 suggests BMO, <1 suggests AMC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gap_t1(symbol: str, earnings_date, prices_cache: dict) -> float:\n",
    "    \"\"\"Compute the overnight gap on T+1 (Open_T+1 - Close_T) / Close_T.\n",
    "    \n",
    "    This is the AMC reaction gap.\n",
    "    \"\"\"\n",
    "    if symbol not in prices_cache:\n",
    "        return np.nan\n",
    "    \n",
    "    pdf = prices_cache[symbol]\n",
    "    earn_date = pd.to_datetime(earnings_date)\n",
    "    \n",
    "    try:\n",
    "        # Find T (earnings day or next trading day)\n",
    "        t_candidates = pdf[pdf.index >= earn_date].head(1)\n",
    "        if t_candidates.empty:\n",
    "            return np.nan\n",
    "        t = t_candidates.index[0]\n",
    "        \n",
    "        # Find T+1\n",
    "        t1_candidates = pdf[pdf.index > t].head(1)\n",
    "        if t1_candidates.empty:\n",
    "            return np.nan\n",
    "        t1 = t1_candidates.index[0]\n",
    "        \n",
    "        close_t = pdf.loc[t, 'close']\n",
    "        open_t1 = pdf.loc[t1, 'open']\n",
    "        \n",
    "        gap_t1 = abs(open_t1 - close_t) / close_t\n",
    "        return gap_t1\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "# Build price cache (indexed by date for faster lookup)\n",
    "print(\"Building price cache...\")\n",
    "prices_df_sorted = prices_df.sort_values(['symbol', 'date'])\n",
    "price_cache = {}\n",
    "for symbol, group in prices_df_sorted.groupby('symbol'):\n",
    "    price_cache[symbol] = group.set_index('date')\n",
    "print(f\"Price cache: {len(price_cache)} symbols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gap on T from existing data (already have open_t and close_t_minus_1)\n",
    "moves_df['gap_t'] = abs(moves_df['open_t'] - moves_df['close_t_minus_1']) / moves_df['close_t_minus_1']\n",
    "\n",
    "# Compute gap on T+1 (need to look up prices)\n",
    "print(\"Computing T+1 gaps (this may take a minute)...\")\n",
    "gap_t1_values = []\n",
    "for idx, row in moves_df.iterrows():\n",
    "    gap_t1 = compute_gap_t1(row['symbol'], row['earnings_date'], price_cache)\n",
    "    gap_t1_values.append(gap_t1)\n",
    "    \n",
    "    if len(gap_t1_values) % 10000 == 0:\n",
    "        print(f\"  Progress: {len(gap_t1_values):,}/{len(moves_df):,}\")\n",
    "\n",
    "moves_df['gap_t1'] = gap_t1_values\n",
    "print(f\"\\nComputed {len(gap_t1_values):,} T+1 gaps\")\n",
    "print(f\"Missing T+1 gaps: {moves_df['gap_t1'].isna().sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute gap ratio\n# Phase 3.4: Use larger epsilon (0.001) to avoid extreme ratios\n# Previously eps=1e-6 could create gap_ratio > 1,000,000 for tiny gap_t1 values\neps = 0.001\nmoves_df['gap_ratio'] = moves_df['gap_t'] / (moves_df['gap_t1'] + eps)\n\n# Also filter out extreme gap ratios (likely data issues or tiny movements)\n# Cap gap_ratio to reasonable range for classification\nmoves_df['gap_ratio'] = moves_df['gap_ratio'].clip(upper=100)\n\n# Show distribution\nvalid_ratios = moves_df['gap_ratio'].dropna()\nprint(\"Gap ratio distribution (gap_T / gap_T+1):\")\nprint(f\"  Mean: {valid_ratios.mean():.2f}\")\nprint(f\"  Median: {valid_ratios.median():.2f}\")\nprint(f\"  25th percentile: {valid_ratios.quantile(0.25):.2f}\")\nprint(f\"  75th percentile: {valid_ratios.quantile(0.75):.2f}\")\nprint(f\"\\n  >2.0 (likely BMO): {(valid_ratios > 2.0).sum():,} ({(valid_ratios > 2.0).mean()*100:.1f}%)\")\nprint(f\"  <0.5 (likely AMC): {(valid_ratios < 0.5).sum():,} ({(valid_ratios < 0.5).mean()*100:.1f}%)\")\nprint(f\"  0.5-2.0 (ambiguous): {((valid_ratios >= 0.5) & (valid_ratios <= 2.0)).sum():,}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gap ratio distribution\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Log-scale histogram\n",
    "    ax = axes[0]\n",
    "    log_ratios = np.log10(valid_ratios.clip(0.01, 100))\n",
    "    ax.hist(log_ratios, bins=100, edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(np.log10(2.0), color='green', linestyle='--', label='BMO threshold (ratio>2)')\n",
    "    ax.axvline(np.log10(0.5), color='red', linestyle='--', label='AMC threshold (ratio<0.5)')\n",
    "    ax.set_xlabel('log10(gap_ratio)')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title('Gap Ratio Distribution (log scale)')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Scatter: gap_T vs gap_T1\n",
    "    ax = axes[1]\n",
    "    sample = moves_df.dropna(subset=['gap_t', 'gap_t1']).sample(min(5000, len(moves_df)))\n",
    "    ax.scatter(sample['gap_t']*100, sample['gap_t1']*100, alpha=0.3, s=10)\n",
    "    ax.plot([0, 20], [0, 20], 'k--', label='Equal gaps')\n",
    "    ax.set_xlabel('Gap on T (%) - BMO reaction')\n",
    "    ax.set_ylabel('Gap on T+1 (%) - AMC reaction')\n",
    "    ax.set_title('Gap T vs Gap T+1')\n",
    "    ax.set_xlim(0, 20)\n",
    "    ax.set_ylim(0, 20)\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"matplotlib not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classify Timing Based on Gap Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_timing(gap_ratio: float, bmo_threshold: float = 2.0, amc_threshold: float = 0.5) -> str:\n",
    "    \"\"\"Infer BMO/AMC timing from gap ratio.\n",
    "    \n",
    "    gap_ratio = gap_T / gap_T1\n",
    "    - High ratio (>bmo_threshold): Gap on T is much larger -> BMO\n",
    "    - Low ratio (<amc_threshold): Gap on T+1 is much larger -> AMC\n",
    "    - Middle: Ambiguous\n",
    "    \"\"\"\n",
    "    if pd.isna(gap_ratio):\n",
    "        return 'unknown'\n",
    "    if gap_ratio > bmo_threshold:\n",
    "        return 'BMO'\n",
    "    elif gap_ratio < amc_threshold:\n",
    "        return 'AMC'\n",
    "    else:\n",
    "        return 'unknown'\n",
    "\n",
    "# Apply inference\n",
    "moves_df['inferred_timing'] = moves_df['gap_ratio'].apply(infer_timing)\n",
    "\n",
    "print(\"Inferred timing distribution:\")\n",
    "print(moves_df['inferred_timing'].value_counts())\n",
    "print(f\"\\nBMO/AMC coverage: {(moves_df['inferred_timing'] != 'unknown').mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validate Against Known Timings\n",
    "\n",
    "Fetch recent upcoming earnings (which have timing) and validate our inference algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def fetch_upcoming_earnings_with_timing(days_ahead: int = 30) -> pd.DataFrame:\n",
    "    \"\"\"Fetch upcoming earnings from Nasdaq (has BMO/AMC timing).\"\"\"\n",
    "    from_date = date.today()\n",
    "    to_date = from_date + timedelta(days=days_ahead)\n",
    "    \n",
    "    all_rows = []\n",
    "    current_date = from_date\n",
    "    \n",
    "    while current_date <= to_date:\n",
    "        date_str = current_date.strftime('%Y-%m-%d')\n",
    "        url = f\"https://api.nasdaq.com/api/calendar/earnings?date={date_str}\"\n",
    "        \n",
    "        try:\n",
    "            r = requests.get(url, headers=NASDAQ_HEADERS, timeout=10)\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                rows = data.get('data', {}).get('rows', [])\n",
    "                if rows:\n",
    "                    for row in rows:\n",
    "                        row['earnings_date'] = date_str\n",
    "                    all_rows.extend(rows)\n",
    "        except Exception as e:\n",
    "            print(f\"  {date_str}: {e}\")\n",
    "        \n",
    "        current_date += timedelta(days=1)\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    df = pd.DataFrame(all_rows)\n",
    "    \n",
    "    # Parse timing\n",
    "    def parse_timing(time_str):\n",
    "        if pd.isna(time_str):\n",
    "            return 'unknown'\n",
    "        time_str = str(time_str).lower()\n",
    "        if 'pre-market' in time_str or 'before' in time_str:\n",
    "            return 'BMO'\n",
    "        elif 'after-hours' in time_str or 'after' in time_str:\n",
    "            return 'AMC'\n",
    "        return 'unknown'\n",
    "    \n",
    "    df['actual_timing'] = df['time'].apply(parse_timing)\n",
    "    return df\n",
    "\n",
    "print(\"Fetching upcoming earnings with known timing...\")\n",
    "upcoming_df = fetch_upcoming_earnings_with_timing(days_ahead=60)\n",
    "print(f\"\\nFetched {len(upcoming_df)} upcoming earnings\")\n",
    "print(f\"\\nTiming distribution:\")\n",
    "print(upcoming_df['actual_timing'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For validation, we need historical earnings that we can still get prices for\n",
    "# Let's use recent historical data (last few months) where we have both timing and prices\n",
    "\n",
    "# Actually, we need to look at PAST earnings with known timing\n",
    "# Let's fetch very recent historical dates (last 30 days) to see if any have timing\n",
    "\n",
    "def fetch_recent_historical_earnings(days_back: int = 60) -> pd.DataFrame:\n",
    "    \"\"\"Fetch recent historical earnings to check if any have timing.\"\"\"\n",
    "    to_date = date.today() - timedelta(days=1)  # Yesterday\n",
    "    from_date = to_date - timedelta(days=days_back)\n",
    "    \n",
    "    all_rows = []\n",
    "    current_date = from_date\n",
    "    \n",
    "    print(f\"Fetching {from_date} to {to_date}...\")\n",
    "    \n",
    "    while current_date <= to_date:\n",
    "        date_str = current_date.strftime('%Y-%m-%d')\n",
    "        url = f\"https://api.nasdaq.com/api/calendar/earnings?date={date_str}\"\n",
    "        \n",
    "        try:\n",
    "            r = requests.get(url, headers=NASDAQ_HEADERS, timeout=10)\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                rows = data.get('data', {}).get('rows', [])\n",
    "                if rows:\n",
    "                    for row in rows:\n",
    "                        row['earnings_date'] = date_str\n",
    "                    all_rows.extend(rows)\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        current_date += timedelta(days=1)\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        if (current_date - from_date).days % 10 == 0:\n",
    "            print(f\"  Progress: {(current_date - from_date).days}/{days_back} days, {len(all_rows)} records\")\n",
    "    \n",
    "    df = pd.DataFrame(all_rows)\n",
    "    \n",
    "    # Parse timing\n",
    "    def parse_timing(time_str):\n",
    "        if pd.isna(time_str):\n",
    "            return 'unknown'\n",
    "        time_str = str(time_str).lower()\n",
    "        if 'pre-market' in time_str or 'before' in time_str:\n",
    "            return 'BMO'\n",
    "        elif 'after-hours' in time_str or 'after' in time_str:\n",
    "            return 'AMC'\n",
    "        return 'unknown'\n",
    "    \n",
    "    if 'time' in df.columns:\n",
    "        df['actual_timing'] = df['time'].apply(parse_timing)\n",
    "    return df\n",
    "\n",
    "print(\"Fetching recent historical earnings...\")\n",
    "recent_hist_df = fetch_recent_historical_earnings(days_back=60)\n",
    "print(f\"\\nFetched {len(recent_hist_df)} historical earnings\")\n",
    "if 'actual_timing' in recent_hist_df.columns:\n",
    "    print(f\"\\nTiming distribution:\")\n",
    "    print(recent_hist_df['actual_timing'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since historical Nasdaq data doesn't have timing, let's validate differently:\n",
    "# Use yfinance to get timing for a sample of stocks and compare with our inference\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "def get_yf_timing(symbol: str) -> tuple:\n",
    "    \"\"\"Get earnings timing from yfinance for validation.\n",
    "    \n",
    "    Returns (earnings_date, timing) if available, else (None, None)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ticker = yf.Ticker(symbol)\n",
    "        info = ticker.info\n",
    "        \n",
    "        ts = info.get('earningsTimestamp')\n",
    "        if not ts:\n",
    "            return None, None\n",
    "        \n",
    "        import pytz\n",
    "        ET = pytz.timezone('US/Eastern')\n",
    "        dt = datetime.fromtimestamp(ts, tz=pytz.UTC).astimezone(ET)\n",
    "        \n",
    "        # Determine timing from hour\n",
    "        if dt.hour < 10:\n",
    "            timing = 'BMO'\n",
    "        elif dt.hour >= 16:\n",
    "            timing = 'AMC'\n",
    "        else:\n",
    "            timing = 'unknown'\n",
    "        \n",
    "        return dt.date(), timing\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "# Sample symbols from our historical data\n",
    "sample_symbols = moves_df['symbol'].value_counts().head(100).index.tolist()\n",
    "print(f\"Validating against yfinance for {len(sample_symbols)} symbols...\")\n",
    "\n",
    "yf_timings = []\n",
    "for i, symbol in enumerate(sample_symbols):\n",
    "    earn_date, timing = get_yf_timing(symbol)\n",
    "    if earn_date and timing != 'unknown':\n",
    "        yf_timings.append({\n",
    "            'symbol': symbol,\n",
    "            'yf_earnings_date': earn_date,\n",
    "            'yf_timing': timing\n",
    "        })\n",
    "    \n",
    "    if (i+1) % 20 == 0:\n",
    "        print(f\"  Progress: {i+1}/{len(sample_symbols)}, found {len(yf_timings)} with timing\")\n",
    "    time.sleep(0.05)  # Be nice to yfinance\n",
    "\n",
    "yf_df = pd.DataFrame(yf_timings)\n",
    "print(f\"\\nFound {len(yf_df)} symbols with yfinance timing\")\n",
    "print(yf_df['yf_timing'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative validation: Use upcoming earnings prices when they become historical\n",
    "# For now, let's validate by checking if the inference is internally consistent\n",
    "\n",
    "# Cross-check: For inferred BMO, gap_t should be >> gap_t1\n",
    "# For inferred AMC, gap_t1 should be >> gap_t\n",
    "\n",
    "bmo_data = moves_df[moves_df['inferred_timing'] == 'BMO']\n",
    "amc_data = moves_df[moves_df['inferred_timing'] == 'AMC']\n",
    "\n",
    "print(\"Validation - Internal Consistency:\")\n",
    "print(\"\\nBMO inferred (should have larger gap_t):\")\n",
    "print(f\"  Mean gap_t: {bmo_data['gap_t'].mean()*100:.2f}%\")\n",
    "print(f\"  Mean gap_t1: {bmo_data['gap_t1'].mean()*100:.2f}%\")\n",
    "print(f\"  Ratio: {bmo_data['gap_t'].mean() / bmo_data['gap_t1'].mean():.2f}x\")\n",
    "\n",
    "print(\"\\nAMC inferred (should have larger gap_t1):\")\n",
    "print(f\"  Mean gap_t: {amc_data['gap_t'].mean()*100:.2f}%\")\n",
    "print(f\"  Mean gap_t1: {amc_data['gap_t1'].mean()*100:.2f}%\")\n",
    "print(f\"  Ratio: {amc_data['gap_t1'].mean() / amc_data['gap_t'].mean():.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tune Thresholds\n",
    "\n",
    "Find optimal thresholds that balance coverage vs accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different threshold combinations\n",
    "threshold_results = []\n",
    "\n",
    "for bmo_thresh in [1.5, 2.0, 2.5, 3.0]:\n",
    "    for amc_thresh in [0.3, 0.4, 0.5, 0.67]:\n",
    "        timing = moves_df['gap_ratio'].apply(\n",
    "            lambda x: infer_timing(x, bmo_threshold=bmo_thresh, amc_threshold=amc_thresh)\n",
    "        )\n",
    "        \n",
    "        bmo_count = (timing == 'BMO').sum()\n",
    "        amc_count = (timing == 'AMC').sum()\n",
    "        unknown_count = (timing == 'unknown').sum()\n",
    "        total = len(timing)\n",
    "        \n",
    "        coverage = (bmo_count + amc_count) / total * 100\n",
    "        bmo_pct = bmo_count / total * 100\n",
    "        amc_pct = amc_count / total * 100\n",
    "        \n",
    "        threshold_results.append({\n",
    "            'bmo_threshold': bmo_thresh,\n",
    "            'amc_threshold': amc_thresh,\n",
    "            'coverage_pct': coverage,\n",
    "            'bmo_pct': bmo_pct,\n",
    "            'amc_pct': amc_pct,\n",
    "            'bmo_amc_ratio': bmo_pct / amc_pct if amc_pct > 0 else 0,\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(threshold_results)\n",
    "print(\"Threshold tuning results:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Best coverage with reasonable BMO/AMC balance (expect roughly 50/50)\n",
    "print(\"\\nRecommended: Choose thresholds with ~50% coverage and ~1.0 BMO/AMC ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the selected thresholds\n",
    "BMO_THRESHOLD = 2.0  # gap_ratio > 2.0 -> BMO\n",
    "AMC_THRESHOLD = 0.5  # gap_ratio < 0.5 -> AMC\n",
    "\n",
    "moves_df['timing'] = moves_df['gap_ratio'].apply(\n",
    "    lambda x: infer_timing(x, bmo_threshold=BMO_THRESHOLD, amc_threshold=AMC_THRESHOLD)\n",
    ")\n",
    "\n",
    "print(f\"Final timing distribution (thresholds: BMO>{BMO_THRESHOLD}, AMC<{AMC_THRESHOLD}):\")\n",
    "print(moves_df['timing'].value_counts())\n",
    "print(f\"\\nCoverage: {(moves_df['timing'] != 'unknown').mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correct Price Move Alignment\n",
    "\n",
    "For AMC earnings, the current data computes moves on the wrong day. We need to recalculate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Current approach: Close_T-1 -> Open_T -> Close_T -> Close_T+1\n# For BMO: Reaction is Open_T (correct - gap from T-1 close to T open)\n# For AMC: Reaction is Open_T+1 (currently we capture Close_T-1 -> Close_T which is BEFORE earnings)\n\n# Phase 3.3: Fix BMO/AMC base price inconsistency\n# Both BMO and AMC should use close_t_minus_1 as base for comparable percentages\n# This allows apples-to-apples comparison across timing types\n\ndef compute_corrected_moves(row, price_cache):\n    \"\"\"Compute timing-corrected moves based on inferred timing.\n    \n    Both BMO and AMC use close_t_minus_1 as the base price for comparable percentages.\n    \"\"\"\n    symbol = row['symbol']\n    timing = row['timing']\n    \n    if symbol not in price_cache:\n        return pd.Series({'corrected_gap': np.nan, 'corrected_full': np.nan})\n    \n    pdf = price_cache[symbol]\n    earn_date = pd.to_datetime(row['earnings_date'])\n    \n    try:\n        # Find T-1, T, T+1\n        t_minus_1_candidates = pdf[pdf.index < earn_date].tail(1)\n        t_candidates = pdf[pdf.index >= earn_date].head(1)\n        \n        if t_minus_1_candidates.empty or t_candidates.empty:\n            return pd.Series({'corrected_gap': np.nan, 'corrected_full': np.nan})\n        \n        t_minus_1 = t_minus_1_candidates.index[0]\n        t = t_candidates.index[0]\n        \n        t_plus_1_candidates = pdf[pdf.index > t].head(1)\n        if t_plus_1_candidates.empty:\n            return pd.Series({'corrected_gap': np.nan, 'corrected_full': np.nan})\n        t_plus_1 = t_plus_1_candidates.index[0]\n        \n        close_t_minus_1 = pdf.loc[t_minus_1, 'close']\n        open_t = pdf.loc[t, 'open']\n        close_t = pdf.loc[t, 'close']\n        open_t_plus_1 = pdf.loc[t_plus_1, 'open']\n        close_t_plus_1 = pdf.loc[t_plus_1, 'close']\n        \n        # Use close_t_minus_1 as base for BOTH timing types for consistency\n        base_price = close_t_minus_1\n        \n        if timing == 'BMO':\n            # BMO: Reaction is T-1 close -> T open -> T close\n            corrected_gap = (open_t - base_price) / base_price\n            corrected_full = (close_t - base_price) / base_price\n        elif timing == 'AMC':\n            # AMC: Reaction is T close -> T+1 open -> T+1 close\n            # But we express as % of T-1 close for consistency with BMO\n            corrected_gap = (open_t_plus_1 - close_t) / base_price\n            corrected_full = (close_t_plus_1 - close_t) / base_price\n        else:\n            # Unknown: Use overnight move as best approximation\n            corrected_gap = np.nan\n            corrected_full = (close_t_plus_1 - base_price) / base_price\n        \n        return pd.Series({'corrected_gap': corrected_gap, 'corrected_full': corrected_full})\n        \n    except Exception:\n        return pd.Series({'corrected_gap': np.nan, 'corrected_full': np.nan})\n\nprint(\"Computing timing-corrected moves (this may take a few minutes)...\")\ncorrected = moves_df.apply(lambda row: compute_corrected_moves(row, price_cache), axis=1)\nmoves_df['corrected_gap'] = corrected['corrected_gap']\nmoves_df['corrected_full'] = corrected['corrected_full']\nmoves_df['corrected_gap_abs'] = moves_df['corrected_gap'].abs()\nmoves_df['corrected_full_abs'] = moves_df['corrected_full'].abs()\n\nprint(\"Done!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs corrected moves\n",
    "print(\"Original vs Corrected Moves (Absolute Values):\")\n",
    "print(\"\\nOriginal gap_move_abs (all data, not timing-aware):\")\n",
    "print(f\"  Mean: {moves_df['gap_move_abs'].mean()*100:.2f}%\")\n",
    "print(f\"  Median: {moves_df['gap_move_abs'].median()*100:.2f}%\")\n",
    "\n",
    "print(\"\\nCorrected gap_abs (timing-aware):\")\n",
    "valid_corrected = moves_df['corrected_gap_abs'].dropna()\n",
    "print(f\"  Mean: {valid_corrected.mean()*100:.2f}%\")\n",
    "print(f\"  Median: {valid_corrected.median()*100:.2f}%\")\n",
    "\n",
    "print(\"\\n--- By Timing ---\")\n",
    "for timing in ['BMO', 'AMC', 'unknown']:\n",
    "    subset = moves_df[moves_df['timing'] == timing]\n",
    "    if len(subset) > 0:\n",
    "        print(f\"\\n{timing} ({len(subset):,} records):\")\n",
    "        print(f\"  Original gap: {subset['gap_move_abs'].mean()*100:.2f}%\")\n",
    "        print(f\"  Corrected gap: {subset['corrected_gap_abs'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Updated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Select columns to save\noutput_cols = [\n    'symbol', 'earnings_date',\n    'close_t_minus_1', 'open_t', 'close_t', 'close_t_plus_1',\n    'gap_move', 'gap_move_abs', 'full_move', 'full_move_abs',\n    'overnight_move', 'overnight_move_abs',\n    'timing',  # Inferred timing\n    'gap_t', 'gap_t1', 'gap_ratio',  # Inference features\n    'corrected_gap', 'corrected_gap_abs',  # Timing-corrected moves\n    'corrected_full', 'corrected_full_abs',\n]\n\noutput_df = moves_df[output_cols].copy()\n\n# === OUTLIER FILTERS ===\nprint(f\"Before filters: {len(output_df):,} rows\")\n\n# Filter 1: Remove penny stocks (price < $1.00) - unreliable % moves\npenny_mask = output_df['close_t_minus_1'] < 1.00\nif penny_mask.sum() > 0:\n    print(f\"  Removing {penny_mask.sum()} penny stock records (price < $1.00)\")\n    output_df = output_df[~penny_mask]\n\n# Filter 2: Remove extreme moves (> 50%) - likely data errors or reverse splits\nmove_col = 'corrected_full_abs' if 'corrected_full_abs' in output_df.columns else 'overnight_move_abs'\nextreme_mask = output_df[move_col] > 0.50\nif extreme_mask.sum() > 0:\n    print(f\"  Removing {extreme_mask.sum()} records with >{move_col} > 50%\")\n    output_df = output_df[~extreme_mask]\n\nprint(f\"After filters: {len(output_df):,} rows\")\n\n# Save\noutput_path = DATA_DIR / 'historical_earnings_moves.parquet'\noutput_df.to_parquet(output_path, index=False)\nprint(f\"\\nSaved to {output_path}\")\nprint(f\"\\nFinal dataset:\")\nprint(f\"  Records: {len(output_df):,}\")\nprint(f\"  Timing coverage: {(output_df['timing'] != 'unknown').mean()*100:.1f}%\")\nprint(f\"  BMO: {(output_df['timing'] == 'BMO').sum():,}\")\nprint(f\"  AMC: {(output_df['timing'] == 'AMC').sum():,}\")\nprint(f\"  Unknown: {(output_df['timing'] == 'unknown').sum():,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TIMING INFERENCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\"\"\n",
    "Problem: Historical Nasdaq data has 0% BMO/AMC timing\n",
    "         (Nasdaq only provides timing for UPCOMING earnings)\n",
    "\n",
    "Solution: Infer timing from overnight gap magnitudes\n",
    "  - BMO: Large gap on T (earnings day open)\n",
    "  - AMC: Large gap on T+1 (day after)\n",
    "  - gap_ratio = gap_T / gap_T+1\n",
    "\n",
    "Thresholds:\n",
    "  - BMO: gap_ratio > {BMO_THRESHOLD}\n",
    "  - AMC: gap_ratio < {AMC_THRESHOLD}\n",
    "\n",
    "Results:\n",
    "  - Total records: {len(output_df):,}\n",
    "  - BMO inferred: {(output_df['timing'] == 'BMO').sum():,} ({(output_df['timing'] == 'BMO').mean()*100:.1f}%)\n",
    "  - AMC inferred: {(output_df['timing'] == 'AMC').sum():,} ({(output_df['timing'] == 'AMC').mean()*100:.1f}%)\n",
    "  - Unknown: {(output_df['timing'] == 'unknown').sum():,} ({(output_df['timing'] == 'unknown').mean()*100:.1f}%)\n",
    "  - Coverage: {(output_df['timing'] != 'unknown').mean()*100:.1f}%\n",
    "\n",
    "Files Updated:\n",
    "  - {output_path}\n",
    "\n",
    "Next Steps:\n",
    "  1. Retrain ML model on timing-corrected data\n",
    "  2. Compare model performance before/after correction\n",
    "  3. Consider filtering to only BMO/AMC records for cleaner training\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}