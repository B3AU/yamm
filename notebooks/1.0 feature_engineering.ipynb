{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 1.0 Feature Engineering for Earnings Move Prediction\n",
    "\n",
    "Build comprehensive feature set to predict post-earnings |move| distribution.\n",
    "\n",
    "**Reuses existing data from news_ranking project:**\n",
    "- `news_embeddings.pqt` - 1.7M+ pre-computed embeddings (768-dim)\n",
    "- `key_metrics.pqt`, `ratios.pqt`, `growth.pqt` - fundamentals\n",
    "- `filing_dates.pqt` - SEC filing dates for point-in-time alignment\n",
    "\n",
    "## Feature Categories\n",
    "\n",
    "1. **Historical earnings behavior** - past moves, consistency\n",
    "2. **Pre-earnings news** - PCA-reduced embeddings (10 components) from T-7 to T-1\n",
    "3. **Fundamentals** - key metrics, ratios, growth (point-in-time)\n",
    "4. **Price context** - momentum, volatility, positioning\n",
    "5. **Analyst expectations** - surprise history\n",
    "\n",
    "## Key Design Decision: PCA-10 for News Embeddings\n",
    "\n",
    "Full 768-dim embeddings hurt model performance (overfitting). Testing showed:\n",
    "- Full embeddings: worse calibration\n",
    "- PCA-10: 31% improvement in q75 calibration\n",
    "\n",
    "We reduce 768-dim â†’ 10 PCA components, capturing ~35% variance while preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "import warnings\n",
    "import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "load_dotenv()\n",
    "FMP_KEY = os.getenv('FMP_API_KEY')\n",
    "\n",
    "DATA_DIR = Path('../data')\n",
    "EARNINGS_DIR = DATA_DIR / 'earnings'\n",
    "NEWS_DIR = DATA_DIR / 'news_ranking'\n",
    "MODEL_DIR = Path('../models')\n",
    "\n",
    "EARNINGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# PCA configuration\n",
    "N_PCA_COMPONENTS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load Existing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earnings moves: 5307 events, 2212 symbols\n",
      "Date range: 2024-03-19 to 2025-12-18\n"
     ]
    }
   ],
   "source": [
    "# Load earnings moves from 0.2 notebook\n",
    "moves_df = pd.read_parquet(EARNINGS_DIR / 'historical_earnings_moves.parquet')\n",
    "moves_df['earnings_date'] = pd.to_datetime(moves_df['earnings_date'])\n",
    "print(f\"Earnings moves: {len(moves_df)} events, {moves_df['symbol'].nunique()} symbols\")\n",
    "print(f\"Date range: {moves_df['earnings_date'].min().date()} to {moves_df['earnings_date'].max().date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earnings calendar: 21931 events\n"
     ]
    }
   ],
   "source": [
    "# Load earnings calendar for timing info\n",
    "earnings_cal = pd.read_parquet(EARNINGS_DIR / 'earnings_calendar.parquet')\n",
    "earnings_cal['date'] = pd.to_datetime(earnings_cal['date'])\n",
    "print(f\"Earnings calendar: {len(earnings_cal)} events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available news_ranking data:\n",
      "  all_the_news_anon.pqt: 1001.2 MB\n",
      "  backtest_vol_comparison.pqt: 0.0 MB\n",
      "  confidence_gating_best.pqt: 0.0 MB\n",
      "  confidence_scores.pqt: 7.5 MB\n",
      "  dropout_gridsearch_results.pqt: 0.0 MB\n",
      "  dropout_search_results.pqt: 0.0 MB\n",
      "  hyperparam_arch_results.pqt: 0.0 MB\n",
      "  hyperparam_train_results.pqt: 0.0 MB\n",
      "  ml_dataset.pqt: 3815.8 MB\n",
      "  news_embeddings.pqt: 6343.4 MB\n",
      "  price_features.pqt: 635.8 MB\n",
      "  risk_management_results.pqt: 0.0 MB\n",
      "  robust_arch_results.pqt: 0.0 MB\n",
      "  robust_dropout_results.pqt: 0.0 MB\n",
      "  robust_train_results.pqt: 0.0 MB\n",
      "  short_backtest_improved.pqt: 0.0 MB\n",
      "  short_backtest_results.pqt: 0.0 MB\n",
      "  strategy_comparison_results.pqt: 0.0 MB\n",
      "  strategy_evaluation_results.pqt: 0.0 MB\n",
      "  symbol_metrics_val.pqt: 0.1 MB\n",
      "  vol_targeting_best.pqt: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "# Check what data we have from news_ranking\n",
    "print(\"Available news_ranking data:\")\n",
    "for f in sorted(NEWS_DIR.glob('*.pqt')):\n",
    "    size_mb = f.stat().st_size / 1e6\n",
    "    print(f\"  {f.name}: {size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Historical Earnings Features\n",
    "\n",
    "For each earnings event, compute features based on that stock's past earnings behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_historical_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute historical earnings features for each event.\n",
    "    Uses only data available BEFORE the event (no lookahead).\n",
    "    \"\"\"\n",
    "    df = df.sort_values(['symbol', 'earnings_date']).copy()\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    for symbol in df['symbol'].unique():\n",
    "        symbol_df = df[df['symbol'] == symbol].copy()\n",
    "        \n",
    "        for i in range(len(symbol_df)):\n",
    "            row = symbol_df.iloc[i]\n",
    "            \n",
    "            # Get all PREVIOUS earnings for this symbol\n",
    "            past = symbol_df.iloc[:i]\n",
    "            \n",
    "            feat = {\n",
    "                'symbol': symbol,\n",
    "                'earnings_date': row['earnings_date'],\n",
    "                'target_move': row['overnight_move_abs'],\n",
    "                'gap_move': row.get('gap_move_abs', np.nan),\n",
    "                'close_t_minus_1': row['close_t_minus_1'],\n",
    "            }\n",
    "            \n",
    "            if len(past) >= 1:\n",
    "                # Historical move statistics\n",
    "                feat['hist_move_mean'] = past['overnight_move_abs'].mean()\n",
    "                feat['hist_move_median'] = past['overnight_move_abs'].median()\n",
    "                feat['hist_move_std'] = past['overnight_move_abs'].std() if len(past) > 1 else 0\n",
    "                feat['hist_move_max'] = past['overnight_move_abs'].max()\n",
    "                feat['hist_move_min'] = past['overnight_move_abs'].min()\n",
    "                \n",
    "                # Coefficient of variation (predictability)\n",
    "                if feat['hist_move_mean'] > 0:\n",
    "                    feat['hist_move_cv'] = feat['hist_move_std'] / feat['hist_move_mean']\n",
    "                else:\n",
    "                    feat['hist_move_cv'] = 0\n",
    "                \n",
    "                # Recent moves (last 2 quarters)\n",
    "                recent = past.tail(2)\n",
    "                feat['recent_move_mean'] = recent['overnight_move_abs'].mean()\n",
    "                \n",
    "                # Trend (are moves getting bigger or smaller?)\n",
    "                if len(past) >= 2:\n",
    "                    feat['move_trend'] = past['overnight_move_abs'].iloc[-1] - past['overnight_move_abs'].iloc[0]\n",
    "                else:\n",
    "                    feat['move_trend'] = 0\n",
    "                \n",
    "                # Gap vs full move ratio (does stock continue or reverse?)\n",
    "                if 'gap_move_abs' in past.columns:\n",
    "                    gap_mean = past['gap_move_abs'].mean()\n",
    "                    if gap_mean > 0:\n",
    "                        feat['gap_continuation_ratio'] = feat['hist_move_mean'] / gap_mean\n",
    "                    else:\n",
    "                        feat['gap_continuation_ratio'] = 1\n",
    "                \n",
    "                # Number of historical observations\n",
    "                feat['n_past_earnings'] = len(past)\n",
    "            else:\n",
    "                # No history - use defaults\n",
    "                feat['hist_move_mean'] = np.nan\n",
    "                feat['hist_move_median'] = np.nan\n",
    "                feat['hist_move_std'] = np.nan\n",
    "                feat['hist_move_max'] = np.nan\n",
    "                feat['hist_move_min'] = np.nan\n",
    "                feat['hist_move_cv'] = np.nan\n",
    "                feat['recent_move_mean'] = np.nan\n",
    "                feat['move_trend'] = np.nan\n",
    "                feat['gap_continuation_ratio'] = np.nan\n",
    "                feat['n_past_earnings'] = 0\n",
    "            \n",
    "            features.append(feat)\n",
    "    \n",
    "    return pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed features for 5307 earnings events\n",
      "Events with history (n_past >= 1): 3095\n",
      "Events with history (n_past >= 4): 575\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>earnings_date</th>\n",
       "      <th>target_move</th>\n",
       "      <th>gap_move</th>\n",
       "      <th>close_t_minus_1</th>\n",
       "      <th>hist_move_mean</th>\n",
       "      <th>hist_move_median</th>\n",
       "      <th>hist_move_std</th>\n",
       "      <th>hist_move_max</th>\n",
       "      <th>hist_move_min</th>\n",
       "      <th>hist_move_cv</th>\n",
       "      <th>recent_move_mean</th>\n",
       "      <th>move_trend</th>\n",
       "      <th>gap_continuation_ratio</th>\n",
       "      <th>n_past_earnings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2024-05-29</td>\n",
       "      <td>0.113285</td>\n",
       "      <td>0.009918</td>\n",
       "      <td>148.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>2024-11-25</td>\n",
       "      <td>0.003960</td>\n",
       "      <td>0.005155</td>\n",
       "      <td>133.84</td>\n",
       "      <td>0.113285</td>\n",
       "      <td>0.113285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113285</td>\n",
       "      <td>0.113285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.113285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.421769</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>2025-05-28</td>\n",
       "      <td>0.018156</td>\n",
       "      <td>0.003865</td>\n",
       "      <td>111.26</td>\n",
       "      <td>0.058623</td>\n",
       "      <td>0.058623</td>\n",
       "      <td>0.077305</td>\n",
       "      <td>0.113285</td>\n",
       "      <td>0.003960</td>\n",
       "      <td>1.318684</td>\n",
       "      <td>0.058623</td>\n",
       "      <td>-0.109325</td>\n",
       "      <td>7.778092</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>2025-08-27</td>\n",
       "      <td>0.056298</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>118.30</td>\n",
       "      <td>0.045134</td>\n",
       "      <td>0.018156</td>\n",
       "      <td>0.059446</td>\n",
       "      <td>0.113285</td>\n",
       "      <td>0.003960</td>\n",
       "      <td>1.317118</td>\n",
       "      <td>0.011058</td>\n",
       "      <td>-0.095130</td>\n",
       "      <td>7.149467</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>2025-11-24</td>\n",
       "      <td>0.039339</td>\n",
       "      <td>0.004298</td>\n",
       "      <td>151.25</td>\n",
       "      <td>0.047925</td>\n",
       "      <td>0.037227</td>\n",
       "      <td>0.048858</td>\n",
       "      <td>0.113285</td>\n",
       "      <td>0.003960</td>\n",
       "      <td>1.019468</td>\n",
       "      <td>0.037227</td>\n",
       "      <td>-0.056988</td>\n",
       "      <td>9.292571</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol earnings_date  target_move  gap_move  close_t_minus_1  \\\n",
       "0      A    2024-05-29     0.113285  0.009918           148.21   \n",
       "1      A    2024-11-25     0.003960  0.005155           133.84   \n",
       "2      A    2025-05-28     0.018156  0.003865           111.26   \n",
       "3      A    2025-08-27     0.056298  0.001691           118.30   \n",
       "4      A    2025-11-24     0.039339  0.004298           151.25   \n",
       "\n",
       "   hist_move_mean  hist_move_median  hist_move_std  hist_move_max  \\\n",
       "0             NaN               NaN            NaN            NaN   \n",
       "1        0.113285          0.113285       0.000000       0.113285   \n",
       "2        0.058623          0.058623       0.077305       0.113285   \n",
       "3        0.045134          0.018156       0.059446       0.113285   \n",
       "4        0.047925          0.037227       0.048858       0.113285   \n",
       "\n",
       "   hist_move_min  hist_move_cv  recent_move_mean  move_trend  \\\n",
       "0            NaN           NaN               NaN         NaN   \n",
       "1       0.113285      0.000000          0.113285    0.000000   \n",
       "2       0.003960      1.318684          0.058623   -0.109325   \n",
       "3       0.003960      1.317118          0.011058   -0.095130   \n",
       "4       0.003960      1.019468          0.037227   -0.056988   \n",
       "\n",
       "   gap_continuation_ratio  n_past_earnings  \n",
       "0                     NaN                0  \n",
       "1               11.421769                1  \n",
       "2                7.778092                2  \n",
       "3                7.149467                3  \n",
       "4                9.292571                4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute historical features\n",
    "hist_features = compute_historical_features(moves_df)\n",
    "print(f\"Computed features for {len(hist_features)} earnings events\")\n",
    "print(f\"Events with history (n_past >= 1): {(hist_features['n_past_earnings'] >= 1).sum()}\")\n",
    "print(f\"Events with history (n_past >= 4): {(hist_features['n_past_earnings'] >= 4).sum()}\")\n",
    "hist_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. Pre-Earnings News Features (PCA-10)\n",
    "\n",
    "Aggregate news embeddings from the X days before each earnings event.\n",
    "\n",
    "Strategy:\n",
    "- Look at news from T-7 to T-1 before earnings\n",
    "- Mean-pool 768-dim embeddings for that window\n",
    "- Apply PCA to reduce to 10 components\n",
    "- Also include news count as a feature\n",
    "\n",
    "**Why PCA?** Full 768-dim embeddings cause overfitting. PCA-10 captures the important variance while being robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading news data...\n",
      "News articles: 1,747,711\n",
      "\n",
      "Loading embeddings (this is 6GB, may take a minute)...\n",
      "Embeddings: 1,748,149 rows\n",
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# Load news data and embeddings\n",
    "print(\"Loading news data...\")\n",
    "news = pd.read_parquet(DATA_DIR / 'news_ranking' / 'all_the_news_anon.pqt')\n",
    "news['publishedDate'] = pd.to_datetime(news['publishedDate'])\n",
    "print(f\"News articles: {len(news):,}\")\n",
    "\n",
    "print(\"\\nLoading embeddings (this is 6GB, may take a minute)...\")\n",
    "embeddings = pd.read_parquet(NEWS_DIR / 'news_embeddings.pqt')\n",
    "print(f\"Embeddings: {len(embeddings):,} rows\")\n",
    "\n",
    "# Get embedding columns\n",
    "emb_cols = [c for c in embeddings.columns if c.startswith('emb_')]\n",
    "print(f\"Embedding dimension: {len(emb_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News with embeddings: 1,747,711\n"
     ]
    }
   ],
   "source": [
    "# Join embeddings with news metadata\n",
    "news_with_emb = embeddings.merge(\n",
    "    news[['url', 'symbol', 'publishedDate']],\n",
    "    on=['url', 'symbol'],\n",
    "    how='inner'\n",
    ")\n",
    "news_with_emb['pub_date'] = news_with_emb['publishedDate'].dt.date\n",
    "print(f\"News with embeddings: {len(news_with_emb):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_pre_earnings_news(earnings_df: pd.DataFrame, \n",
    "                                 news_df: pd.DataFrame,\n",
    "                                 emb_cols: list,\n",
    "                                 lookback_days: int = 7) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each earnings event, aggregate news embeddings from [T-lookback, T-1].\n",
    "    Returns DataFrame with mean-pooled 768-dim embeddings and news count.\n",
    "    (PCA is applied separately after aggregation)\n",
    "    \"\"\"\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Group news by symbol for faster lookup\n",
    "    news_by_symbol = {symbol: grp for symbol, grp in news_df.groupby('symbol')}\n",
    "    \n",
    "    for _, row in tqdm(earnings_df.iterrows(), total=len(earnings_df), desc=\"Aggregating news\"):\n",
    "        symbol = row['symbol']\n",
    "        earn_date = row['earnings_date'].date()\n",
    "        \n",
    "        result = {\n",
    "            'symbol': symbol,\n",
    "            'earnings_date': row['earnings_date'],\n",
    "        }\n",
    "        \n",
    "        if symbol not in news_by_symbol:\n",
    "            # No news for this symbol\n",
    "            result['pre_earnings_news_count'] = 0\n",
    "            for col in emb_cols:\n",
    "                result[col] = 0.0  # Store raw embedding, not news_emb_\n",
    "            results.append(result)\n",
    "            continue\n",
    "        \n",
    "        symbol_news = news_by_symbol[symbol]\n",
    "        \n",
    "        # Filter to lookback window [T-lookback, T-1]\n",
    "        start_date = earn_date - timedelta(days=lookback_days)\n",
    "        end_date = earn_date - timedelta(days=1)\n",
    "        \n",
    "        window_news = symbol_news[\n",
    "            (symbol_news['pub_date'] >= start_date) &\n",
    "            (symbol_news['pub_date'] <= end_date)\n",
    "        ]\n",
    "        \n",
    "        result['pre_earnings_news_count'] = len(window_news)\n",
    "        \n",
    "        if len(window_news) > 0:\n",
    "            # Mean-pool embeddings\n",
    "            mean_emb = window_news[emb_cols].mean()\n",
    "            for col in emb_cols:\n",
    "                result[col] = mean_emb[col]\n",
    "        else:\n",
    "            for col in emb_cols:\n",
    "                result[col] = 0.0\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached full news features...\n",
      "Full news features shape: (5313, 771)\n",
      "Events with news: 2381 (44.8%)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate pre-earnings news (7-day lookback)\n",
    "# This takes a while - cache the result\n",
    "news_features_full_file = EARNINGS_DIR / 'pre_earnings_news_features_full.parquet'\n",
    "\n",
    "if news_features_full_file.exists():\n",
    "    print(\"Loading cached full news features...\")\n",
    "    news_features_full = pd.read_parquet(news_features_full_file)\n",
    "else:\n",
    "    print(\"Computing pre-earnings news features (this may take 10-20 minutes)...\")\n",
    "    news_features_full = aggregate_pre_earnings_news(\n",
    "        hist_features,\n",
    "        news_with_emb,\n",
    "        emb_cols,\n",
    "        lookback_days=7\n",
    "    )\n",
    "    news_features_full.to_parquet(news_features_full_file, index=False)\n",
    "    print(f\"Saved to {news_features_full_file}\")\n",
    "\n",
    "print(f\"Full news features shape: {news_features_full.shape}\")\n",
    "print(f\"Events with news: {(news_features_full['pre_earnings_news_count'] > 0).sum()} ({(news_features_full['pre_earnings_news_count'] > 0).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18p6ikg724q",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying PCA: 768 dims -> 10 components\n",
      "Fitting PCA on 2381 rows with news...\n",
      "Variance explained: 34.5%\n",
      "Per component: ['9.8%', '6.8%', '4.5%', '2.8%', '2.4%', '2.1%', '2.0%', '1.5%', '1.3%', '1.3%']\n",
      "Saved PCA model to ../models/news_pca.joblib\n"
     ]
    }
   ],
   "source": [
    "# Apply PCA to reduce 768-dim embeddings to 10 components\n",
    "\n",
    "# Detect embedding columns in the cached file (could be 'emb_*' or 'news_emb_*')\n",
    "emb_cols_in_cache = [c for c in news_features_full.columns if c.startswith('emb_') or c.startswith('news_emb_')]\n",
    "print(f\"\\nApplying PCA: {len(emb_cols_in_cache)} dims -> {N_PCA_COMPONENTS} components\")\n",
    "\n",
    "# Get rows with actual news (non-zero embeddings)\n",
    "has_news = news_features_full['pre_earnings_news_count'] > 0\n",
    "X_emb = news_features_full.loc[has_news, emb_cols_in_cache].values\n",
    "\n",
    "print(f\"Fitting PCA on {len(X_emb)} rows with news...\")\n",
    "\n",
    "# Fit PCA\n",
    "pca = PCA(n_components=N_PCA_COMPONENTS, random_state=42)\n",
    "pca.fit(X_emb)\n",
    "\n",
    "print(f\"Variance explained: {pca.explained_variance_ratio_.sum()*100:.1f}%\")\n",
    "print(f\"Per component: {[f'{v:.1%}' for v in pca.explained_variance_ratio_]}\")\n",
    "\n",
    "# Save PCA model for inference\n",
    "pca_path = MODEL_DIR / 'news_pca.joblib'\n",
    "joblib.dump(pca, pca_path)\n",
    "print(f\"Saved PCA model to {pca_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "kt1u2tcfv5p",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News features shape (with PCA): (5313, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>earnings_date</th>\n",
       "      <th>pre_earnings_news_count</th>\n",
       "      <th>news_pca_0</th>\n",
       "      <th>news_pca_1</th>\n",
       "      <th>news_pca_2</th>\n",
       "      <th>news_pca_3</th>\n",
       "      <th>news_pca_4</th>\n",
       "      <th>news_pca_5</th>\n",
       "      <th>news_pca_6</th>\n",
       "      <th>news_pca_7</th>\n",
       "      <th>news_pca_8</th>\n",
       "      <th>news_pca_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HYFT</td>\n",
       "      <td>2024-09-16</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.165235</td>\n",
       "      <td>-0.141558</td>\n",
       "      <td>-0.152304</td>\n",
       "      <td>0.128352</td>\n",
       "      <td>-0.114902</td>\n",
       "      <td>0.128625</td>\n",
       "      <td>-0.155911</td>\n",
       "      <td>0.012585</td>\n",
       "      <td>-0.059119</td>\n",
       "      <td>-0.073341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HYFT</td>\n",
       "      <td>2024-12-10</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.122203</td>\n",
       "      <td>-0.224748</td>\n",
       "      <td>-0.133205</td>\n",
       "      <td>0.126947</td>\n",
       "      <td>-0.063816</td>\n",
       "      <td>0.069849</td>\n",
       "      <td>-0.121516</td>\n",
       "      <td>-0.029824</td>\n",
       "      <td>-0.061042</td>\n",
       "      <td>-0.062860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HYFT</td>\n",
       "      <td>2025-03-28</td>\n",
       "      <td>0</td>\n",
       "      <td>0.030873</td>\n",
       "      <td>-0.170455</td>\n",
       "      <td>-0.197268</td>\n",
       "      <td>0.010112</td>\n",
       "      <td>-0.106102</td>\n",
       "      <td>0.053732</td>\n",
       "      <td>0.076889</td>\n",
       "      <td>0.151010</td>\n",
       "      <td>-0.028557</td>\n",
       "      <td>-0.018098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HYFT</td>\n",
       "      <td>2025-09-15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.030873</td>\n",
       "      <td>-0.170455</td>\n",
       "      <td>-0.197268</td>\n",
       "      <td>0.010112</td>\n",
       "      <td>-0.106102</td>\n",
       "      <td>0.053732</td>\n",
       "      <td>0.076889</td>\n",
       "      <td>0.151010</td>\n",
       "      <td>-0.028557</td>\n",
       "      <td>-0.018098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HYFT</td>\n",
       "      <td>2025-12-15</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.314191</td>\n",
       "      <td>0.074169</td>\n",
       "      <td>0.037352</td>\n",
       "      <td>0.064734</td>\n",
       "      <td>0.052504</td>\n",
       "      <td>0.015498</td>\n",
       "      <td>0.007807</td>\n",
       "      <td>-0.053889</td>\n",
       "      <td>-0.017339</td>\n",
       "      <td>-0.006043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol earnings_date  pre_earnings_news_count  news_pca_0  news_pca_1  \\\n",
       "0   HYFT    2024-09-16                        1   -0.165235   -0.141558   \n",
       "1   HYFT    2024-12-10                        2   -0.122203   -0.224748   \n",
       "2   HYFT    2025-03-28                        0    0.030873   -0.170455   \n",
       "3   HYFT    2025-09-15                        0    0.030873   -0.170455   \n",
       "4   HYFT    2025-12-15                        1   -0.314191    0.074169   \n",
       "\n",
       "   news_pca_2  news_pca_3  news_pca_4  news_pca_5  news_pca_6  news_pca_7  \\\n",
       "0   -0.152304    0.128352   -0.114902    0.128625   -0.155911    0.012585   \n",
       "1   -0.133205    0.126947   -0.063816    0.069849   -0.121516   -0.029824   \n",
       "2   -0.197268    0.010112   -0.106102    0.053732    0.076889    0.151010   \n",
       "3   -0.197268    0.010112   -0.106102    0.053732    0.076889    0.151010   \n",
       "4    0.037352    0.064734    0.052504    0.015498    0.007807   -0.053889   \n",
       "\n",
       "   news_pca_8  news_pca_9  \n",
       "0   -0.059119   -0.073341  \n",
       "1   -0.061042   -0.062860  \n",
       "2   -0.028557   -0.018098  \n",
       "3   -0.028557   -0.018098  \n",
       "4   -0.017339   -0.006043  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform all embeddings to PCA features\n",
    "# Fill NaN with 0 for rows without news (PCA doesn't accept NaN)\n",
    "X_all_emb = news_features_full[emb_cols_in_cache].fillna(0).values\n",
    "X_pca = pca.transform(X_all_emb)\n",
    "\n",
    "# Create news_features with PCA columns instead of 768-dim\n",
    "news_features = news_features_full[['symbol', 'earnings_date', 'pre_earnings_news_count']].copy()\n",
    "for i in range(N_PCA_COMPONENTS):\n",
    "    news_features[f'news_pca_{i}'] = X_pca[:, i]\n",
    "\n",
    "print(f\"News features shape (with PCA): {news_features.shape}\")\n",
    "news_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After news merge: (5343, 26)\n"
     ]
    }
   ],
   "source": [
    "# Merge news features with historical features\n",
    "news_features['earnings_date'] = pd.to_datetime(news_features['earnings_date'])\n",
    "features_df = hist_features.merge(\n",
    "    news_features,\n",
    "    on=['symbol', 'earnings_date'],\n",
    "    how='left'\n",
    ")\n",
    "print(f\"After news merge: {features_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 4. Fundamental Features (Point-in-Time)\n",
    "\n",
    "Use fundamentals from news_ranking project with proper point-in-time alignment via SEC filing dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: 307,009 rows\n",
      "Ratios: 307,009 rows\n",
      "Growth: 307,009 rows\n",
      "Filing dates: 305,371 rows\n"
     ]
    }
   ],
   "source": [
    "# Load fundamentals\n",
    "metrics = pd.read_parquet(DATA_DIR / 'key_metrics.pqt')\n",
    "ratios = pd.read_parquet(DATA_DIR / 'ratios.pqt')\n",
    "growth = pd.read_parquet(DATA_DIR / 'growth.pqt')\n",
    "filing_dates = pd.read_parquet(DATA_DIR / 'filing_dates.pqt')\n",
    "\n",
    "print(f\"Metrics: {len(metrics):,} rows\")\n",
    "print(f\"Ratios: {len(ratios):,} rows\")\n",
    "print(f\"Growth: {len(growth):,} rows\")\n",
    "print(f\"Filing dates: {len(filing_dates):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16 fundamental features\n"
     ]
    }
   ],
   "source": [
    "# Select key fundamental features\n",
    "METRIC_COLS = [\n",
    "    'evToEBITDA',           # Value\n",
    "    'freeCashFlowYield',    # Value\n",
    "    'earningsYield',        # Value\n",
    "    'returnOnEquity',       # Quality\n",
    "    'returnOnAssets',       # Quality\n",
    "    'currentRatio',         # Liquidity\n",
    "]\n",
    "\n",
    "RATIO_COLS = [\n",
    "    'priceToEarningsRatio',  # Value (P/E)\n",
    "    'priceToBookRatio',      # Value\n",
    "    'priceToSalesRatio',     # Value\n",
    "    'grossProfitMargin',     # Quality\n",
    "    'operatingProfitMargin', # Quality\n",
    "    'netProfitMargin',       # Quality\n",
    "    'debtToEquityRatio',     # Leverage\n",
    "]\n",
    "\n",
    "GROWTH_COLS = [\n",
    "    'revenueGrowth',         # Growth\n",
    "    'netIncomeGrowth',       # Growth\n",
    "    'epsgrowth',             # Growth\n",
    "]\n",
    "\n",
    "FUND_COLS = METRIC_COLS + RATIO_COLS + GROWTH_COLS\n",
    "print(f\"Using {len(FUND_COLS)} fundamental features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined fundamentals: 307,481 rows\n"
     ]
    }
   ],
   "source": [
    "# Merge fundamentals into single table\n",
    "metrics_sub = metrics[['symbol', 'date'] + [c for c in METRIC_COLS if c in metrics.columns]].copy()\n",
    "ratios_sub = ratios[['symbol', 'date'] + [c for c in RATIO_COLS if c in ratios.columns]].copy()\n",
    "growth_sub = growth[['symbol', 'date'] + [c for c in GROWTH_COLS if c in growth.columns]].copy()\n",
    "\n",
    "fundamentals = metrics_sub.merge(ratios_sub, on=['symbol', 'date'], how='outer')\n",
    "fundamentals = fundamentals.merge(growth_sub, on=['symbol', 'date'], how='outer')\n",
    "fundamentals['period_end'] = pd.to_datetime(fundamentals['date'])\n",
    "\n",
    "print(f\"Combined fundamentals: {len(fundamentals):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filing date coverage: 98.9%\n"
     ]
    }
   ],
   "source": [
    "# Add filing dates for point-in-time alignment\n",
    "filing_dates_clean = filing_dates[['symbol', 'period_end', 'filing_date']].copy()\n",
    "filing_dates_clean['period_end'] = pd.to_datetime(filing_dates_clean['period_end'])\n",
    "filing_dates_clean['filing_date'] = pd.to_datetime(filing_dates_clean['filing_date'])\n",
    "\n",
    "fundamentals = fundamentals.merge(\n",
    "    filing_dates_clean,\n",
    "    on=['symbol', 'period_end'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Use filing_date where available, fallback to period_end + 45 days\n",
    "FALLBACK_LAG_DAYS = 45\n",
    "fundamentals['available_date'] = fundamentals['filing_date'].fillna(\n",
    "    fundamentals['period_end'] + timedelta(days=FALLBACK_LAG_DAYS)\n",
    ")\n",
    "\n",
    "# Sort for merge_asof\n",
    "fundamentals = fundamentals.sort_values(['symbol', 'available_date'])\n",
    "\n",
    "print(f\"Filing date coverage: {fundamentals['filing_date'].notna().mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pit_join_fundamentals(earnings_df: pd.DataFrame, fund_df: pd.DataFrame, fund_cols: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Point-in-time join: for each earnings event, get most recent fundamentals\n",
    "    where available_date < earnings_date.\n",
    "    \"\"\"\n",
    "    # Prepare for merge_asof\n",
    "    earnings_sorted = earnings_df.sort_values('earnings_date').copy()\n",
    "    fund_sorted = fund_df.sort_values('available_date').copy()\n",
    "    \n",
    "    # Filter fund_cols to those that exist\n",
    "    fund_cols_exist = [c for c in fund_cols if c in fund_sorted.columns]\n",
    "    \n",
    "    merged = pd.merge_asof(\n",
    "        earnings_sorted,\n",
    "        fund_sorted[['symbol', 'available_date'] + fund_cols_exist],\n",
    "        left_on='earnings_date',\n",
    "        right_on='available_date',\n",
    "        by='symbol',\n",
    "        direction='backward'\n",
    "    )\n",
    "    \n",
    "    merged['has_fundamentals'] = merged[fund_cols_exist[0]].notna().astype(int)\n",
    "    merged = merged.drop(columns=['available_date'], errors='ignore')\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining fundamentals (point-in-time)...\n",
      "After fundamental join: (5343, 43)\n",
      "Has fundamentals: 5329 (99.7%)\n"
     ]
    }
   ],
   "source": [
    "# Join fundamentals\n",
    "print(\"Joining fundamentals (point-in-time)...\")\n",
    "features_df = pit_join_fundamentals(features_df, fundamentals, FUND_COLS)\n",
    "\n",
    "print(f\"After fundamental join: {features_df.shape}\")\n",
    "print(f\"Has fundamentals: {features_df['has_fundamentals'].sum()} ({features_df['has_fundamentals'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 5. Price Context Features\n",
    "\n",
    "Realized volatility, momentum, positioning before earnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prices: 5,888,410 rows, 5644 symbols\n"
     ]
    }
   ],
   "source": [
    "# Load price data\n",
    "prices = pd.read_parquet(DATA_DIR / 'prices.pqt')\n",
    "prices['date'] = pd.to_datetime(prices['date'])\n",
    "print(f\"Prices: {len(prices):,} rows, {prices['symbol'].nunique()} symbols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_price_context(prices_df: pd.DataFrame, earnings_df: pd.DataFrame, lookback: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute price-based features for each earnings event.\n",
    "    \"\"\"\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    # Pre-compute returns\n",
    "    prices_df = prices_df.sort_values(['symbol', 'date']).copy()\n",
    "    prices_df['return'] = prices_df.groupby('symbol')['close'].pct_change()\n",
    "    \n",
    "    # Group by symbol for faster lookup\n",
    "    prices_by_symbol = {symbol: grp.set_index('date') for symbol, grp in prices_df.groupby('symbol')}\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for _, row in tqdm(earnings_df.iterrows(), total=len(earnings_df), desc=\"Computing price context\"):\n",
    "        symbol = row['symbol']\n",
    "        earn_date = row['earnings_date']\n",
    "        \n",
    "        result = {\n",
    "            'symbol': symbol,\n",
    "            'earnings_date': earn_date,\n",
    "        }\n",
    "        \n",
    "        if symbol not in prices_by_symbol:\n",
    "            for col in ['rvol_5d', 'rvol_10d', 'rvol_20d', 'ret_5d', 'ret_10d', 'ret_20d',\n",
    "                       'dist_from_high_20d', 'dist_from_low_20d', 'gap_frequency', 'volume_ratio']:\n",
    "                result[col] = np.nan\n",
    "            results.append(result)\n",
    "            continue\n",
    "        \n",
    "        symbol_prices = prices_by_symbol[symbol]\n",
    "        \n",
    "        # Get prices before earnings\n",
    "        before = symbol_prices[symbol_prices.index < earn_date].tail(lookback + 5)\n",
    "        \n",
    "        if len(before) < 5:\n",
    "            for col in ['rvol_5d', 'rvol_10d', 'rvol_20d', 'ret_5d', 'ret_10d', 'ret_20d',\n",
    "                       'dist_from_high_20d', 'dist_from_low_20d', 'gap_frequency', 'volume_ratio']:\n",
    "                result[col] = np.nan\n",
    "            results.append(result)\n",
    "            continue\n",
    "        \n",
    "        returns = before['return'].dropna()\n",
    "        \n",
    "        # Realized volatility (annualized)\n",
    "        result['rvol_5d'] = returns.tail(5).std() * np.sqrt(252) if len(returns) >= 5 else np.nan\n",
    "        result['rvol_10d'] = returns.tail(10).std() * np.sqrt(252) if len(returns) >= 10 else np.nan\n",
    "        result['rvol_20d'] = returns.tail(20).std() * np.sqrt(252) if len(returns) >= 20 else np.nan\n",
    "        \n",
    "        # Momentum\n",
    "        closes = before['close']\n",
    "        result['ret_5d'] = (closes.iloc[-1] / closes.iloc[-5] - 1) if len(closes) >= 5 else np.nan\n",
    "        result['ret_10d'] = (closes.iloc[-1] / closes.iloc[-10] - 1) if len(closes) >= 10 else np.nan\n",
    "        result['ret_20d'] = (closes.iloc[-1] / closes.iloc[-20] - 1) if len(closes) >= 20 else np.nan\n",
    "        \n",
    "        # Position relative to recent range\n",
    "        if len(before) >= 20:\n",
    "            result['dist_from_high_20d'] = closes.iloc[-1] / before['high'].tail(20).max() - 1\n",
    "            result['dist_from_low_20d'] = closes.iloc[-1] / before['low'].tail(20).min() - 1\n",
    "        else:\n",
    "            result['dist_from_high_20d'] = np.nan\n",
    "            result['dist_from_low_20d'] = np.nan\n",
    "        \n",
    "        # Gap frequency (how often does stock gap > 2%?)\n",
    "        if len(before) > 1 and 'open' in before.columns:\n",
    "            gaps = np.abs(before['open'] / before['close'].shift(1) - 1)\n",
    "            result['gap_frequency'] = (gaps > 0.02).mean()\n",
    "        else:\n",
    "            result['gap_frequency'] = np.nan\n",
    "        \n",
    "        # Volume ratio (recent vs average)\n",
    "        if 'volume' in before.columns and len(before) >= 20:\n",
    "            recent_vol = before['volume'].tail(5).mean()\n",
    "            avg_vol = before['volume'].mean()\n",
    "            result['volume_ratio'] = recent_vol / avg_vol if avg_vol > 0 else np.nan\n",
    "        else:\n",
    "            result['volume_ratio'] = np.nan\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached price context...\n",
      "Price context shape: (5313, 12)\n"
     ]
    }
   ],
   "source": [
    "# Compute price context features\n",
    "price_context_file = EARNINGS_DIR / 'price_context_features.parquet'\n",
    "\n",
    "if price_context_file.exists():\n",
    "    print(\"Loading cached price context...\")\n",
    "    price_context = pd.read_parquet(price_context_file)\n",
    "else:\n",
    "    print(\"Computing price context features...\")\n",
    "    price_context = compute_price_context(prices, hist_features)\n",
    "    price_context.to_parquet(price_context_file, index=False)\n",
    "    print(f\"Saved to {price_context_file}\")\n",
    "\n",
    "print(f\"Price context shape: {price_context.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After price context merge: (5427, 53)\n"
     ]
    }
   ],
   "source": [
    "# Merge price context\n",
    "price_context['earnings_date'] = pd.to_datetime(price_context['earnings_date'])\n",
    "features_df = features_df.merge(\n",
    "    price_context,\n",
    "    on=['symbol', 'earnings_date'],\n",
    "    how='left'\n",
    ")\n",
    "print(f\"After price context merge: {features_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## 6. Earnings Surprise Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_earnings_surprises(symbol: str, limit: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"Fetch historical earnings data from FMP (actual vs estimated EPS).\"\"\"\n",
    "    url = f\"https://financialmodelingprep.com/stable/earnings?symbol={symbol}&apikey={FMP_KEY}\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        if r.status_code == 200:\n",
    "            data = r.json()\n",
    "            if data:\n",
    "                df = pd.DataFrame(data)\n",
    "                # Filter to rows with actual data and limit\n",
    "                if 'epsActual' in df.columns:\n",
    "                    df = df[df['epsActual'].notna()].head(limit)\n",
    "                return df\n",
    "    except:\n",
    "        pass\n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cell-29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cached surprises: 40191 rows\n"
     ]
    }
   ],
   "source": [
    "# Fetch/load earnings surprises\n",
    "surprise_cache_file = EARNINGS_DIR / 'earnings_surprises_cache.parquet'\n",
    "\n",
    "if surprise_cache_file.exists():\n",
    "    all_surprises = pd.read_parquet(surprise_cache_file)\n",
    "    print(f\"Loaded cached surprises: {len(all_surprises)} rows\")\n",
    "else:\n",
    "    print(\"Fetching earnings surprises...\")\n",
    "    # Use hist_features (always available) instead of features_df (created later)\n",
    "    symbols = hist_features['symbol'].unique()\n",
    "    \n",
    "    all_surprises = []\n",
    "    for i, symbol in enumerate(symbols):\n",
    "        if i > 0 and i % 50 == 0:\n",
    "            print(f\"  Progress: {i}/{len(symbols)}\")\n",
    "        \n",
    "        surprises = fetch_earnings_surprises(symbol)\n",
    "        if not surprises.empty:\n",
    "            surprises['symbol'] = symbol\n",
    "            all_surprises.append(surprises)\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    if all_surprises:\n",
    "        all_surprises = pd.concat(all_surprises, ignore_index=True)\n",
    "        all_surprises.to_parquet(surprise_cache_file, index=False)\n",
    "        print(f\"Cached surprises: {len(all_surprises)} rows\")\n",
    "    else:\n",
    "        all_surprises = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_surprise_features(features_df: pd.DataFrame, surprises_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute earnings surprise features for each event.\n",
    "    Only uses data available BEFORE the event.\n",
    "    \"\"\"\n",
    "    features_df = features_df.copy()\n",
    "    \n",
    "    if surprises_df.empty:\n",
    "        features_df['surprise_pct_mean'] = np.nan\n",
    "        features_df['surprise_pct_std'] = np.nan\n",
    "        features_df['beat_rate'] = np.nan\n",
    "        features_df['surprise_streak'] = np.nan\n",
    "        return features_df\n",
    "    \n",
    "    surprises_df = surprises_df.copy()\n",
    "    surprises_df['date'] = pd.to_datetime(surprises_df['date'])\n",
    "    \n",
    "    # Initialize columns\n",
    "    features_df['surprise_pct_mean'] = np.nan\n",
    "    features_df['surprise_pct_std'] = np.nan\n",
    "    features_df['beat_rate'] = np.nan\n",
    "    features_df['surprise_streak'] = np.nan\n",
    "    \n",
    "    # Group surprises by symbol for faster lookup\n",
    "    surprises_by_symbol = {sym: grp for sym, grp in surprises_df.groupby('symbol')}\n",
    "    \n",
    "    for idx, row in features_df.iterrows():\n",
    "        symbol = row['symbol']\n",
    "        earn_date = row['earnings_date']\n",
    "        \n",
    "        if symbol not in surprises_by_symbol:\n",
    "            continue\n",
    "            \n",
    "        symbol_surprises = surprises_by_symbol[symbol]\n",
    "        \n",
    "        # Get past surprises for this symbol (before current earnings)\n",
    "        past = symbol_surprises[symbol_surprises['date'] < earn_date].sort_values('date')\n",
    "        \n",
    "        # Use epsActual/epsEstimated from FMP /stable/earnings endpoint\n",
    "        if len(past) >= 1 and 'epsActual' in past.columns and 'epsEstimated' in past.columns:\n",
    "            # Surprise percentage\n",
    "            past_valid = past.dropna(subset=['epsActual', 'epsEstimated'])\n",
    "            if len(past_valid) > 0:\n",
    "                past_valid = past_valid.copy()\n",
    "                past_valid['surprise_pct'] = (past_valid['epsActual'] - past_valid['epsEstimated']) / past_valid['epsEstimated'].abs().clip(lower=0.01)\n",
    "                \n",
    "                features_df.at[idx, 'surprise_pct_mean'] = past_valid['surprise_pct'].mean()\n",
    "                features_df.at[idx, 'surprise_pct_std'] = past_valid['surprise_pct'].std() if len(past_valid) > 1 else 0\n",
    "                \n",
    "                # Beat rate\n",
    "                features_df.at[idx, 'beat_rate'] = (past_valid['epsActual'] > past_valid['epsEstimated']).mean()\n",
    "                \n",
    "                # Recent streak\n",
    "                recent = past_valid.tail(4)\n",
    "                beats = (recent['epsActual'] > recent['epsEstimated']).values\n",
    "                streak = 0\n",
    "                if len(beats) > 0:\n",
    "                    last_val = beats[-1]\n",
    "                    for b in reversed(beats):\n",
    "                        if b == last_val:\n",
    "                            streak += 1\n",
    "                        else:\n",
    "                            break\n",
    "                    if not last_val:\n",
    "                        streak = -streak\n",
    "                features_df.at[idx, 'surprise_streak'] = streak\n",
    "    \n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cell-31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features after surprises: (5427, 57)\n"
     ]
    }
   ],
   "source": [
    "# Add surprise features\n",
    "features_df = compute_surprise_features(features_df, all_surprises)\n",
    "print(f\"Features after surprises: {features_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## 7. Timing Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cell-33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timing distribution:\n",
      "timing\n",
      "unknown    5631\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Add timing from earnings calendar\n",
    "def parse_timing(time_str):\n",
    "    if pd.isna(time_str):\n",
    "        return 'unknown'\n",
    "    time_str = str(time_str).lower()\n",
    "    if 'bmo' in time_str or 'before' in time_str:\n",
    "        return 'BMO'\n",
    "    elif 'amc' in time_str or 'after' in time_str:\n",
    "        return 'AMC'\n",
    "    return 'unknown'\n",
    "\n",
    "if 'time' in earnings_cal.columns:\n",
    "    earnings_cal['timing'] = earnings_cal['time'].apply(parse_timing)\n",
    "else:\n",
    "    earnings_cal['timing'] = 'unknown'\n",
    "\n",
    "timing_df = earnings_cal[['symbol', 'date', 'timing']].rename(columns={'date': 'earnings_date'})\n",
    "\n",
    "features_df = features_df.merge(timing_df, on=['symbol', 'earnings_date'], how='left')\n",
    "features_df['timing'] = features_df['timing'].fillna('unknown')\n",
    "\n",
    "print(\"Timing distribution:\")\n",
    "print(features_df['timing'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add calendar features\n",
    "features_df['day_of_week'] = features_df['earnings_date'].dt.dayofweek\n",
    "features_df['month'] = features_df['earnings_date'].dt.month\n",
    "features_df['quarter'] = features_df['earnings_date'].dt.quarter\n",
    "\n",
    "# Earnings season flag\n",
    "def is_earnings_season(month):\n",
    "    return month in [1, 2, 4, 5, 7, 8, 10, 11]\n",
    "\n",
    "features_df['is_earnings_season'] = features_df['month'].apply(is_earnings_season).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## 8. Final Dataset Assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cell-36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns: 62\n",
      "\n",
      "Feature columns by category:\n",
      "Historical: 10 cols\n",
      "News PCA: 10 cols\n",
      "Fundamentals: 16 cols\n",
      "Price context: 10 cols\n",
      "Surprise: 4 cols\n",
      "Timing: 4 cols\n",
      "\n",
      "Total model features: 56 (expected ~52)\n"
     ]
    }
   ],
   "source": [
    "# List all columns\n",
    "print(f\"Total columns: {len(features_df.columns)}\")\n",
    "print(\"\\nFeature columns by category:\")\n",
    "\n",
    "# Historical\n",
    "hist_cols = ['hist_move_mean', 'hist_move_median', 'hist_move_std', 'hist_move_max',\n",
    "             'hist_move_min', 'hist_move_cv', 'recent_move_mean', 'move_trend',\n",
    "             'gap_continuation_ratio', 'n_past_earnings']\n",
    "print(f\"Historical: {len(hist_cols)} cols\")\n",
    "\n",
    "# News PCA features (not full 768-dim embeddings!)\n",
    "news_pca_cols = [c for c in features_df.columns if c.startswith('news_pca_')]\n",
    "print(f\"News PCA: {len(news_pca_cols)} cols\")\n",
    "\n",
    "# Fundamentals\n",
    "fund_cols_actual = [c for c in FUND_COLS if c in features_df.columns]\n",
    "print(f\"Fundamentals: {len(fund_cols_actual)} cols\")\n",
    "\n",
    "# Price context\n",
    "price_cols = ['rvol_5d', 'rvol_10d', 'rvol_20d', 'ret_5d', 'ret_10d', 'ret_20d',\n",
    "              'dist_from_high_20d', 'dist_from_low_20d', 'gap_frequency', 'volume_ratio']\n",
    "print(f\"Price context: {len(price_cols)} cols\")\n",
    "\n",
    "# Surprise\n",
    "surprise_cols = ['surprise_pct_mean', 'surprise_pct_std', 'beat_rate', 'surprise_streak']\n",
    "print(f\"Surprise: {len(surprise_cols)} cols\")\n",
    "\n",
    "# Timing\n",
    "timing_cols = ['day_of_week', 'month', 'quarter', 'is_earnings_season']\n",
    "print(f\"Timing: {len(timing_cols)} cols\")\n",
    "\n",
    "total_features = len(hist_cols) + len(news_pca_cols) + len(fund_cols_actual) + len(price_cols) + len(surprise_cols) + len(timing_cols) + 2  # +2 for news_count and timing_encoded\n",
    "print(f\"\\nTotal model features: {total_features} (expected ~52)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cell-37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature coverage:\n",
      "  hist_move_mean: 59.6%\n",
      "  hist_move_median: 59.6%\n",
      "  hist_move_std: 59.6%\n",
      "  hist_move_max: 59.6%\n",
      "  hist_move_min: 59.6%\n",
      "  hist_move_cv: 59.6%\n",
      "  recent_move_mean: 59.6%\n",
      "  move_trend: 59.6%\n",
      "  gap_continuation_ratio: 59.6%\n",
      "  n_past_earnings: 100.0%\n",
      "  rvol_5d: 97.6%\n",
      "  rvol_10d: 97.5%\n",
      "  rvol_20d: 97.0%\n",
      "  ret_5d: 97.6%\n",
      "  ret_10d: 97.5%\n",
      "  ret_20d: 97.0%\n",
      "  dist_from_high_20d: 97.0%\n",
      "  dist_from_low_20d: 97.0%\n",
      "  gap_frequency: 97.6%\n",
      "  volume_ratio: 97.0%\n",
      "  surprise_pct_mean: 72.6%\n",
      "  surprise_pct_std: 72.6%\n",
      "  beat_rate: 72.6%\n",
      "  surprise_streak: 72.6%\n",
      "  evToEBITDA: 99.8%\n",
      "  freeCashFlowYield: 99.8%\n",
      "  earningsYield: 99.8%\n",
      "  returnOnEquity: 99.8%\n",
      "  returnOnAssets: 99.8%\n"
     ]
    }
   ],
   "source": [
    "# Feature coverage\n",
    "print(\"\\nFeature coverage:\")\n",
    "for col in hist_cols + price_cols + surprise_cols + fund_cols_actual[:5]:\n",
    "    if col in features_df.columns:\n",
    "        coverage = features_df[col].notna().mean() * 100\n",
    "        print(f\"  {col}: {coverage:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cell-38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtering dataset...\n",
      "Starting rows: 5631\n",
      "With target: 5631\n",
      "With history (n>=1): 3358\n",
      "After outlier removal: 3350\n"
     ]
    }
   ],
   "source": [
    "# Filter to usable rows\n",
    "print(f\"\\nFiltering dataset...\")\n",
    "print(f\"Starting rows: {len(features_df)}\")\n",
    "\n",
    "# Must have target\n",
    "df_clean = features_df[features_df['target_move'].notna()].copy()\n",
    "print(f\"With target: {len(df_clean)}\")\n",
    "\n",
    "# Must have some history\n",
    "df_clean = df_clean[df_clean['n_past_earnings'] >= 1]\n",
    "print(f\"With history (n>=1): {len(df_clean)}\")\n",
    "\n",
    "# Remove extreme outliers (>100% moves)\n",
    "df_clean = df_clean[df_clean['target_move'] < 1.0]\n",
    "print(f\"After outlier removal: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cell-39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: 51\n",
      "Categorical features: 1\n",
      "\n",
      "Feature list:\n",
      "  1. hist_move_mean\n",
      "  2. hist_move_median\n",
      "  3. hist_move_std\n",
      "  4. hist_move_max\n",
      "  5. hist_move_min\n",
      "  6. hist_move_cv\n",
      "  7. recent_move_mean\n",
      "  8. move_trend\n",
      "  9. gap_continuation_ratio\n",
      "  10. n_past_earnings\n",
      "  11. rvol_5d\n",
      "  12. rvol_10d\n",
      "  13. rvol_20d\n",
      "  14. ret_5d\n",
      "  15. ret_10d\n",
      "  16. ret_20d\n",
      "  17. dist_from_high_20d\n",
      "  18. dist_from_low_20d\n",
      "  19. gap_frequency\n",
      "  20. volume_ratio\n",
      "  21. surprise_pct_mean\n",
      "  22. surprise_pct_std\n",
      "  23. beat_rate\n",
      "  24. surprise_streak\n",
      "  25. evToEBITDA\n",
      "  26. freeCashFlowYield\n",
      "  27. earningsYield\n",
      "  28. returnOnEquity\n",
      "  29. returnOnAssets\n",
      "  30. currentRatio\n",
      "  31. priceToEarningsRatio\n",
      "  32. priceToBookRatio\n",
      "  33. priceToSalesRatio\n",
      "  34. grossProfitMargin\n",
      "  35. operatingProfitMargin\n",
      "  36. netProfitMargin\n",
      "  37. debtToEquityRatio\n",
      "  38. revenueGrowth\n",
      "  39. netIncomeGrowth\n",
      "  40. epsgrowth\n",
      "  41. pre_earnings_news_count\n",
      "  42. news_pca_0\n",
      "  43. news_pca_1\n",
      "  44. news_pca_2\n",
      "  45. news_pca_3\n",
      "  46. news_pca_4\n",
      "  47. news_pca_5\n",
      "  48. news_pca_6\n",
      "  49. news_pca_7\n",
      "  50. news_pca_8\n",
      "  51. news_pca_9\n"
     ]
    }
   ],
   "source": [
    "# Define all feature columns for model\n",
    "ALL_NUMERIC_FEATURES = hist_cols + price_cols + surprise_cols + fund_cols_actual + ['pre_earnings_news_count'] + news_pca_cols\n",
    "ALL_NUMERIC_FEATURES = [c for c in ALL_NUMERIC_FEATURES if c in df_clean.columns]\n",
    "\n",
    "CATEGORICAL_FEATURES = ['timing']\n",
    "\n",
    "print(f\"Numeric features: {len(ALL_NUMERIC_FEATURES)}\")\n",
    "print(f\"Categorical features: {len(CATEGORICAL_FEATURES)}\")\n",
    "print(f\"\\nFeature list:\")\n",
    "for i, col in enumerate(ALL_NUMERIC_FEATURES):\n",
    "    print(f\"  {i+1}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cell-40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values filled.\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values\n",
    "for col in ALL_NUMERIC_FEATURES:\n",
    "    if col in df_clean.columns:\n",
    "        median_val = df_clean[col].median()\n",
    "        df_clean[col] = df_clean[col].fillna(median_val)\n",
    "\n",
    "for col in CATEGORICAL_FEATURES:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = df_clean[col].fillna('unknown')\n",
    "\n",
    "# News embeddings already default to 0\n",
    "print(\"Missing values filled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cell-41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved ML features: (3350, 62)\n",
      "File: ../data/earnings/ml_features.parquet\n",
      "File size: 1.2 MB\n"
     ]
    }
   ],
   "source": [
    "# Save final dataset\n",
    "output_file = EARNINGS_DIR / 'ml_features.parquet'\n",
    "df_clean.to_parquet(output_file, index=False)\n",
    "\n",
    "print(f\"\\nSaved ML features: {df_clean.shape}\")\n",
    "print(f\"File: {output_file}\")\n",
    "print(f\"File size: {output_file.stat().st_size / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "## 9. Feature Correlation with Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cell-43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top feature correlations with target |move|:\n",
      "               feature  correlation\n",
      "              rvol_10d     0.225289\n",
      "              rvol_20d     0.208986\n",
      "               rvol_5d     0.204769\n",
      "     dist_from_low_20d     0.198595\n",
      "      priceToBookRatio    -0.167643\n",
      "         hist_move_min     0.160385\n",
      "      hist_move_median     0.131637\n",
      "      recent_move_mean     0.125316\n",
      "            news_pca_4     0.123456\n",
      "            news_pca_8     0.122636\n",
      "        hist_move_mean     0.120830\n",
      "                ret_5d     0.119312\n",
      "               ret_10d     0.117211\n",
      "    dist_from_high_20d    -0.108244\n",
      "         gap_frequency     0.104386\n",
      "            news_pca_2     0.101398\n",
      "            news_pca_7    -0.088076\n",
      "     debtToEquityRatio    -0.084581\n",
      "            news_pca_1     0.077258\n",
      "         hist_move_max     0.075607\n",
      "            news_pca_5    -0.073776\n",
      "               ret_20d     0.069772\n",
      "          volume_ratio     0.069288\n",
      "gap_continuation_ratio     0.067769\n",
      "            news_pca_6    -0.065369\n"
     ]
    }
   ],
   "source": [
    "# Check correlation of features with target\n",
    "correlations = {}\n",
    "for col in ALL_NUMERIC_FEATURES:\n",
    "    if col in df_clean.columns:\n",
    "        corr = df_clean[col].corr(df_clean['target_move'])\n",
    "        correlations[col] = corr\n",
    "\n",
    "corr_df = pd.DataFrame({\n",
    "    'feature': correlations.keys(),\n",
    "    'correlation': correlations.values()\n",
    "}).sort_values('correlation', key=abs, ascending=False)\n",
    "\n",
    "print(\"Top feature correlations with target |move|:\")\n",
    "print(corr_df.head(25).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cell-44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "News PCA feature correlations:\n",
      "   feature  correlation\n",
      "news_pca_4     0.123456\n",
      "news_pca_8     0.122636\n",
      "news_pca_2     0.101398\n",
      "news_pca_7    -0.088076\n",
      "news_pca_1     0.077258\n",
      "news_pca_5    -0.073776\n",
      "news_pca_6    -0.065369\n",
      "news_pca_0    -0.050050\n",
      "news_pca_9    -0.034627\n",
      "news_pca_3    -0.001125\n",
      "\n",
      "Mean |correlation| for news PCA: 0.0738\n"
     ]
    }
   ],
   "source": [
    "# News PCA correlations\n",
    "news_pca_corrs = {}\n",
    "for col in news_pca_cols:\n",
    "    if col in df_clean.columns:\n",
    "        corr = df_clean[col].corr(df_clean['target_move'])\n",
    "        news_pca_corrs[col] = corr\n",
    "\n",
    "news_corr_df = pd.DataFrame({\n",
    "    'feature': news_pca_corrs.keys(),\n",
    "    'correlation': news_pca_corrs.values()\n",
    "}).sort_values('correlation', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nNews PCA feature correlations:\")\n",
    "print(news_corr_df.to_string(index=False))\n",
    "print(f\"\\nMean |correlation| for news PCA: {np.abs(list(news_pca_corrs.values())).mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-45",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Features engineered (~52 total):\n",
    "1. **Historical earnings** (10 features) - past moves, consistency, trends\n",
    "2. **Pre-earnings news PCA** (10 features) - PCA-reduced from 768-dim embeddings\n",
    "3. **Fundamentals** (~16 features) - key metrics, ratios, growth (point-in-time)\n",
    "4. **Price context** (10 features) - realized vol, momentum, positioning\n",
    "5. **Earnings surprises** (4 features) - beat/miss history, streaks\n",
    "6. **Timing** (4+1 features) - day of week, earnings season, timing_encoded\n",
    "\n",
    "**Key outputs:**\n",
    "- `ml_features.parquet` - training dataset\n",
    "- `news_pca.joblib` - PCA model for live inference\n",
    "\n",
    "Ready for model training in `1.1 model_training.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb3c2a-675a-44da-8f4f-e7d9aedde017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c92e2-fef9-457e-af8f-e72ff2b66226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
