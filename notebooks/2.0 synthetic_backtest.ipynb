{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Synthetic Backtest\n",
    "\n",
    "Backtest the earnings straddle strategy WITHOUT historical options data.\n",
    "\n",
    "## Approach\n",
    "\n",
    "1. **Re-train model on early period** (2024-03 to 2024-09) to avoid lookahead\n",
    "2. **Out-of-sample period** (2024-10 to 2025-12) - ~14 months\n",
    "3. **Estimate straddle price** using Black-Scholes approximation:\n",
    "   - `straddle_price ≈ 0.8 × spot × IV × sqrt(T)`\n",
    "   - Use historical realized vol as IV proxy\n",
    "4. **Apply liquidity/spread filters** conservatively\n",
    "5. **Calculate P&L** = realized_move - estimated_straddle_cost\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- No real bid/ask spreads (estimate 10-15% of mid)\n",
    "- No real IV (use realized vol as proxy)\n",
    "- No fill probability model\n",
    "- Assumes all trades fill at estimated price\n",
    "\n",
    "This gives directional insight, not exact P&L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_DIR = Path('../data/earnings')\n",
    "MODEL_DIR = Path('../models')\n",
    "\n",
    "# Quantiles to predict\n",
    "QUANTILES = [0.50, 0.75, 0.90, 0.95]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Define Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 3350 rows, 1419 symbols\n",
      "Date range: 2024-03-30 to 2025-12-18\n",
      "\n",
      "Train: 534 samples (2024-03-30 to 2024-09-30)\n",
      "Test:  2816 samples (2024-10-01 to 2025-12-18)\n"
     ]
    }
   ],
   "source": [
    "# Load the feature dataset\n",
    "df = pd.read_parquet(DATA_DIR / 'ml_features.parquet')\n",
    "df['earnings_date'] = pd.to_datetime(df['earnings_date'])\n",
    "df = df.sort_values('earnings_date').reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset: {len(df)} rows, {df['symbol'].nunique()} symbols\")\n",
    "print(f\"Date range: {df['earnings_date'].min().date()} to {df['earnings_date'].max().date()}\")\n",
    "\n",
    "# Define train/test split\n",
    "# Train: 2024-03 to 2024-09 (6 months)\n",
    "# Test: 2024-10 to 2025-12 (14 months)\n",
    "TRAIN_END = pd.Timestamp('2024-09-30')\n",
    "\n",
    "train_df = df[df['earnings_date'] <= TRAIN_END].copy()\n",
    "test_df = df[df['earnings_date'] > TRAIN_END].copy()\n",
    "\n",
    "print(f\"\\nTrain: {len(train_df)} samples ({train_df['earnings_date'].min().date()} to {train_df['earnings_date'].max().date()})\")\n",
    "print(f\"Test:  {len(test_df)} samples ({test_df['earnings_date'].min().date()} to {test_df['earnings_date'].max().date()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train Quantile Models on Early Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load feature config from production models\nimport json\nfrom sklearn.preprocessing import LabelEncoder\n\nwith open(MODEL_DIR / 'feature_config.json') as f:\n    config = json.load(f)\n\nFEATURE_COLS = config['feature_cols']\nprint(f\"Features: {len(FEATURE_COLS)}\")\n\n# Create timing_encoded feature (not in parquet, created during training)\nle = LabelEncoder()\ndf['timing_encoded'] = le.fit_transform(df['timing'].fillna('unknown'))\n\n# Update train/test splits with the new column\ntrain_df = df[df['earnings_date'] <= TRAIN_END].copy()\ntest_df = df[df['earnings_date'] > TRAIN_END].copy()\n\nprint(f\"timing_encoded created: {df['timing_encoded'].unique()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_quantile_model(X_train, y_train, X_val, y_val, quantile):\n",
    "    \"\"\"Train a LightGBM quantile regression model.\"\"\"\n",
    "    params = {\n",
    "        'objective': 'quantile',\n",
    "        'alpha': quantile,\n",
    "        'metric': 'quantile',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'seed': 42,\n",
    "    }\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=500,\n",
    "        valid_sets=[train_data, val_data],\n",
    "        valid_names=['train', 'val'],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "            lgb.log_evaluation(period=0),\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['timing_encoded'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Prepare training data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mFEATURE_COLS\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      3\u001b[0m y_train \u001b[38;5;241m=\u001b[39m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_move\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Use last 20% as validation\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['timing_encoded'] not in index\""
     ]
    }
   ],
   "source": [
    "# Prepare training data\n",
    "X_train = train_df[FEATURE_COLS].values\n",
    "y_train = train_df['target_move'].values\n",
    "\n",
    "# Use last 20% as validation\n",
    "val_size = int(len(X_train) * 0.2)\n",
    "X_val = X_train[-val_size:]\n",
    "y_val = y_train[-val_size:]\n",
    "X_train_sub = X_train[:-val_size]\n",
    "y_train_sub = y_train[:-val_size]\n",
    "\n",
    "print(f\"Training: {len(X_train_sub)}, Validation: {len(X_val)}\")\n",
    "\n",
    "# Train models\n",
    "backtest_models = {}\n",
    "for q in QUANTILES:\n",
    "    print(f\"Training q{int(q*100)}...\")\n",
    "    model = train_quantile_model(X_train_sub, y_train_sub, X_val, y_val, q)\n",
    "    backtest_models[q] = model\n",
    "    print(f\"  Done ({model.best_iteration} iterations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "X_test = test_df[FEATURE_COLS].values\n",
    "\n",
    "for q in QUANTILES:\n",
    "    col = f'pred_q{int(q*100)}'\n",
    "    test_df[col] = backtest_models[q].predict(X_test)\n",
    "\n",
    "# Calculate edge\n",
    "test_df['edge_q75'] = test_df['pred_q75'] - test_df['hist_move_mean']\n",
    "\n",
    "print(f\"Predictions generated for {len(test_df)} test samples\")\n",
    "print(f\"\\nEdge distribution:\")\n",
    "print(test_df['edge_q75'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Estimate Straddle Costs\n",
    "\n",
    "Without real options data, we estimate straddle price using:\n",
    "- `straddle ≈ 0.8 × spot × IV × sqrt(T)`\n",
    "- Use 20-day realized vol as IV proxy (typically IV > RV for earnings)\n",
    "- Apply IV multiplier to approximate earnings IV inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_straddle_price(row, iv_multiplier=1.5, days_to_expiry=2):\n",
    "    \"\"\"\n",
    "    Estimate ATM straddle price as fraction of spot.\n",
    "    \n",
    "    Args:\n",
    "        row: DataFrame row with 'rvol_20d' (20-day realized vol, annualized)\n",
    "        iv_multiplier: How much IV inflates vs realized vol for earnings\n",
    "        days_to_expiry: Days until option expiration\n",
    "    \n",
    "    Returns:\n",
    "        Straddle price as fraction of spot (e.g., 0.05 = 5%)\n",
    "    \"\"\"\n",
    "    rvol = row.get('rvol_20d', 0.3)  # Default 30% if missing\n",
    "    if pd.isna(rvol) or rvol <= 0:\n",
    "        rvol = 0.3\n",
    "    \n",
    "    # Estimate IV (realized vol * multiplier for earnings)\n",
    "    iv = rvol * iv_multiplier\n",
    "    \n",
    "    # Time to expiry in years\n",
    "    T = days_to_expiry / 252\n",
    "    \n",
    "    # Black-Scholes ATM straddle approximation\n",
    "    # straddle ≈ 0.8 × S × σ × √T\n",
    "    straddle_pct = 0.8 * iv * np.sqrt(T)\n",
    "    \n",
    "    return straddle_pct\n",
    "\n",
    "# Estimate straddle costs for test set\n",
    "test_df['est_straddle_pct'] = test_df.apply(estimate_straddle_price, axis=1)\n",
    "\n",
    "print(\"Estimated straddle costs (as % of spot):\")\n",
    "print(test_df['est_straddle_pct'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare estimated straddle cost vs hist_move_mean (our implied move proxy)\n",
    "print(\"Comparison: estimated straddle vs historical mean move\")\n",
    "print(f\"\\nEstimated straddle (mean): {test_df['est_straddle_pct'].mean():.2%}\")\n",
    "print(f\"Historical mean move:      {test_df['hist_move_mean'].mean():.2%}\")\n",
    "print(f\"Actual realized move:      {test_df['target_move'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Apply Trading Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_trading_filters(df, edge_threshold=0.05, spread_estimate=0.12):\n",
    "    \"\"\"\n",
    "    Apply trading filters similar to live strategy.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with predictions\n",
    "        edge_threshold: Minimum edge (pred_q75 - implied_move) to trade\n",
    "        spread_estimate: Estimated bid-ask spread as fraction of mid\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with 'trade' column indicating tradeable candidates\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Edge filter: pred_q75 > implied_move + cost_buffer\n",
    "    # Use hist_move_mean as implied move proxy\n",
    "    cost_buffer = spread_estimate / 2  # Half the spread as execution cost\n",
    "    result['trade'] = result['edge_q75'] > (edge_threshold + cost_buffer)\n",
    "    \n",
    "    # Additional filters (conservative estimates)\n",
    "    # - Skip very low vol stocks (straddle too cheap, likely wide spreads)\n",
    "    result.loc[result['rvol_20d'] < 0.20, 'trade'] = False\n",
    "    \n",
    "    # - Skip extreme edge (likely data issues)\n",
    "    result.loc[result['edge_q75'] > 0.50, 'trade'] = False\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply filters with different thresholds\n",
    "for threshold in [0.02, 0.05, 0.10]:\n",
    "    filtered = apply_trading_filters(test_df, edge_threshold=threshold)\n",
    "    trade_count = filtered['trade'].sum()\n",
    "    print(f\"Edge threshold {threshold:.0%}: {trade_count} trades ({trade_count/len(filtered):.1%} of candidates)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 5% edge threshold (matches live strategy)\n",
    "EDGE_THRESHOLD = 0.05\n",
    "SPREAD_ESTIMATE = 0.12  # 12% bid-ask spread\n",
    "\n",
    "backtest_df = apply_trading_filters(test_df, edge_threshold=EDGE_THRESHOLD, spread_estimate=SPREAD_ESTIMATE)\n",
    "trades_df = backtest_df[backtest_df['trade']].copy()\n",
    "\n",
    "print(f\"Total test samples: {len(backtest_df)}\")\n",
    "print(f\"Tradeable: {len(trades_df)} ({len(trades_df)/len(backtest_df):.1%})\")\n",
    "print(f\"\\nTrades per month: {len(trades_df) / 14:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate Synthetic P&L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_synthetic_pnl(row, spread_cost=0.12):\n",
    "    \"\"\"\n",
    "    Calculate synthetic straddle P&L.\n",
    "    \n",
    "    P&L = realized_move - straddle_cost - spread_costs\n",
    "    \n",
    "    All values as fraction of spot.\n",
    "    \"\"\"\n",
    "    realized_move = row['target_move']\n",
    "    straddle_cost = row['est_straddle_pct']\n",
    "    \n",
    "    # Entry cost: straddle at ask (mid + half spread)\n",
    "    entry_cost = straddle_cost * (1 + spread_cost / 2)\n",
    "    \n",
    "    # Exit value: intrinsic value minus exit slippage\n",
    "    # At expiration, straddle worth = |move| (intrinsic only)\n",
    "    # Exit at bid (mid - half spread), applied to realized value\n",
    "    exit_value = realized_move * (1 - spread_cost / 4)  # Less slippage on exit\n",
    "    \n",
    "    pnl = exit_value - entry_cost\n",
    "    return pnl\n",
    "\n",
    "# Calculate P&L for all trades\n",
    "trades_df['pnl_pct'] = trades_df.apply(calculate_synthetic_pnl, axis=1)\n",
    "\n",
    "print(\"=== SYNTHETIC P&L RESULTS ===\")\n",
    "print(f\"\\nTotal trades: {len(trades_df)}\")\n",
    "print(f\"Win rate: {(trades_df['pnl_pct'] > 0).mean():.1%}\")\n",
    "print(f\"\\nP&L (as % of position):\")\n",
    "print(f\"  Mean:   {trades_df['pnl_pct'].mean():.2%}\")\n",
    "print(f\"  Median: {trades_df['pnl_pct'].median():.2%}\")\n",
    "print(f\"  Std:    {trades_df['pnl_pct'].std():.2%}\")\n",
    "print(f\"  Min:    {trades_df['pnl_pct'].min():.2%}\")\n",
    "print(f\"  Max:    {trades_df['pnl_pct'].max():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative P&L\n",
    "# Assume fixed $300 position size per trade\n",
    "POSITION_SIZE = 300\n",
    "\n",
    "trades_df = trades_df.sort_values('earnings_date')\n",
    "trades_df['pnl_dollars'] = trades_df['pnl_pct'] * POSITION_SIZE\n",
    "trades_df['cumulative_pnl'] = trades_df['pnl_dollars'].cumsum()\n",
    "\n",
    "print(f\"=== CUMULATIVE RESULTS (${POSITION_SIZE}/trade) ===\")\n",
    "print(f\"\\nTotal P&L: ${trades_df['cumulative_pnl'].iloc[-1]:.2f}\")\n",
    "print(f\"Per trade: ${trades_df['pnl_dollars'].mean():.2f}\")\n",
    "\n",
    "# Sharpe-like ratio (monthly)\n",
    "monthly_pnl = trades_df.set_index('earnings_date')['pnl_dollars'].resample('M').sum()\n",
    "if len(monthly_pnl) > 1 and monthly_pnl.std() > 0:\n",
    "    monthly_sharpe = monthly_pnl.mean() / monthly_pnl.std() * np.sqrt(12)\n",
    "    print(f\"\\nAnnualized Sharpe (approx): {monthly_sharpe:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative P&L\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(trades_df['earnings_date'], trades_df['cumulative_pnl'])\n",
    "plt.axhline(0, color='black', linestyle='--', alpha=0.3)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative P&L ($)')\n",
    "plt.title(f'Synthetic Backtest: Cumulative P&L\\n({len(trades_df)} trades, ${POSITION_SIZE}/trade, edge>{EDGE_THRESHOLD:.0%})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analysis by Edge Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze P&L by edge bucket\n",
    "trades_df['edge_bucket'] = pd.cut(\n",
    "    trades_df['edge_q75'],\n",
    "    bins=[0.05, 0.10, 0.15, 0.20, 0.50],\n",
    "    labels=['5-10%', '10-15%', '15-20%', '20%+']\n",
    ")\n",
    "\n",
    "print(\"=== P&L BY EDGE BUCKET ===\")\n",
    "bucket_stats = trades_df.groupby('edge_bucket', observed=True).agg({\n",
    "    'pnl_pct': ['count', 'mean', 'std'],\n",
    "    'target_move': 'mean',\n",
    "}).round(4)\n",
    "bucket_stats.columns = ['Count', 'Mean P&L', 'Std P&L', 'Mean Move']\n",
    "print(bucket_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Win rate by edge bucket\n",
    "print(\"\\n=== WIN RATE BY EDGE BUCKET ===\")\n",
    "for bucket in trades_df['edge_bucket'].dropna().unique():\n",
    "    bucket_trades = trades_df[trades_df['edge_bucket'] == bucket]\n",
    "    win_rate = (bucket_trades['pnl_pct'] > 0).mean()\n",
    "    print(f\"{bucket}: {win_rate:.1%} ({len(bucket_trades)} trades)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison: Traded vs Non-Traded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate hypothetical P&L for non-traded candidates\n",
    "non_trades = backtest_df[~backtest_df['trade']].copy()\n",
    "non_trades['pnl_pct'] = non_trades.apply(calculate_synthetic_pnl, axis=1)\n",
    "\n",
    "print(\"=== TRADED vs NON-TRADED ===\")\n",
    "print(f\"\\nTraded ({len(trades_df)}):\")\n",
    "print(f\"  Mean P&L: {trades_df['pnl_pct'].mean():.2%}\")\n",
    "print(f\"  Win rate: {(trades_df['pnl_pct'] > 0).mean():.1%}\")\n",
    "\n",
    "print(f\"\\nNon-traded ({len(non_trades)}):\")\n",
    "print(f\"  Mean P&L: {non_trades['pnl_pct'].mean():.2%}\")\n",
    "print(f\"  Win rate: {(non_trades['pnl_pct'] > 0).mean():.1%}\")\n",
    "\n",
    "print(f\"\\nEdge of model: {trades_df['pnl_pct'].mean() - non_trades['pnl_pct'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Monthly Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly P&L breakdown\n",
    "trades_df['month'] = trades_df['earnings_date'].dt.to_period('M')\n",
    "\n",
    "monthly = trades_df.groupby('month').agg({\n",
    "    'pnl_pct': ['count', 'sum', 'mean'],\n",
    "    'pnl_dollars': 'sum',\n",
    "}).round(4)\n",
    "monthly.columns = ['Trades', 'Total P&L %', 'Avg P&L %', 'Total $ P&L']\n",
    "\n",
    "print(\"=== MONTHLY BREAKDOWN ===\")\n",
    "print(monthly.to_string())\n",
    "\n",
    "print(f\"\\n=== SUMMARY ===\")\n",
    "print(f\"Profitable months: {(monthly['Total $ P&L'] > 0).sum()} / {len(monthly)}\")\n",
    "print(f\"Best month:  ${monthly['Total $ P&L'].max():.2f}\")\n",
    "print(f\"Worst month: ${monthly['Total $ P&L'].min():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different parameters\n",
    "print(\"=== SENSITIVITY ANALYSIS ===\")\n",
    "print(\"\\nVarying edge threshold:\")\n",
    "\n",
    "for threshold in [0.02, 0.05, 0.08, 0.10, 0.15]:\n",
    "    filtered = apply_trading_filters(test_df, edge_threshold=threshold)\n",
    "    trades = filtered[filtered['trade']].copy()\n",
    "    if len(trades) > 0:\n",
    "        trades['pnl_pct'] = trades.apply(calculate_synthetic_pnl, axis=1)\n",
    "        print(f\"  {threshold:.0%}: {len(trades):4d} trades, \"\n",
    "              f\"win rate {(trades['pnl_pct'] > 0).mean():.1%}, \"\n",
    "              f\"mean P&L {trades['pnl_pct'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different spread assumptions\n",
    "print(\"\\nVarying spread estimate (with 5% edge threshold):\")\n",
    "\n",
    "filtered = apply_trading_filters(test_df, edge_threshold=0.05, spread_estimate=0.12)\n",
    "trades = filtered[filtered['trade']].copy()\n",
    "\n",
    "for spread in [0.08, 0.10, 0.12, 0.15, 0.20]:\n",
    "    trades['pnl_pct'] = trades.apply(lambda r: calculate_synthetic_pnl(r, spread_cost=spread), axis=1)\n",
    "    print(f\"  {spread:.0%} spread: \"\n",
    "          f\"win rate {(trades['pnl_pct'] > 0).mean():.1%}, \"\n",
    "          f\"mean P&L {trades['pnl_pct'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- **Training period:** 2024-03 to 2024-09 (6 months)\n",
    "- **Test period:** 2024-10 to 2025-12 (14 months, true out-of-sample)\n",
    "\n",
    "### Limitations\n",
    "\n",
    "1. **No real options data** - Straddle prices estimated from realized vol\n",
    "2. **No real spreads** - Using 12% estimate (may be optimistic for small caps)\n",
    "3. **No fill probability** - Assumes all trades fill\n",
    "4. **No position limits** - Doesn't account for capital constraints\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Compare to live paper trading results when available\n",
    "2. Get real options data (ORATS $99/mo) for accurate backtest\n",
    "3. Add fill probability model from Phase 0 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}